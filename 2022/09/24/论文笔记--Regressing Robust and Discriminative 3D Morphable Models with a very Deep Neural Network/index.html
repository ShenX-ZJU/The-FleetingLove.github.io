<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.4.2">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/zju32x32.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/zju16x16.ico">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/all.min.css" integrity="sha256-AbA177XfpSnFEvgpYu1jMygiLabzPCJCRIBtR5jGc0k=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"the-fleetinglove.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.13.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":"mac"},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":false,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="摘要 ​ 人脸的3维形状是易于区分的，但尽管如此，3维人脸仍很少用于人脸识别以及3维人脸数据经常是在受控环境下采集的。现有的方法在野外环境下对3D人脸的估计是不稳定的，会因同一主题的不同照片而发生变化，要么是过拟合，要么是过于通用（平均脸）。本文直接对输入图片处理，通过CNN回归3DMM的形状和纹理参数。本文通过提供一种生成大量标记示例的方法来克服所需的训练数据不足的问题。本文方法在MICC数">
<meta property="og:type" content="article">
<meta property="og:title" content="论文笔记--Regressing Robust and Discriminative 3D Morphable Models with a very Deep Neural Network">
<meta property="og:url" content="http://the-fleetinglove.github.io/2022/09/24/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0--Regressing%20Robust%20and%20Discriminative%203D%20Morphable%20Models%20with%20a%20very%20Deep%20Neural%20Network/index.html">
<meta property="og:site_name" content="Starry Light">
<meta property="og:description" content="摘要 ​ 人脸的3维形状是易于区分的，但尽管如此，3维人脸仍很少用于人脸识别以及3维人脸数据经常是在受控环境下采集的。现有的方法在野外环境下对3D人脸的估计是不稳定的，会因同一主题的不同照片而发生变化，要么是过拟合，要么是过于通用（平均脸）。本文直接对输入图片处理，通过CNN回归3DMM的形状和纹理参数。本文通过提供一种生成大量标记示例的方法来克服所需的训练数据不足的问题。本文方法在MICC数">
<meta property="og:locale">
<meta property="og:image" content="https://gitee.com/Forever_XS/cloud_img/raw/master/blog/image-20220428131155413.png">
<meta property="og:image" content="https://gitee.com/Forever_XS/cloud_img/raw/master/blog/image-20220428131711825.png">
<meta property="og:image" content="c:/Users/Admin/AppData/Roaming/Typora/typora-user-images/image-20220428132309766.png">
<meta property="og:image" content="https://gitee.com/Forever_XS/cloud_img/raw/master/blog/image-20220428132703834.png">
<meta property="og:image" content="https://gitee.com/Forever_XS/cloud_img/raw/master/blog/image-20220428132730719.png">
<meta property="article:published_time" content="2022-09-23T16:00:00.000Z">
<meta property="article:modified_time" content="2022-09-24T08:51:21.350Z">
<meta property="article:author" content="ZJU_XS">
<meta property="article:tag" content="Face Reconstruction">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://gitee.com/Forever_XS/cloud_img/raw/master/blog/image-20220428131155413.png">


<link rel="canonical" href="http://the-fleetinglove.github.io/2022/09/24/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0--Regressing%20Robust%20and%20Discriminative%203D%20Morphable%20Models%20with%20a%20very%20Deep%20Neural%20Network/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-Hans","comments":true,"permalink":"http://the-fleetinglove.github.io/2022/09/24/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0--Regressing%20Robust%20and%20Discriminative%203D%20Morphable%20Models%20with%20a%20very%20Deep%20Neural%20Network/","path":"2022/09/24/论文笔记--Regressing Robust and Discriminative 3D Morphable Models with a very Deep Neural Network/","title":"论文笔记--Regressing Robust and Discriminative 3D Morphable Models with a very Deep Neural Network"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>论文笔记--Regressing Robust and Discriminative 3D Morphable Models with a very Deep Neural Network | Starry Light</title>
  






  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Starry Light</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">长路灯火，漫天星光</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li>
  </ul>
</nav>




</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%91%98%E8%A6%81"><span class="nav-number">1.</span> <span class="nav-text">摘要</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%8B%E7%BB%8D"><span class="nav-number">2.</span> <span class="nav-text">介绍</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%96%B9%E6%B3%95"><span class="nav-number">3.</span> <span class="nav-text">方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#generating-training-data"><span class="nav-number">3.1.</span> <span class="nav-text">Generating Training
data</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#learning-to-regress-pooled-3dmm"><span class="nav-number">3.2.</span> <span class="nav-text">Learning to regress pooled
3DMM</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#parameter-based-3d-3d-recognition"><span class="nav-number">3.3.</span> <span class="nav-text">Parameter based 3D-3D
recognition</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C"><span class="nav-number">4.</span> <span class="nav-text">实验结果</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="ZJU_XS"
      src="/images/IMG_0045.JPG">
  <p class="site-author-name" itemprop="name">ZJU_XS</p>
  <div class="site-description" itemprop="description">记录生活的点滴，探索未知的领域</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">6</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/The-FleetingLove" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;The-FleetingLove" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:shenxie@zju.edu.cn" title="E-Mail → mailto:shenxie@zju.edu.cn" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.zhihu.com/people/gua-ke-34-22" title="Zhihu → https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;gua-ke-34-22" rel="noopener" target="_blank"><i class="fab fa-zhihu fa-fw"></i>Zhihu</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://space.bilibili.com/398418235?spm_id_from=333.1007.0.0" title="bilibili → https:&#x2F;&#x2F;space.bilibili.com&#x2F;398418235?spm_id_from&#x3D;333.1007.0.0" rel="noopener" target="_blank"><i class="fab fa-bilibili fa-fw"></i>bilibili</a>
      </span>
  </div>
  <div class="cc-license site-overview-item animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://the-fleetinglove.github.io/2022/09/24/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0--Regressing%20Robust%20and%20Discriminative%203D%20Morphable%20Models%20with%20a%20very%20Deep%20Neural%20Network/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/IMG_0045.JPG">
      <meta itemprop="name" content="ZJU_XS">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Starry Light">
      <meta itemprop="description" content="记录生活的点滴，探索未知的领域">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="论文笔记--Regressing Robust and Discriminative 3D Morphable Models with a very Deep Neural Network | Starry Light">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          论文笔记--Regressing Robust and Discriminative 3D Morphable Models with a very Deep Neural Network
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2022-09-24 00:00:00 / 修改时间：16:51:21" itemprop="dateCreated datePublished" datetime="2022-09-24T00:00:00+08:00">2022-09-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Face-Recon/" itemprop="url" rel="index"><span itemprop="name">Face Recon</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h2 id="摘要"><strong><em>摘要</em></strong></h2>
<p>​
人脸的3维形状是易于区分的，但尽管如此，3维人脸仍很少用于人脸识别以及3维人脸数据经常是在受控环境下采集的。现有的方法在野外环境下对3D人脸的估计是不稳定的，会因同一主题的不同照片而发生变化，要么是过拟合，要么是过于通用（平均脸）。本文直接对输入图片处理，通过CNN回归3DMM的形状和纹理参数。<strong>本文通过提供一种生成大量标记示例的方法来克服所需的训练数据不足的问题</strong>。本文方法在<strong>MICC数据集</strong>实现SOTA效果，同时在人脸识别效果上也有成效，本文在人脸识别上是使用3D人脸形状作为表示，而不是其他系统使用的不透明深度特征向量。
<span id="more"></span></p>
<h2 id="介绍"><strong><em>介绍</em></strong></h2>
<p>​
3D人脸形状是易于辨识的，因为每个人的人脸形状是不同的，且人脸形状不受光照和纹理等因素影响。目前没有关于在野外环境中成功使用单视图人脸形状估计来识别具有挑战性的无约束人脸的方法。</p>
<p><img src="https://gitee.com/Forever_XS/cloud_img/raw/master/blog/image-20220428131155413.png" alt="image-20220428131155413" style="zoom: 33%;"></p>
<h2 id="方法"><strong><em>方法</em></strong></h2>
<p>​
本文借助深度卷积神经网络来实现单张图片输入，回归得到3DMM面部形状参数。训练这样的CNN需要大量的带有正确3D面部形状标签的无约束面部数据。作者认为，之前没有将CNN用在三维人脸建模方面，主要是因为从二维图像重建三维人脸模型，我们需要回归高维的形状参数，这就要求非常深的网络，而训练非常深的网络又需要大量的训练数据，已知的三维人脸模型的训练集非常少。针对这一问题，作者提出利用一个对象的多姿态人脸图片生成准确率相当高的三维人脸形状（Automated
3D face reconstruction from multiple images using quality
measures），然后把生成的模型作为训练集；对于鲁棒性和区别性的人脸形状怎么解决呢？借鉴二维空间中的深度卷积神经网络模型，而且模型还是现成的~</p>
<p>​ 目前有三个主要现状认识：</p>
<ol type="1">
<li><p>利用同一人脸的多辐图像可以得到精确的三维估计</p></li>
<li><p>正确的3D人脸形状的标签很少可以利用，而每个主体却是包含有多张照片的</p></li>
<li><p>深度神经网络能很好提取和区分人脸特征</p>
<p>方法概览：</p>
<figure>
<img src="https://gitee.com/Forever_XS/cloud_img/raw/master/blog/image-20220428131711825.png" alt="image-20220428131711825">
<figcaption aria-hidden="true">image-20220428131711825</figcaption>
</figure></li>
</ol>
<h3 id="generating-training-data"><strong>Generating Training
data</strong></h3>
<p>​
3DMM的表示采用的是BFM模型，模型数学含义表达式如公式(1)所示。其中<span class="math inline">\(\alpha\)</span>和<span class="math inline">\(\beta\)</span>分别表示形体和纹理参数，且都是99维的向量。
<span class="math display">\[
S^{&#39;}=\hat{s}+W_S\alpha{\quad}{\quad}T^{&#39;}=\hat{t}+W_T\beta
\]</span> ​ 在中科院的CASIA WebFace
dataset选定一张照片，利用CLNF做人脸检测，得到68个人脸关键点和该图片的置信度。将得到的关键点用来初始化估计人脸模型的姿态，该姿态用六个自由度表示，
三个自由度是旋转角度，三个是位移。
然后再优化3DMM的形状，纹理，姿态，光照和颜色，利用前人的方法解决定位误差。
一旦损失函数收敛，就得到的形状参数和纹理参数，这就是单个图像得到的3DMM估计。</p>
<p>​
一张图片的3DMM参数可以用公式(2)表示，N表示同一目标下所有视图数量，一个目标的一张视图的3DMM参数用<span class="math display">\[\gamma_i\]</span>表示，一个目标的3DMM参数用<span class="math inline">\(\gamma\)</span>表示，<span class="math inline">\(w_i\)</span>表示归一化后的置信度，如公式(3)所示。至此，每个CASIA数据集的目标对象都有一个合并好的3DMM参数向量<span class="math inline">\(\gamma\)</span>相关联。 <span class="math display">\[
γ_i=[\alpha_i,\beta_i]\quad i\in1..N\quad(2)
\]</span></p>
<p><span class="math display">\[
{\gamma}={\sum}_{i=1}^{N}{w_i·\gamma_i}\quad{and}\quad{\sum}_{i=1}^{N}w_i=1\quad(3)
\]</span></p>
<h3 id="learning-to-regress-pooled-3dmm">Learning to regress pooled
3DMM</h3>
<p>​
使用之前生成的数据用于训练一个函数式，理想情况下，同一主题的不同照片通过该函数式回归得到的3DMM特征向量是一致的。本文采用了ResNet101，并经过人脸识别数据集的预训练。本文将该网络的最后一层全连接层进行修改，使其输出198维的3DMM特征向量<span class="math inline">\(\gamma\)</span>。</p>
<p>​
本文是将CNN在CASIA数据集上进行微调，标签真值是之前生成的3DMM估计；同一目标对象的不同视角的图片共用一个3DMM形状参数；本文还采用了16层的VGG-Face架构进行训练验证，其结果稍逊于ResNet101。</p>
<p>​
3DMM向量属于多元高斯分布，均值在原点处，代表平均人脸。因此，在训练过程中采用标准的欧几里得损失会使得人脸模型太泛化，没有区别性。本文提出一种非对称欧拉损失，公式如下：其中<span class="math inline">\(\gamma\)</span>是标签值，<span class="math inline">\(\gamma_p\)</span>是预测值，<span class="math inline">\(\lambda_1\)</span>和<span class="math inline">\(\lambda_2\)</span>控制过度和不足的估计误差之间的平衡，<span class="math inline">\(\lambda_1\)</span>和<span class="math inline">\(\lambda_2\)</span>都等于1时就是传统的欧几里得损失。本文设定<span class="math inline">\(\lambda_1\)</span>的值为1，<span class="math inline">\(\lambda_2\)</span>的值为3，使模型更快摆脱欠拟合并且鼓励网络生成更真实的三维人脸模型。
<span class="math display">\[
\ell(\gamma_p,\gamma)=\lambda_1·\underbrace{||\gamma^+-\gamma_{max}||^{2}_{2}}_{over-estimate}+\lambda_2·\underbrace{||\gamma_{p}^{+}-\gamma_{max}||^{2}_{2}}_{under-estimate}
\]</span></p>
<p><span class="math display">\[
{\gamma}^{+}{\doteq}{abs(\gamma)}{\doteq}{sign(\gamma)}·{\gamma}{\quad}{\quad}{\gamma^{+}_{p}{\doteq}{sign(\gamma)·\gamma_p}}{\quad}{\quad}{\gamma_{max}{\doteq}{max(\gamma^+,\gamma_{p}^{+})}}
\]</span></p>
<p>​ 训练参数设置：SGD，batch_size: 144，momentum: 0.9，l2 weight decay:
0.0005，learning rate: 0.01</p>
<p>​
3DMM参数直接通过CNN对输入图像回归得到，没有进行纹理渲染的优化，只为得到准确的形状。</p>
<h3 id="parameter-based-3d-3d-recognition">Parameter based 3D-3D
recognition</h3>
<p>​ 用余弦相似度来判断两个人脸三维模型是否相似，公式如下： <span class="math display">\[
s(\gamma_1,\gamma_2)=\frac{\gamma_{p1}·\gamma_{p2}^T}{||\gamma_{p1}||·||\gamma_{p2}||}
\]</span></p>
<h2 id="实验结果">实验结果</h2>
<p>评价指标：</p>
<figure>
<img src="C:/Users/Admin/AppData/Roaming/Typora/typora-user-images/image-20220428132309766.png" alt="image-20220428132309766">
<figcaption aria-hidden="true">image-20220428132309766</figcaption>
</figure>
<p>重建实验效果及对比：</p>
<figure>
<img src="https://gitee.com/Forever_XS/cloud_img/raw/master/blog/image-20220428132703834.png" alt="image-20220428132703834">
<figcaption aria-hidden="true">image-20220428132703834</figcaption>
</figure>
<p>野外数据重建效果：</p>
<p><img src="https://gitee.com/Forever_XS/cloud_img/raw/master/blog/image-20220428132730719.png" alt="image-20220428132730719" style="zoom: 67%;"></p>
<p><strong>END</strong></p>

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>ZJU_XS
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="http://the-fleetinglove.github.io/2022/09/24/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0--Regressing%20Robust%20and%20Discriminative%203D%20Morphable%20Models%20with%20a%20very%20Deep%20Neural%20Network/" title="论文笔记--Regressing Robust and Discriminative 3D Morphable Models with a very Deep Neural Network">http://the-fleetinglove.github.io/2022/09/24/论文笔记--Regressing Robust and Discriminative 3D Morphable Models with a very Deep Neural Network/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/Face-Reconstruction/" rel="tag"><i class="fa fa-tag"></i> Face Reconstruction</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2022/09/24/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0--Accurate%203D%20Face%20Reconstruction%20with%20Weakly-Supervised%20LearningFrom%20Single%20Image%20to%20Image%20Set/" rel="prev" title="论文笔记--Accurate 3D Face Reconstruction with Weakly-Supervised Learning:From Single Image to Image Set">
                  <i class="fa fa-chevron-left"></i> 论文笔记--Accurate 3D Face Reconstruction with Weakly-Supervised Learning:From Single Image to Image Set
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2022/09/24/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0--Cross-modal%20Deep%20Face%20Normals%20with%20Deactivable%20Skip%20Connections/" rel="next" title="论文笔记--Cross-modal Deep Face Normals with Deactivable Skip Connections***">
                  论文笔记--Cross-modal Deep Face Normals with Deactivable Skip Connections*** <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">ZJU_XS</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/next-boot.js"></script>

  





  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
