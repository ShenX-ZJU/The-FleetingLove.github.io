<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.4.2">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/eagle32x32.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/eagle16x16.ico">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/all.min.css" integrity="sha256-AbA177XfpSnFEvgpYu1jMygiLabzPCJCRIBtR5jGc0k=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"the-fleetinglove.github.io","root":"/","images":"/images","scheme":"Pisces","darkmode":false,"version":"8.13.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":"mac"},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"changyan","storage":true,"lazyload":false,"nav":null,"activeClass":"changyan"},"stickytabs":false,"motion":{"enable":false,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/./public/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="Awesome 3D Human Reconstruction A curated list of related resources for 3d human reconstruction. Link Contents  papers related papers parametric model dataset labs other related awesome sur">
<meta property="og:type" content="article">
<meta property="og:title" content="3D Human Reconstruction Paper List">
<meta property="og:url" content="http://the-fleetinglove.github.io/2022/09/30/%E8%AE%BA%E6%96%87%E5%90%88%E9%9B%86--Awesome%203D%20Human%20Reconstruction/index.html">
<meta property="og:site_name" content="Starry Light">
<meta property="og:description" content="Awesome 3D Human Reconstruction A curated list of related resources for 3d human reconstruction. Link Contents  papers related papers parametric model dataset labs other related awesome sur">
<meta property="og:locale">
<meta property="article:published_time" content="2022-09-30T04:50:05.000Z">
<meta property="article:modified_time" content="2022-09-30T11:38:32.318Z">
<meta property="article:author" content="ZJU_XS">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://the-fleetinglove.github.io/2022/09/30/%E8%AE%BA%E6%96%87%E5%90%88%E9%9B%86--Awesome%203D%20Human%20Reconstruction/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-Hans","comments":true,"permalink":"http://the-fleetinglove.github.io/2022/09/30/%E8%AE%BA%E6%96%87%E5%90%88%E9%9B%86--Awesome%203D%20Human%20Reconstruction/","path":"2022/09/30/论文合集--Awesome 3D Human Reconstruction/","title":"3D Human Reconstruction Paper List"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>3D Human Reconstruction Paper List | Starry Light</title>
  






  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Starry Light</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">长路灯火 漫天星光</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li><li class="menu-item menu-item-guestbook"><a href="/guestbook/" rel="section"><i class="fa fa-book fa-fw"></i>留言</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#awesome-3d-human-reconstruction"><span class="nav-number">1.</span> <span class="nav-text">Awesome 3D Human
Reconstruction</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#contents"><span class="nav-number">1.1.</span> <span class="nav-text">Contents</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#d-human"><span class="nav-number">1.2.</span> <span class="nav-text">3d human</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#nerf-or-pifu"><span class="nav-number">1.2.1.</span> <span class="nav-text">nerf or pifu</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#stereopifu-depth-aware-clothed-human-digitization-via-stereo-vision-paper"><span class="nav-number">1.2.1.0.1.</span> <span class="nav-text">•
StereoPIFu: Depth Aware Clothed Human Digitization via Stereo Vision paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#learning-implicit-3d-representations-of-dressed-humans-from-sparse-views-paper"><span class="nav-number">1.2.1.0.2.</span> <span class="nav-text">•
Learning Implicit 3D Representations of Dressed Humans from Sparse Views
paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#animatable-neural-radiance-fields-for-human-body-modeling-paper"><span class="nav-number">1.2.1.0.3.</span> <span class="nav-text">•
Animatable Neural Radiance Fields for Human Body Modeling paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#pamir-parametric-model-conditioned-implicit-representation-for-image-based-human-reconstruction-paper-code"><span class="nav-number">1.2.1.0.4.</span> <span class="nav-text">•
PaMIR: Parametric Model-Conditioned Implicit Representation for
Image-based Human Reconstruction paper code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#neural-actor-neural-free-view-synthesis-of-human-actors-with-pose-control-papercode"><span class="nav-number">1.2.1.0.5.</span> <span class="nav-text">•
Neural Actor: Neural Free-view Synthesis of Human Actors with Pose
Control paper&amp;code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#moco-flow-neural-motion-consensus-flow-for-dynamic-humans-in-stationary-monocular-cameras-paper"><span class="nav-number">1.2.1.0.6.</span> <span class="nav-text">•
MoCo-Flow: Neural Motion Consensus Flow for Dynamic Humans in Stationary
Monocular Cameras paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#doublefield-bridging-the-neural-surface-and-radiance-fields-for-high-fidelity-human-rendering-paper"><span class="nav-number">1.2.1.0.7.</span> <span class="nav-text">•
DoubleField: Bridging the Neural Surface and Radiance Fields for
High-fidelity Human Rendering paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#bridge-the-gap-between-model-based-and-model-free-human-reconstruction-paper"><span class="nav-number">1.2.1.0.8.</span> <span class="nav-text">•
Bridge the Gap Between Model-based and Model-free Human Reconstruction
paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#metaavatar-learning-animatable-clothed-human-models-from-few-depth-images-paper"><span class="nav-number">1.2.1.0.9.</span> <span class="nav-text">•
MetaAvatar: Learning Animatable Clothed Human Models from Few Depth
Images paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#animatable-neural-radiance-fields-from-monocular-rgb-video-paper"><span class="nav-number">1.2.1.0.10.</span> <span class="nav-text">•
Animatable Neural Radiance Fields from Monocular RGB Video paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#few-shot-neural-human-performance-rendering-from-sparse-rgbd-videos-paper"><span class="nav-number">1.2.1.0.11.</span> <span class="nav-text">•
Few-shot Neural Human Performance Rendering from Sparse RGBD Videos paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#relightable-neural-video-portrait-paper"><span class="nav-number">1.2.1.0.12.</span> <span class="nav-text">• Relightable Neural
Video Portrait paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#flame-in-nerf-neural-control-of-radiance-fields-for-free-view-face-animation-paper"><span class="nav-number">1.2.1.0.13.</span> <span class="nav-text">•
FLAME-in-NeRF : Neural control of Radiance Fields for Free View Face
Animation paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#arch-animation-ready-clothed-human-reconstruction-revisited-paper"><span class="nav-number">1.2.1.0.14.</span> <span class="nav-text">•
ARCH++: Animation-Ready Clothed Human Reconstruction Revisited paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#neural-gif-neural-generalized-implicit-functions-for-animating-people-in-clothing-paper"><span class="nav-number">1.2.1.0.15.</span> <span class="nav-text">•
Neural-GIF: Neural Generalized Implicit Functions for Animating People
in Clothing paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#neural-human-performer-learning-generalizable-radiance-fields-for-human-performance-rendering-paper-code"><span class="nav-number">1.2.1.0.16.</span> <span class="nav-text">•
Neural Human Performer: Learning Generalizable Radiance Fields for Human
Performance Rendering paper code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#topologically-consistent-multi-view-face-inference-using-volumetric-sampling-paper"><span class="nav-number">1.2.1.0.17.</span> <span class="nav-text">•
Topologically Consistent Multi-View Face Inference Using Volumetric
Sampling paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#creating-and-reenacting-controllable-3d-humans-with-differentiable-rendering-paper"><span class="nav-number">1.2.1.0.18.</span> <span class="nav-text">•
Creating and Reenacting Controllable 3D Humans with Differentiable
Rendering paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#h-nerf-neural-radiance-fields-for-rendering-and-temporal-reconstruction-of-humans-in-motion-paper"><span class="nav-number">1.2.1.0.19.</span> <span class="nav-text">•
H-NeRF: Neural Radiance Fields for Rendering and Temporal Reconstruction
of Humans in Motion paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#fenerf-face-editing-in-neural-radiance-fields-paper"><span class="nav-number">1.2.1.0.20.</span> <span class="nav-text">• FENeRF:
Face Editing in Neural Radiance Fields paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#latenthuman-shape-and-pose-disentangled-latent-representation-for-human-bodies-papercode"><span class="nav-number">1.2.1.0.21.</span> <span class="nav-text">•
LatentHuman: Shape-and-Pose Disentangled Latent Representation for Human
Bodies paper&amp;code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#neural-head-avatars-from-monocular-rgb-videos-paper"><span class="nav-number">1.2.1.0.22.</span> <span class="nav-text">• Neural
Head Avatars from Monocular RGB Videos paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#humannerf-generalizable-neural-human-radiance-field-from-sparse-inputs-paper"><span class="nav-number">1.2.1.0.23.</span> <span class="nav-text">•
HumanNeRF: Generalizable Neural Human Radiance Field from Sparse Inputs
paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#implicit-neural-deformation-for-multi-view-face-reconstruction-paper"><span class="nav-number">1.2.1.0.24.</span> <span class="nav-text">•
Implicit Neural Deformation for Multi-View Face Reconstruction paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#mofanerf-morphable-facial-neural-radiance-field-paper-code"><span class="nav-number">1.2.1.0.25.</span> <span class="nav-text">•
MoFaNeRF: Morphable Facial Neural Radiance Field paper code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#geometry-guided-progressive-nerf-for-generalizable-and-efficient-neural-human-rendering-paper"><span class="nav-number">1.2.1.0.26.</span> <span class="nav-text">•
Geometry-Guided Progressive NeRF for Generalizable and Efficient Neural
Human Rendering paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#headnerf-a-real-time-nerf-based-parametric-head-model-paper-code"><span class="nav-number">1.2.1.0.27.</span> <span class="nav-text">•
HeadNeRF: A Real-time NeRF-based Parametric Head Model paper code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#i-m-avatar-implicit-morphable-head-avatars-from-videos-paper-code"><span class="nav-number">1.2.1.0.28.</span> <span class="nav-text">•
I M Avatar: Implicit Morphable Head Avatars from Videos paper code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#lookingood%CF%80-real-time-person-independent-neural-re-rendering-for-high-quality-human-performance-capture-paper"><span class="nav-number">1.2.1.0.29.</span> <span class="nav-text">•
LookinGoodπ: Real-time Person-independent Neural Re-rendering for
High-quality Human Performance Capture paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#icon-implicit-clothed-humans-obtained-from-normals-paper-code"><span class="nav-number">1.2.1.0.30.</span> <span class="nav-text">•
ICON: Implicit Clothed humans Obtained from Normals paper code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#dd-nerf-double-diffusion-neural-radiance-field-as-a-generalizable-implicit-body-representation-paper"><span class="nav-number">1.2.1.0.31.</span> <span class="nav-text">•
DD-NeRF: Double-Diffusion Neural Radiance Field as a Generalizable
Implicit Body Representation paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#human-view-synthesis-using-a-single-sparse-rgb-d-input-paper"><span class="nav-number">1.2.1.0.32.</span> <span class="nav-text">•
Human View Synthesis using a Single Sparse RGB-D Input paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#surface-aligned-neural-radiance-fields-for-controllable-3d-human-synthesis-paper"><span class="nav-number">1.2.1.0.33.</span> <span class="nav-text">•
Surface-Aligned Neural Radiance Fields for Controllable 3D Human
Synthesis paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#embodied-hands-modeling-and-capturing-hands-and-bodies-together-paper"><span class="nav-number">1.2.1.0.34.</span> <span class="nav-text">•
Embodied Hands: Modeling and Capturing Hands and Bodies Together paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#humannerf-free-viewpoint-rendering-of-moving-people-from-monocular-video-paper-code"><span class="nav-number">1.2.1.0.35.</span> <span class="nav-text">•
HumanNeRF: Free-viewpoint Rendering of Moving People from Monocular
Video paper code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#gdna-towards-generative-detailed-neural-avatars-paper-code"><span class="nav-number">1.2.1.0.36.</span> <span class="nav-text">•
gDNA: Towards Generative Detailed Neural Avatars paper code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#selfrecon-self-reconstruction-your-digital-avatar-from-monocular-video-paper"><span class="nav-number">1.2.1.0.37.</span> <span class="nav-text">•
SelfRecon: Self Reconstruction Your Digital Avatar from Monocular Video
paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#neuvv-neural-volumetric-videos-with-immersive-rendering-and-editing-paper"><span class="nav-number">1.2.1.0.38.</span> <span class="nav-text">•
NeuVV: Neural Volumetric Videos with Immersive Rendering and Editing paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#animatable-neural-radiance-fields-for-modeling-dynamic-human-bodies-paper-code"><span class="nav-number">1.2.1.0.39.</span> <span class="nav-text">•
Animatable Neural Radiance Fields for Modeling Dynamic Human Bodies paper code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#deepmulticap-performance-capture-of-multiple-characters-using-sparse-multiview-cameras-paper-code"><span class="nav-number">1.2.1.0.40.</span> <span class="nav-text">•
DeepMultiCap: Performance Capture of Multiple Characters Using Sparse
Multiview Cameras paper
code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#neuralfusion-neural-volumetric-rendering-under-human-object-interactions-paper"><span class="nav-number">1.2.1.0.41.</span> <span class="nav-text">•
NeuralFusion: Neural Volumetric Rendering under Human-object
Interactions paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#pina-learning-a-personalized-implicit-neural-avatar-from-a-single-rgb-d-video-sequence-paper-code"><span class="nav-number">1.2.1.0.42.</span> <span class="nav-text">•
PINA: Learning a Personalized Implicit Neural Avatar from a Single RGB-D
Video Sequence paper code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#neuman-neural-human-radiance-field-from-a-single-video-paper"><span class="nav-number">1.2.1.0.43.</span> <span class="nav-text">•
NeuMan: Neural Human Radiance Field from a Single Video paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#m-avatar-implicit-morphable-head-avatars-from-videos-paper-code"><span class="nav-number">1.2.1.0.44.</span> <span class="nav-text">• M
Avatar: Implicit Morphable Head Avatars from Videos paper code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#imface-a-nonlinear-3d-morphable-face-model-with-implicit-neural-representations-paper"><span class="nav-number">1.2.1.0.45.</span> <span class="nav-text">•
ImFace: A Nonlinear 3D Morphable Face Model with Implicit Neural
Representations paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#coap-compositional-articulated-occupancy-of-people-paper-code"><span class="nav-number">1.2.1.0.46.</span> <span class="nav-text">•
COAP: Compositional Articulated Occupancy of People paper code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#sunstage-portrait-reconstruction-and-relighting-using-the-sun-as-a-light-stage-paper"><span class="nav-number">1.2.1.0.47.</span> <span class="nav-text">•
SunStage: Portrait Reconstruction and Relighting using the Sun as a
Light Stage paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#lisa-learning-implicit-shape-and-appearance-of-hands-paper"><span class="nav-number">1.2.1.0.48.</span> <span class="nav-text">•
LISA: Learning Implicit Shape and Appearance of Hands paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#animatable-neural-radiance-fields-from-monocular-rgb-d-paper"><span class="nav-number">1.2.1.0.49.</span> <span class="nav-text">•
Animatable Neural Radiance Fields from Monocular RGB-D paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#jiff-jointly-aligned-implicit-face-function-for-high-quality-single-view-clothed-human-reconstruction-paper"><span class="nav-number">1.2.1.0.50.</span> <span class="nav-text">•
JIFF: Jointly-aligned Implicit Face Function for High Quality Single
View Clothed Human Reconstruction paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#generalizable-neural-performer-learning-robust-radiance-fields-for-human-novel-view-synthesis-paper"><span class="nav-number">1.2.1.0.51.</span> <span class="nav-text">•
Generalizable Neural Performer: Learning Robust Radiance Fields for
Human Novel View Synthesis paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#danbo-disentangled-articulated-neural-body-representations-via-graph-neural-networks-paper"><span class="nav-number">1.2.1.0.52.</span> <span class="nav-text">•
DANBO: Disentangled Articulated Neural Body Representations via Graph
Neural Networks paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#single-view-3d-body-and-cloth-reconstruction-under-complex-poses-paper"><span class="nav-number">1.2.1.0.53.</span> <span class="nav-text">•
Single-view 3D Body and Cloth Reconstruction under Complex Poses paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#keypointnerf-generalizing-image-based-volumetric-avatars-using-relative-spatial-encoding-of-keypoints-paper"><span class="nav-number">1.2.1.0.54.</span> <span class="nav-text">•
KeypointNeRF: Generalizing Image-based Volumetric Avatars using Relative
Spatial Encoding of Keypoints paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#h3d-net-few-shot-high-fidelity-3d-head-reconstruction-paper-code"><span class="nav-number">1.2.1.0.55.</span> <span class="nav-text">•
H3D-Net: Few-Shot High-Fidelity 3D Head Reconstruction paper code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#photorealistic-monocular-3d-reconstruction-of-humans-wearing-clothing-paper"><span class="nav-number">1.2.1.0.56.</span> <span class="nav-text">•
Photorealistic Monocular 3D Reconstruction of Humans Wearing Clothing paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#high-fidelity-human-avatars-from-a-single-rgb-camera-paper"><span class="nav-number">1.2.1.0.57.</span> <span class="nav-text">•
High-Fidelity Human Avatars from a Single RGB Camera paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#uv-volumes-for-real-time-rendering-of-editable-free-view-human-performance-code"><span class="nav-number">1.2.1.0.58.</span> <span class="nav-text">•
UV Volumes for Real-time Rendering of Editable Free-view Human
Performance code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#fof-learning-fourier-occupancy-field-for-monocular-real-time-human-reconstruction-paper"><span class="nav-number">1.2.1.0.59.</span> <span class="nav-text">•
FOF: Learning Fourier Occupancy Field for Monocular Real-time Human
Reconstruction paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#nemf-neural-motion-fields-for-kinematic-animation-paper"><span class="nav-number">1.2.1.0.60.</span> <span class="nav-text">• NeMF:
Neural Motion Fields for Kinematic Animation paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#rignerf-fully-controllable-neural-3d-portraits-paper"><span class="nav-number">1.2.1.0.61.</span> <span class="nav-text">• RigNeRF:
Fully Controllable Neural 3D Portraits paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#eyenerf-a-hybrid-representation-for-photorealistic-synthesis-animation-and-relighting-of-human-eyes-paper"><span class="nav-number">1.2.1.0.62.</span> <span class="nav-text">•
EyeNeRF: A Hybrid Representation for Photorealistic Synthesis, Animation
and Relighting of Human Eyes paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#tava-template-free-animatable-volumetric-actors-paper"><span class="nav-number">1.2.1.0.63.</span> <span class="nav-text">• TAVA:
Template-free Animatable Volumetric Actors paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#neural-surface-reconstruction-of-dynamic-scenes-with-monocular-rgb-d-camera-homepage"><span class="nav-number">1.2.1.0.64.</span> <span class="nav-text">•
Neural Surface Reconstruction of Dynamic Scenes with Monocular RGB-D
Camera homepage</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#generative-neural-articulated-radiance-fields-paper"><span class="nav-number">1.2.1.0.65.</span> <span class="nav-text">•
Generative Neural Articulated Radiance Fields paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#neural-parameterization-for-dynamic-human-head-editing-paper"><span class="nav-number">1.2.1.0.66.</span> <span class="nav-text">•
Neural Parameterization for Dynamic Human Head Editing paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#avatarcap-animatable-avatar-conditioned-monocular-human-volumetric-capture-paper-code"><span class="nav-number">1.2.1.0.67.</span> <span class="nav-text">•
AvatarCap: Animatable Avatar Conditioned Monocular Human Volumetric
Capture paper code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#learning-implicit-templates-for-point-based-clothed-human-modeling-homepage"><span class="nav-number">1.2.1.0.68.</span> <span class="nav-text">•
Learning Implicit Templates for Point-Based Clothed Human Modeling homepage</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#relighting4d-neural-relightable-human-from-videos-code"><span class="nav-number">1.2.1.0.69.</span> <span class="nav-text">•Relighting4D:
Neural Relightable Human from Videos code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#high-quality-human-reconstruction-via-diffusion-based-stereo-using-sparse-cameras-paper"><span class="nav-number">1.2.1.0.70.</span> <span class="nav-text">•High
Quality Human Reconstruction via Diffusion-based Stereo Using Sparse
Cameras paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#crosshuman-learning-cross-guidance-from-multi-frame-images-for-human-reconstruction-paper"><span class="nav-number">1.2.1.0.71.</span> <span class="nav-text">•CrossHuman:
Learning Cross-Guidance from Multi-Frame Images for Human Reconstruction
paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#unif-united-neural-implicit-functions-for-clothed-human-reconstruction-and-animation-paper-code"><span class="nav-number">1.2.1.0.72.</span> <span class="nav-text">•UNIF:
United Neural Implicit Functions for Clothed Human Reconstruction and
Animation paper code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#drivable-volumetric-avatars-using-texel-aligned-features-paper"><span class="nav-number">1.2.1.0.73.</span> <span class="nav-text">•Drivable
Volumetric Avatars using Texel-Aligned Features paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#the-one-where-they-reconstructed-3d-humans-and-environments-in-tv-shows-homepage"><span class="nav-number">1.2.1.0.74.</span> <span class="nav-text">•The
One Where They Reconstructed 3D Humans and Environments in TV Shows homepage</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#avatargen-a-3d-generative-model-for-animatable-human-avatars-paper"><span class="nav-number">1.2.1.0.75.</span> <span class="nav-text">•AvatarGen:
a 3D Generative Model for Animatable Human Avatars paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#voltemorph-realtime-controllable-and-generalisable-animation-of-volumetric-representations-paper"><span class="nav-number">1.2.1.0.76.</span> <span class="nav-text">•VolTeMorph:
Realtime, Controllable and Generalisable Animation of Volumetric
Representations paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#multi-neus-3d-head-portraits-from-single-image-with-neural-implicit-functions-paper"><span class="nav-number">1.2.1.0.77.</span> <span class="nav-text">•Multi-NeuS:
3D Head Portraits from Single Image with Neural Implicit Functions paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#learning-to-relight-portrait-images-via-a-virtual-light-stage-and-synthetic-to-real-adaptation-paper"><span class="nav-number">1.2.1.0.78.</span> <span class="nav-text">•Learning
to Relight Portrait Images via a Virtual Light Stage and
Synthetic-to-Real Adaptation paper</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#geo-fusion"><span class="nav-number">1.2.2.</span> <span class="nav-text">geo fusion</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#doublefusion-real-time-capture-of-human-performances-with-inner-body-shapes-from-a-single-depth-sensor-paper"><span class="nav-number">1.2.2.0.1.</span> <span class="nav-text">•
DoubleFusion: Real-time Capture of Human Performances with Inner Body
Shapes from a Single Depth Sensor paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#robust-3d-self-portraits-in-seconds-paper"><span class="nav-number">1.2.2.0.2.</span> <span class="nav-text">• Robust 3D
Self-portraits in Seconds paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#accurate-human-body-reconstruction-for-volumetric-video-paper"><span class="nav-number">1.2.2.0.3.</span> <span class="nav-text">•
ACCURATE HUMAN BODY RECONSTRUCTION FOR VOLUMETRIC VIDEO paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#occlusionfusion-occlusion-aware-motion-estimation-for-real-time-dynamic-3d-reconstruction-paper-code"><span class="nav-number">1.2.2.0.4.</span> <span class="nav-text">•
OcclusionFusion: Occlusion-aware Motion Estimation for Real-time Dynamic
3D Reconstruction paper code</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#photo"><span class="nav-number">1.2.3.</span> <span class="nav-text">photo</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#portrait-reconstruction-and-relighting-using-the-sun-as-a-light-stage-homepage"><span class="nav-number">1.2.3.0.1.</span> <span class="nav-text">•
Portrait Reconstruction and Relighting using the Sun as a Light Stage homepage</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#d-human-whole-body"><span class="nav-number">1.2.4.</span> <span class="nav-text">3D human whole body</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#monocular-expressive-body-regression-through-body-driven-attention-paper-code"><span class="nav-number">1.2.4.0.1.</span> <span class="nav-text">•
Monocular Expressive Body Regression through Body-Driven Attention paper code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#frankmocap-fast-monocular-3d-hand-and-body-motion-capture-by-regression-and-integration-paper-code"><span class="nav-number">1.2.4.0.2.</span> <span class="nav-text">•
FrankMocap: Fast Monocular 3D Hand and Body Motion Capture by Regression
and Integration paper
code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#collaborative-regression-of-expressive-bodies-using-moderation-papercode"><span class="nav-number">1.2.4.0.3.</span> <span class="nav-text">•
Collaborative Regression of Expressive Bodies using Moderation paper&amp;code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#monocular-real-time-full-body-capture-with-inter-part-correlations-papercode"><span class="nav-number">1.2.4.0.4.</span> <span class="nav-text">•
Monocular Real-time Full Body Capture with Inter-part Correlations paper&amp;code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#monocular-real-time-hand-shape-and-motion-capture-using-multi-modal-data-papercode"><span class="nav-number">1.2.4.0.5.</span> <span class="nav-text">•
Monocular Real-time Hand Shape and Motion Capture using Multi-modal Data
paper&amp;code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#real-time-rgbd-based-extended-body-pose-estimation-paper-code"><span class="nav-number">1.2.4.0.6.</span> <span class="nav-text">•
Real-time RGBD-based Extended Body Pose Estimation paper code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#detailed-avatar-recovery-from-single-image-paper"><span class="nav-number">1.2.4.0.7.</span> <span class="nav-text">• Detailed
Avatar Recovery from Single Image paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#lightweight-multi-person-total-motion-capture-using-sparse-multi-view-cameras-paper"><span class="nav-number">1.2.4.0.8.</span> <span class="nav-text">•
Lightweight Multi-person Total Motion Capture Using Sparse Multi-view
Cameras paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#imposing-temporal-consistency-on-deep-monocular-body-shape-and-pose-estimation-paper"><span class="nav-number">1.2.4.0.9.</span> <span class="nav-text">•
Imposing Temporal Consistency on Deep Monocular Body Shape and Pose
Estimation paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#piano-a-parametric-hand-bone-model-from-magnetic-resonance-imaging-github"><span class="nav-number">1.2.4.0.10.</span> <span class="nav-text">•
PIANO: A Parametric Hand Bone Model from Magnetic Resonance Imaging github</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#goal-generating-4d-whole-body-motion-for-hand-object-grasping-paper-code"><span class="nav-number">1.2.4.0.11.</span> <span class="nav-text">•
GOAL: Generating 4D Whole-Body Motion for Hand-Object Grasping paper code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#pymaf-x-towards-well-aligned-full-body-model-regression-from-monocular-images-paper"><span class="nav-number">1.2.4.0.12.</span> <span class="nav-text">•
PyMAF-X: Towards Well-aligned Full-body Model Regression from Monocular
Images paper</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#d-human-body"><span class="nav-number">1.2.5.</span> <span class="nav-text">3D human body</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#pose2mesh-graph-convolutional-network-for-3d-human-pose-and-mesh-recovery-from-a-2d-human-pose-paper-code"><span class="nav-number">1.2.5.0.1.</span> <span class="nav-text">•
Pose2Mesh: Graph Convolutional Network for 3D Human Pose and Mesh
Recovery from a 2D Human Pose paper code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#monocular-real-time-volumetric-performance-capture-paper-code"><span class="nav-number">1.2.5.0.2.</span> <span class="nav-text">•
Monocular Real-Time Volumetric Performance Capture paper code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#full-body-awareness-from-partial-observations-paper-code"><span class="nav-number">1.2.5.0.3.</span> <span class="nav-text">•
Full-Body Awareness from Partial Observations paper code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#centerhmr-a-bottom-up-single-shot-method-for-multi-person-3d-mesh-recovery-from-a-single-image-paper-code"><span class="nav-number">1.2.5.0.4.</span> <span class="nav-text">•
CenterHMR: a Bottom-up Single-shot Method for Multi-person 3D Mesh
Recovery from a Single Image paper code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#reconstructing-nba-players-paper-code"><span class="nav-number">1.2.5.0.5.</span> <span class="nav-text">• Reconstructing NBA
players paper code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#going-beyond-free-viewpoint-creating-animatable-volumetric-video-of-human-performances-paper"><span class="nav-number">1.2.5.0.6.</span> <span class="nav-text">•
Going beyond Free Viewpoint: Creating Animatable Volumetric Video of
Human Performances paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#synthetic-training-for-accurate-3d-human-pose-and-shape-estimation-in-the-wild-paper-code"><span class="nav-number">1.2.5.0.7.</span> <span class="nav-text">•
Synthetic Training for Accurate 3D Human Pose and Shape Estimation in
the Wild paper code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#monoclothcap-towards-temporally-coherent-clothing-capture-from-monocular-rgb-video-paper"><span class="nav-number">1.2.5.0.8.</span> <span class="nav-text">•
MonoClothCap: Towards Temporally Coherent Clothing Capture from
Monocular RGB Video paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#multi-view-consistency-loss-for-improved-single-image-3d-reconstruction-of-clothed-people-paper-code"><span class="nav-number">1.2.5.0.9.</span> <span class="nav-text">•
Multi-View Consistency Loss for Improved Single-Image 3D Reconstruction
of Clothed People paper
code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#synthetic-training-for-monocular-human-mesh-recovery-paper"><span class="nav-number">1.2.5.0.10.</span> <span class="nav-text">•
Synthetic Training for Monocular Human Mesh Recovery paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#pose2pose-3d-positional-pose-guided-3d-rotational-pose-prediction-for-expressive-3d-human-pose-and-mesh-estimation-paper"><span class="nav-number">1.2.5.0.11.</span> <span class="nav-text">•
Pose2Pose: 3D Positional Pose-Guided 3D Rotational Pose Prediction for
Expressive 3D Human Pose and Mesh Estimation paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#deep-physics-aware-inference-of-cloth-deformation-for-monocular-human-performance-capture"><span class="nav-number">1.2.5.0.12.</span> <span class="nav-text">•
Deep Physics-aware Inference of Cloth Deformation for Monocular Human
Performance Capture</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#d-human-body-capture-from-egocentric-video-via-3d-scene-grounding-paper"><span class="nav-number">1.2.5.0.13.</span> <span class="nav-text">•
4D Human Body Capture from Egocentric Video via 3D Scene Grounding paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#hybrik-a-hybrid-analytical-neural-inverse-kinematics-solution-for-3d-human-pose-and-shape-estimation-paper-code"><span class="nav-number">1.2.5.0.14.</span> <span class="nav-text">•
HybrIK: A Hybrid Analytical-Neural Inverse Kinematics Solution for 3D
Human Pose and Shape Estimation paper code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#we-are-more-than-our-joints-predicting-how-3d-bodies-move-paper"><span class="nav-number">1.2.5.0.15.</span> <span class="nav-text">•
We are More than Our Joints: Predicting how 3D Bodies Move paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#smply-benchmarking-3d-human-pose-estimation-in-the-wild-paper"><span class="nav-number">1.2.5.0.16.</span> <span class="nav-text">•
SMPLy Benchmarking 3D Human Pose Estimation in the Wild paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#synthesizing-long-term-3d-human-motion-and-interaction-in-3d-paper"><span class="nav-number">1.2.5.0.17.</span> <span class="nav-text">•
Synthesizing Long-Term 3D Human Motion and Interaction in 3D paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#detailed-3d-human-body-reconstruction-from-multi-view-images-combining-voxel-super-resolution-and-learned-implicit-representation"><span class="nav-number">1.2.5.0.18.</span> <span class="nav-text">•
Detailed 3D Human Body Reconstruction from Multi-view Images Combining
Voxel Super-Resolution and Learned Implicit Representation</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#a-novel-joint-points-and-silhouette-based-method-to-estimate-3d-human-pose-and-shape"><span class="nav-number">1.2.5.0.19.</span> <span class="nav-text">•
A novel joint points and silhouette-based method to estimate 3D human
pose and shape</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#facedet3d-facial-expressions-with-3d-geometric-detail-prediction-paper"><span class="nav-number">1.2.5.0.20.</span> <span class="nav-text">•
FaceDet3D: Facial Expressions with 3D Geometric Detail Prediction paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#nerface-dynamic-neural-radiance-fields-for-monocular-4d-facial-avatar-reconstruction-code"><span class="nav-number">1.2.5.0.21.</span> <span class="nav-text">•
NerFACE: Dynamic Neural Radiance Fields for Monocular 4D Facial Avatar
Reconstruction code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#learning-complex-3d-human-self-contact-paper"><span class="nav-number">1.2.5.0.22.</span> <span class="nav-text">• Learning Complex
3D Human Self-Contact paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#populating-3d-scenes-by-learning-human-scene-interaction-paper"><span class="nav-number">1.2.5.0.23.</span> <span class="nav-text">•
Populating 3D Scenes by Learning Human-Scene Interaction paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#anr-articulated-neural-rendering-for-virtual-avatars-paper"><span class="nav-number">1.2.5.0.24.</span> <span class="nav-text">•
ANR: Articulated Neural Rendering for Virtual Avatars paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#human-mesh-recovery-from-multiple-shots-paper"><span class="nav-number">1.2.5.0.25.</span> <span class="nav-text">• Human Mesh
Recovery from Multiple Shots paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#lifting-2d-stylegan-for-3d-aware-face-generation-paper"><span class="nav-number">1.2.5.0.26.</span> <span class="nav-text">•
Lifting 2D StyleGAN for 3D-Aware Face Generation paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#capturing-detailed-deformations-of-moving-human-bodies-paper"><span class="nav-number">1.2.5.0.27.</span> <span class="nav-text">•
Capturing Detailed Deformations of Moving Human Bodies paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#d-human-pose-shape-and-texture-from-low-resolution-images-and-videos-paper"><span class="nav-number">1.2.5.0.28.</span> <span class="nav-text">•
3D Human Pose, Shape and Texture from Low-Resolution Images and Videos
paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#challencap-monocular-3d-capture-of-challenging-human-performances-using-multi-modal-references-paper"><span class="nav-number">1.2.5.0.29.</span> <span class="nav-text">•
ChallenCap: Monocular 3D Capture of Challenging Human Performances using
Multi-Modal References paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#smplicit-topology-aware-generative-model-for-clothed-people-papercode"><span class="nav-number">1.2.5.0.30.</span> <span class="nav-text">•
SMPLicit: Topology-aware Generative Model for Clothed People paper&amp;code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#learning-high-fidelity-depths-of-dressed-humans-by-watching-social-media-dance-videos-paper"><span class="nav-number">1.2.5.0.31.</span> <span class="nav-text">•
Learning High Fidelity Depths of Dressed Humans by Watching Social Media
Dance Videos paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#neuralhumanfvv-real-time-neural-volumetric-human-performance-rendering-using-rgb-cameras-paper"><span class="nav-number">1.2.5.0.32.</span> <span class="nav-text">•
NeuralHumanFVV: Real-Time Neural Volumetric Human Performance Rendering
using RGB Cameras paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#probabilistic-3d-human-shape-and-pose-estimation-from-multiple-unconstrained-images-in-the-wild-paper"><span class="nav-number">1.2.5.0.33.</span> <span class="nav-text">•
Probabilistic 3D Human Shape and Pose Estimation from Multiple
Unconstrained Images in the Wild paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#dcrowdnet-2d-human-pose-guided-3d-crowd-human-pose-and-shape-estimation-in-the-wild-paper"><span class="nav-number">1.2.5.0.34.</span> <span class="nav-text">•
3DCrowdNet: 2D Human Pose-Guided 3D Crowd Human Pose and Shape
Estimation in the Wild paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#scale-modeling-clothed-humans-with-a-surface-codec-of-articulated-local-elements-paper"><span class="nav-number">1.2.5.0.35.</span> <span class="nav-text">•
SCALE: Modeling Clothed Humans with a Surface Codec of Articulated Local
Elements paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#multi-person-implicit-reconstruction-from-a-single-image-paper"><span class="nav-number">1.2.5.0.36.</span> <span class="nav-text">•
Multi-person Implicit Reconstruction from a Single Image paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#pare-part-attention-regressor-for-3d-human-body-estimation-paper-code"><span class="nav-number">1.2.5.0.37.</span> <span class="nav-text">•
PARE: Part Attention Regressor for 3D Human Body Estimation paper code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#temporal-consistency-loss-for-high-resolution-textured-and-clothed-3d-human-reconstruction-from-monocular-video-paper"><span class="nav-number">1.2.5.0.38.</span> <span class="nav-text">•
Temporal Consistency Loss for High Resolution Textured and Clothed 3D
Human Reconstruction from Monocular Video paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#function4d-real-time-human-volumetric-capture-from-very-sparse-consumer-rgbd-sensors-paper"><span class="nav-number">1.2.5.0.39.</span> <span class="nav-text">•
Function4D: Real-time Human Volumetric Capture from Very Sparse Consumer
RGBD Sensors paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#end-to-end-human-pose-and-mesh-reconstruction-with-transformers-paper"><span class="nav-number">1.2.5.0.40.</span> <span class="nav-text">•
End-to-End Human Pose and Mesh Reconstruction with Transformers paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#revitalizing-optimization-for-3d-human-pose-and-shape-estimation-a-sparse-constrained-formulation-paper"><span class="nav-number">1.2.5.0.41.</span> <span class="nav-text">•
Revitalizing Optimization for 3D Human Pose and Shape Estimation: A
Sparse Constrained Formulation paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#sharp-shape-aware-reconstruction-of-people-in-loose-clothing-paper"><span class="nav-number">1.2.5.0.42.</span> <span class="nav-text">•
SHARP: Shape-Aware Reconstruction of People In Loose Clothing paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#thundr-transformer-based-3d-human-reconstruction-with-markers-paper"><span class="nav-number">1.2.5.0.43.</span> <span class="nav-text">•
THUNDR: Transformer-based 3D HUmaN Reconstruction with Markers paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#deep3dpose-realtime-reconstruction-of-arbitrarily-posed-human-bodies-from-single-rgb-images-paper"><span class="nav-number">1.2.5.0.44.</span> <span class="nav-text">•
Deep3DPose: Realtime Reconstruction of Arbitrarily Posed Human Bodies
from Single RGB Images paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#learning-local-recurrent-models-for-human-mesh-recovery-paper"><span class="nav-number">1.2.5.0.45.</span> <span class="nav-text">•
Learning Local Recurrent Models for Human Mesh Recovery paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#posefusion2-simultaneous-background-reconstruction-and-human-shape-recovery-in-real-time-paper"><span class="nav-number">1.2.5.0.46.</span> <span class="nav-text">•
PoseFusion2: Simultaneous Background Reconstruction and Human Shape
Recovery in Real-time paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#lasor-learning-accurate-3d-human-pose-and-shape-via-synthetic-occlusion-aware-data-and-neural-mesh-rendering-paper"><span class="nav-number">1.2.5.0.47.</span> <span class="nav-text">•
LASOR: Learning Accurate 3D Human Pose and Shape Via Synthetic
Occlusion-Aware Data and Neural Mesh Rendering paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#learning-motion-priors-for-4d-human-body-capture-in-3d-scenes-paper-code"><span class="nav-number">1.2.5.0.48.</span> <span class="nav-text">•
Learning Motion Priors for 4D Human Body Capture in 3D Scenes paper code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#probabilistic-modeling-for-human-mesh-recovery-papercode"><span class="nav-number">1.2.5.0.49.</span> <span class="nav-text">•
Probabilistic Modeling for Human Mesh Recovery paper&amp;code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#dc-gnet-deep-mesh-relation-capturing-graph-convolution-network-for-3d-human-shape-reconstruction-paper"><span class="nav-number">1.2.5.0.50.</span> <span class="nav-text">•
DC-GNet: Deep Mesh Relation Capturing Graph Convolution Network for 3D
Human Shape Reconstruction paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#encoder-decoder-with-multi-level-attention-for-3d-human-shape-and-pose-estimation-paper-code"><span class="nav-number">1.2.5.0.51.</span> <span class="nav-text">•
Encoder-decoder with Multi-level Attention for 3D Human Shape and Pose
Estimation paper code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#action-conditioned-3d-human-motion-synthesis-with-transformer-vae-code"><span class="nav-number">1.2.5.0.52.</span> <span class="nav-text">•
Action-Conditioned 3D Human Motion Synthesis with Transformer VAE code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#learning-to-regress-bodies-from-images-using-differentiable-semantic-rendering-paper"><span class="nav-number">1.2.5.0.53.</span> <span class="nav-text">•
Learning to Regress Bodies from Images using Differentiable Semantic
Rendering paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#deep-two-stream-video-inference-for-human-body-pose-and-shape-estimation-paper"><span class="nav-number">1.2.5.0.54.</span> <span class="nav-text">•
Deep Two-Stream Video Inference for Human Body Pose and Shape Estimation
paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#ultrapose-synthesizing-dense-pose-with-1-billion-points-by-human-body-decoupling-3d-model-paper"><span class="nav-number">1.2.5.0.55.</span> <span class="nav-text">•
UltraPose: Synthesizing Dense Pose with 1 Billion Points by Human-body
Decoupling 3D Model paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#body-size-and-depth-disambiguation-in-multi-person-reconstruction-from-single-images-paper-code"><span class="nav-number">1.2.5.0.56.</span> <span class="nav-text">•
Body Size and Depth Disambiguation in Multi-Person Reconstruction from
Single Images paper
code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#unified-3d-mesh-recovery-of-humans-and-animals-by-learning-animal-exercise-paper"><span class="nav-number">1.2.5.0.57.</span> <span class="nav-text">•
Unified 3D Mesh Recovery of Humans and Animals by Learning Animal
Exercise paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#d-human-shape-and-pose-from-a-single-low-resolution-image-with-self-supervised-learning-paper-code"><span class="nav-number">1.2.5.0.58.</span> <span class="nav-text">•
3D Human Shape and Pose from a Single Low-Resolution Image with
Self-Supervised Learning paper code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#out-of-domain-human-mesh-reconstruction-via-bilevel-online-adaptation-paper-code"><span class="nav-number">1.2.5.0.59.</span> <span class="nav-text">•
Out-of-Domain Human Mesh Reconstruction via Bilevel Online Adaptation paper
code</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#meshletemp-leveraging-the-learnable-vertex-vertex-relationship-to-generalize-human-pose-and-mesh-reconstruction-for-in-the-wild-scenes-paper"><span class="nav-number">1.2.5.1.</span> <span class="nav-text">•
MeshLeTemp: Leveraging the Learnable Vertex-Vertex Relationship to
Generalize Human Pose and Mesh Reconstruction for In-the-Wild Scenes paper</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#monocular-human-shape-and-pose-with-dense-mesh-borne-local-image-features-paper"><span class="nav-number">1.2.5.1.1.</span> <span class="nav-text">•
Monocular Human Shape and Pose with Dense Mesh-borne Local Image
Features paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#human-performance-capture-from-monocular-video-in-the-wild-paper"><span class="nav-number">1.2.5.1.2.</span> <span class="nav-text">•
Human Performance Capture from Monocular Video in the Wild paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#probabilistic-estimation-of-3d-human-shape-and-pose-with-a-semantic-local-parametric-model-paper"><span class="nav-number">1.2.5.1.3.</span> <span class="nav-text">•
Probabilistic Estimation of 3D Human Shape and Pose with a Semantic
Local Parametric Model paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#glamr-global-occlusion-aware-human-mesh-recovery-with-dynamic-cameras-paper-code"><span class="nav-number">1.2.5.1.4.</span> <span class="nav-text">•
GLAMR: Global Occlusion-Aware Human Mesh Recovery with Dynamic Cameras
paper code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#metaavatar-learning-animatable-clothed-human-models-from-few-depth-images-paper-code"><span class="nav-number">1.2.5.1.5.</span> <span class="nav-text">•
MetaAvatar: Learning Animatable Clothed Human Models from Few Depth
Images} paper
code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#egobody-human-body-shape-motion-and-social-interactions-from-head-mounted-devices-paper-code"><span class="nav-number">1.2.5.1.6.</span> <span class="nav-text">•
EgoBody: Human Body Shape, Motion and Social Interactions from
Head-Mounted Devices paper code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#putting-people-in-their-place-monocular-regression-of-3d-people-in-depth-paper"><span class="nav-number">1.2.5.1.7.</span> <span class="nav-text">•
Putting People in their Place: Monocular Regression of 3D People in
Depth paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#multi-initialization-optimization-network-for-accurate-3d-human-pose-and-shape-estimation-paper"><span class="nav-number">1.2.5.1.8.</span> <span class="nav-text">•
Multi-initialization Optimization Network for Accurate 3D Human Pose and
Shape Estimation paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#moothnet-a-plug-and-play-network-for-refining-human-poses-in-videos-paper"><span class="nav-number">1.2.5.1.9.</span> <span class="nav-text">•
moothNet: A Plug-and-Play Network for Refining Human Poses in Videos paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#hspace-synthetic-parametric-humans-animated-in-complex-environments-paper"><span class="nav-number">1.2.5.1.10.</span> <span class="nav-text">•
HSPACE: Synthetic Parametric Humans Animated in Complex Environments paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#votehmr-occlusion-aware-voting-network-for-robust-3d-human-mesh-recovery-from-partial-point-clouds-paper-code"><span class="nav-number">1.2.5.1.11.</span> <span class="nav-text">•
VoteHMR: Occlusion-Aware Voting Network for Robust 3D Human Mesh
Recovery from Partial Point Clouds paper code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#h4d-human-4d-modeling-by-learning-neural-compositional-representation-paper"><span class="nav-number">1.2.5.1.12.</span> <span class="nav-text">•
H4D: Human 4D Modeling by Learning Neural Compositional Representation
paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#capturing-humans-in-motion-temporal-attentive-3d-human-pose-and-shape-estimation-from-monocular-video-paper-code"><span class="nav-number">1.2.5.1.13.</span> <span class="nav-text">•
Capturing Humans in Motion: Temporal-Attentive 3D Human Pose and Shape
Estimation from Monocular Video paper code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#hybridcap-inertia-aid-monocular-capture-of-challenging-human-motions-paper"><span class="nav-number">1.2.5.1.14.</span> <span class="nav-text">•
HybridCap: Inertia-aid Monocular Capture of Challenging Human Motions paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#learning-to-estimate-robust-3d-human-mesh-from-in-the-wild-crowded-scenes-paper-code"><span class="nav-number">1.2.5.1.15.</span> <span class="nav-text">•
Learning to Estimate Robust 3D Human Mesh from In-the-Wild Crowded
Scenes paper code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#bodyslam-joint-camera-localisation-mapping-and-human-motion-tracking-paper"><span class="nav-number">1.2.5.1.16.</span> <span class="nav-text">•
BodySLAM: Joint Camera Localisation, Mapping, and Human Motion Tracking
paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#hulc-3d-human-motion-capture-with-pose-manifold-sampling-and-dense-contact-guidance-paper"><span class="nav-number">1.2.5.1.17.</span> <span class="nav-text">•
HULC: 3D Human Motion Capture with Pose Manifold Sampling and Dense
Contact Guidance paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#learned-vertex-descent-a-new-direction-for-3d-human-model-fitting-paper-code"><span class="nav-number">1.2.5.1.18.</span> <span class="nav-text">•
Learned Vertex Descent: A New Direction for 3D Human Model Fitting paper code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#mug-multi-human-graph-network-for-3d-mesh-reconstruction-from-2d-pose-paper"><span class="nav-number">1.2.5.1.19.</span> <span class="nav-text">•
MUG: Multi-human Graph Network for 3D Mesh Reconstruction from 2D Pose
paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#accurate-3d-body-shape-regression-using-metric-and-semantic-attributes-paper"><span class="nav-number">1.2.5.1.20.</span> <span class="nav-text">•
Accurate 3D Body Shape Regression using Metric and Semantic Attributes
paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#capturing-and-inferring-dense-full-body-human-scene-contact-homepage"><span class="nav-number">1.2.5.1.21.</span> <span class="nav-text">•
Capturing and Inferring Dense Full-Body Human-Scene Contact homepage</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#occluded-human-body-capture-with-self-supervised-spatial-temporal-motion-prior-paper"><span class="nav-number">1.2.5.1.22.</span> <span class="nav-text">•
Occluded Human Body Capture with Self-Supervised Spatial-Temporal Motion
Prior paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#live-stream-temporally-embedded-3d-human-body-pose-and-shape-estimation-paper-code"><span class="nav-number">1.2.5.1.23.</span> <span class="nav-text">•
Live Stream Temporally Embedded 3D Human Body Pose and Shape Estimation
paper code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#cliff-carrying-location-information-in-full-frames-into-human-pose-and-shape-estimation-paper"><span class="nav-number">1.2.5.1.24.</span> <span class="nav-text">•
CLIFF: Carrying Location Information in Full Frames into Human Pose and
Shape Estimation paper</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#d-human-face"><span class="nav-number">1.2.6.</span> <span class="nav-text">3d human face</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#high-fidelity-3d-digital-human-creation-from-rgb-d-selfies-paper-code"><span class="nav-number">1.2.6.0.1.</span> <span class="nav-text">•
High-Fidelity 3D Digital Human Creation from RGB-D Selfies paper code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#styleuv-diverse-and-high-quality-uv-map-generative-model-paper"><span class="nav-number">1.2.6.0.2.</span> <span class="nav-text">•
StyleUV: Diverse and High-quality UV Map Generative Model paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#i3dmm-deep-implicit-3d-morphable-model-of-human-heads-paper"><span class="nav-number">1.2.6.0.3.</span> <span class="nav-text">•
i3DMM: Deep Implicit 3D Morphable Model of Human Heads paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#relightable-3d-head-portraits-from-a-smartphone-video-paper"><span class="nav-number">1.2.6.0.4.</span> <span class="nav-text">•
Relightable 3D Head Portraits from a Smartphone Video paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#learning-compositional-radiance-fields-of-dynamic-human-heads-paper"><span class="nav-number">1.2.6.0.5.</span> <span class="nav-text">•
Learning Compositional Radiance Fields of Dynamic Human Heads paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#sider-single-image-neural-optimization-for-facial-geometric-detail-recovery-paper"><span class="nav-number">1.2.6.0.6.</span> <span class="nav-text">•
SIDER : Single-Image Neural Optimization for Facial Geometric Detail
Recovery paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#synergy-between-3dmm-and-3d-landmarks-for-accurate-3d-facial-geometry-paper-code"><span class="nav-number">1.2.6.0.7.</span> <span class="nav-text">•
Synergy between 3DMM and 3D Landmarks for Accurate 3D Facial Geometry paper code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#high-quality-real-time-facial-capture-based-on-single-camera-paper"><span class="nav-number">1.2.6.0.8.</span> <span class="nav-text">•
HIGH-QUALITY REAL TIME FACIAL CAPTURE BASED ON SINGLE CAMERA paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#self-supervised-high-fidelity-and-re-renderable-3d-facial-reconstruction-from-a-single-imag-paper"><span class="nav-number">1.2.6.0.9.</span> <span class="nav-text">•
Self-supervised High-fidelity and Re-renderable 3D Facial Reconstruction
from a Single Imag paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#generating-diverse-3d-reconstructions-from-a-single-occluded-face-image-paper"><span class="nav-number">1.2.6.0.10.</span> <span class="nav-text">•
Generating Diverse 3D Reconstructions from a Single Occluded Face Image
paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#self-supervised-robustifying-guidance-for-monocular-3d-face-reconstruction-paper"><span class="nav-number">1.2.6.0.11.</span> <span class="nav-text">•
Self-Supervised Robustifying Guidance for Monocular 3D Face
Reconstruction paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#babynet-reconstructing-3d-faces-of-babies-from-uncalibrated-photographs-paper"><span class="nav-number">1.2.6.0.12.</span> <span class="nav-text">•
BabyNet: Reconstructing 3D faces of babies from uncalibrated photographs
paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#s2f2-self-supervised-high-fidelity-face-reconstruction-from-monocular-image-paper"><span class="nav-number">1.2.6.0.13.</span> <span class="nav-text">•
S2F2: Self-Supervised High Fidelity Face Reconstruction from Monocular
Image paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#facial-geometric-detail-recovery-via-implicit-representation-paper-code"><span class="nav-number">1.2.6.0.14.</span> <span class="nav-text">•
Facial Geometric Detail Recovery via Implicit Representation paper code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#beyond-3dmm-learning-to-capture-high-fidelity-3d-face-shape-paper"><span class="nav-number">1.2.6.0.15.</span> <span class="nav-text">•
Beyond 3DMM: Learning to Capture High-fidelity 3D Face Shape paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#from-2d-images-to-3d-model-weakly-supervised-multi-view-face-reconstruction-with-deep-fusion-paper"><span class="nav-number">1.2.6.0.16.</span> <span class="nav-text">•
From 2D Images to 3D Model: Weakly Supervised Multi-View Face
Reconstruction with Deep Fusion paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#f3d-face-reconstruction-with-dense-landmarks-paper"><span class="nav-number">1.2.6.0.17.</span> <span class="nav-text">• F3D face
reconstruction with dense landmarks paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#emoca-emotion-driven-monocular-face-capture-and-animation-paper"><span class="nav-number">1.2.6.0.18.</span> <span class="nav-text">•
EMOCA: Emotion Driven Monocular Face Capture and Animation paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#single-image-3d-face-reconstruction-under-perspective-projection-paper"><span class="nav-number">1.2.6.0.19.</span> <span class="nav-text">•
Single-Image 3D Face Reconstruction under Perspective Projection paper</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#d-human-head"><span class="nav-number">1.2.7.</span> <span class="nav-text">3d human head</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#deca-detailed-expression-capture-and-animation-paper-code"><span class="nav-number">1.2.7.0.1.</span> <span class="nav-text">•
DECA: Detailed Expression Capture and Animation paper code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#pixel-codec-avatars-paper"><span class="nav-number">1.2.7.0.2.</span> <span class="nav-text">• Pixel Codec Avatars paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#h3d-net-few-shot-high-fidelity-3d-head-reconstruction-paper"><span class="nav-number">1.2.7.0.3.</span> <span class="nav-text">•
H3D-Net: Few-Shot High-Fidelity 3D Head Reconstruction paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#towards-metrical-reconstruction-of-human-faces-paper-code"><span class="nav-number">1.2.7.0.4.</span> <span class="nav-text">•Towards
Metrical Reconstruction of Human Faces paper code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#data-driven-3d-human-head-reconstruction-paper"><span class="nav-number">1.2.7.0.5.</span> <span class="nav-text">•Data-driven 3D
human head reconstruction paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#dynamic-3d-avatar-creation-from-hand-held-video-input-acm-paper"><span class="nav-number">1.2.7.0.6.</span> <span class="nav-text">•Dynamic
3D avatar creation from hand-held video input, ACM paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#realistic-one-shot-mesh-based-head-avatars-paper"><span class="nav-number">1.2.7.0.7.</span> <span class="nav-text">•Realistic
One-shot Mesh-based Head Avatars paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#authentic-volumetric-avatars-from-a-phone-scan-paper"><span class="nav-number">1.2.7.0.8.</span> <span class="nav-text">•Authentic
Volumetric Avatars from a Phone Scan paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#neural-head-avatars-from-monocular-rgb-videos-homepage"><span class="nav-number">1.2.7.0.9.</span> <span class="nav-text">•Neural
Head Avatars from Monocular RGB Videos homepage</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#towards-metrical-reconstruction-of-human-faces-homepage"><span class="nav-number">1.2.7.0.10.</span> <span class="nav-text">•Towards
Metrical Reconstruction of Human Faces homepage</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#d-human-hand"><span class="nav-number">1.2.8.</span> <span class="nav-text">3D human hand</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#active-learning-for-bayesian-3d-hand-pose-estimation-paper-code"><span class="nav-number">1.2.8.0.1.</span> <span class="nav-text">•
Active Learning for Bayesian 3D Hand Pose Estimation paper code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#multi-view-consistency-loss-for-improved-single-image-3d-reconstruction-of-clothed-people-paper-code-1"><span class="nav-number">1.2.8.0.2.</span> <span class="nav-text">•
Multi-View Consistency Loss for Improved Single-Image 3D Reconstruction
of Clothed People paper
code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#eventhands-real-time-neural-3d-hand-reconstruction-from-an-event-stream"><span class="nav-number">1.2.8.0.3.</span> <span class="nav-text">•
EventHands: Real-Time Neural 3D Hand Reconstruction from an Event
Stream</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#monocular-real-time-full-body-capture-with-inter-part-correlations"><span class="nav-number">1.2.8.0.4.</span> <span class="nav-text">•
Monocular Real-time Full Body Capture with Inter-part Correlations</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#im2mesh-gan-accurate-3d-hand-mesh-recovery-from-a-single-rgb-image"><span class="nav-number">1.2.8.0.5.</span> <span class="nav-text">•
Im2Mesh GAN: Accurate 3D Hand Mesh Recovery from a Single RGB Image</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#handtailor-towards-high-precision-monocular-3d-hand-recovery-paper-code"><span class="nav-number">1.2.8.0.6.</span> <span class="nav-text">•
HandTailor: Towards High-Precision Monocular 3D Hand Recovery paper code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#camera-space-hand-mesh-recovery-via-semantic-aggregation-and-adaptive-2d-1d-registration-paper-code"><span class="nav-number">1.2.8.0.7.</span> <span class="nav-text">•
Camera-Space Hand Mesh Recovery via Semantic Aggregation and Adaptive
2D-1D Registration paper code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#model-based-3d-hand-reconstruction-via-self-supervised-learning-paper-code"><span class="nav-number">1.2.8.0.8.</span> <span class="nav-text">•
Model-based 3D Hand Reconstruction via Self-Supervised Learning paper code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#action-conditioned-3d-human-motion-synthesis-with-transformer-vae-paper"><span class="nav-number">1.2.8.0.9.</span> <span class="nav-text">•
Action-Conditioned 3D Human Motion Synthesis with Transformer VAE paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#semi-supervised-3d-hand-object-poses-estimation-with-interactions-in-time-paper"><span class="nav-number">1.2.8.0.10.</span> <span class="nav-text">•
Semi-Supervised 3D Hand-Object Poses Estimation with Interactions in
Time paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#rgb2hands-real-time-tracking-of-3d-hand-interactions-from-monocular-rgb-video-paper"><span class="nav-number">1.2.8.0.11.</span> <span class="nav-text">•
RGB2Hands: Real-Time Tracking of 3D Hand Interactions from Monocular RGB
Video paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#artiboost-boosting-articulated-3d-hand-object-pose-estimation-via-online-exploration-and-synthesis-paper-code"><span class="nav-number">1.2.8.0.12.</span> <span class="nav-text">•
ArtiBoost: Boosting Articulated 3D Hand-Object Pose Estimation via
Online Exploration and Synthesis paper code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#monocular-3d-reconstruction-of-interacting-hands-via-collision-aware-factorized-refinements-paper-code"><span class="nav-number">1.2.8.0.13.</span> <span class="nav-text">•
Monocular 3D Reconstruction of Interacting Hands via Collision-Aware
Factorized Refinements paper code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#dynamic-iterative-refinement-for-efficient-3d-hand-pose-estimation-paper"><span class="nav-number">1.2.8.0.14.</span> <span class="nav-text">•
Dynamic Iterative Refinement for Efficient 3D Hand Pose Estimation paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#semi-supervised-3d-hand-shape-and-pose-estimation-with-label-propagation-paper"><span class="nav-number">1.2.8.0.15.</span> <span class="nav-text">•
Semi-Supervised 3D Hand Shape and Pose Estimation with Label Propagation
paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#mobrecon-mobile-friendly-hand-mesh-reconstruction-from-monocular-image-paper-code"><span class="nav-number">1.2.8.0.16.</span> <span class="nav-text">•
MobRecon: Mobile-Friendly Hand Mesh Reconstruction from Monocular Image
paper code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#consistent-3d-hand-reconstruction-in-video-via-self-supervised-learning-paper"><span class="nav-number">1.2.8.0.17.</span> <span class="nav-text">•
Consistent 3D Hand Reconstruction in Video via Self-Supervised Learning
paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#interacting-attention-graph-for-single-image-two-hand-reconstruction-paper"><span class="nav-number">1.2.8.0.18.</span> <span class="nav-text">•
Interacting Attention Graph for Single Image Two-Hand Reconstruction paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#handoccnet-occlusion-robust-3d-hand-mesh-estimation-network-paper-code"><span class="nav-number">1.2.8.0.19.</span> <span class="nav-text">•
HandOccNet: Occlusion-Robust 3D Hand Mesh Estimation Network paper code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#toch-spatio-temporal-object-correspondence-to-hand-for-motion-refinement-paper"><span class="nav-number">1.2.8.0.20.</span> <span class="nav-text">•
TOCH: Spatio-Temporal Object Correspondence to Hand for Motion
Refinement paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#end-to-end-3d-hand-pose-estimation-from-stereo-cameras-paper"><span class="nav-number">1.2.8.0.21.</span> <span class="nav-text">•
End-to-End 3D Hand Pose Estimation from Stereo Cameras paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#efficient-annotation-and-learning-for-3dhand-pose-estimation-a-survey-paper"><span class="nav-number">1.2.8.0.22.</span> <span class="nav-text">•
Efficient Annotation and Learning for 3DHand Pose Estimation: A Survey
paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#d-interacting-hand-pose-estimation-by-hand-de-occlusion-and-removal-code"><span class="nav-number">1.2.8.0.23.</span> <span class="nav-text">•
3D Interacting Hand Pose Estimation by Hand De-occlusion and Removal code</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#d-hair"><span class="nav-number">1.2.9.</span> <span class="nav-text">3d hair</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#neuralhdhair-automatic-high-fidelity-hair-modeling-from-a-single-image-using-implicit-neural-representations-paper"><span class="nav-number">1.2.9.0.1.</span> <span class="nav-text">•
NeuralHDHair: Automatic High-fidelity Hair Modeling from a Single Image
Using Implicit Neural Representations paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#d-hair-synthesis-using-volumetric-variational-autoencoders"><span class="nav-number">1.2.9.0.2.</span> <span class="nav-text">•3D
hair synthesis using volumetric variational autoencoders</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#ao-cnn-filament-aware-hair-reconstruction-based-on-volumetric-vector-fields"><span class="nav-number">1.2.9.0.3.</span> <span class="nav-text">•AO-CNN:
filament-aware hair reconstruction based on volumetric vector
fields</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#neural-strands-learning-hair-geometry-and-appearance-from-multi-view-images-paper"><span class="nav-number">1.2.9.0.4.</span> <span class="nav-text">•Neural
Strands: Learning Hair Geometry and Appearance from Multi-View Images paper</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#d-teeth"><span class="nav-number">1.2.10.</span> <span class="nav-text">3d teeth</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#model-based-teeth-reconstruction-paper"><span class="nav-number">1.2.10.0.1.</span> <span class="nav-text">• Model-based teeth
reconstruction paper</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#d-eyelids"><span class="nav-number">1.2.11.</span> <span class="nav-text">3d eyelids</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#real-time-3d-eyelids-tracking-from-semantic-edges-paper"><span class="nav-number">1.2.11.0.1.</span> <span class="nav-text">•
Real-time 3D Eyelids Tracking from Semantic Edges paper</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#related"><span class="nav-number">1.3.</span> <span class="nav-text">related</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#human-mattting"><span class="nav-number">1.3.1.</span> <span class="nav-text">human mattting</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#real-time-high-resolution-background-matting-code"><span class="nav-number">1.3.1.0.1.</span> <span class="nav-text">• Real-Time
High-Resolution Background Matting code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#real-time-monocular-human-depth-estimation-and-segmentation-on-embedded-systems-paper"><span class="nav-number">1.3.1.0.2.</span> <span class="nav-text">•
Real-Time Monocular Human Depth Estimation and Segmentation on Embedded
Systems paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#deepsportlab-a-unified-framework-for-ball-detection-player-instance-segmentation-and-pose-estimation-in-team-sports-scenes-paper"><span class="nav-number">1.3.1.0.3.</span> <span class="nav-text">•
DeepSportLab: a Unified Framework for Ball Detection, Player Instance
Segmentation and Pose Estimation in Team Sports Scenes paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#pp-humanseg-connectivity-aware-portrait-segmentation-with-a-large-scale-teleconferencing-video-dataset-paper-code"><span class="nav-number">1.3.1.0.4.</span> <span class="nav-text">•
PP-HumanSeg: Connectivity-Aware Portrait Segmentation with a Large-Scale
Teleconferencing Video Dataset paper code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#portrait-segmentation-using-deep-learning-paper"><span class="nav-number">1.3.1.0.5.</span> <span class="nav-text">• Portrait
Segmentation Using Deep Learning paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#human-instance-matting-via-mutual-guidance-and-multi-instance-refinement-paper-code"><span class="nav-number">1.3.1.0.6.</span> <span class="nav-text">•
Human Instance Matting via Mutual Guidance and Multi-Instance Refinement
paper code</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#pose-estimation"><span class="nav-number">1.3.2.</span> <span class="nav-text">pose estimation</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#canonpose-self-supervised-monocular-3d-human-pose-estimation-in-the-wild-paper"><span class="nav-number">1.3.2.0.1.</span> <span class="nav-text">•
CanonPose: Self-Supervised Monocular 3D Human Pose Estimation in the
Wild paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#active-learning-for-bayesian-3d-hand-pose-estimation-paper-code-1"><span class="nav-number">1.3.2.0.2.</span> <span class="nav-text">•
Active Learning for Bayesian 3D Hand Pose Estimation paper code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#a-nerf-surface-free-human-3d-pose-refinement-via-neural-rendering"><span class="nav-number">1.3.2.0.3.</span> <span class="nav-text">•
A-NeRF: Surface-free Human 3D Pose Refinement via Neural Rendering</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#handsformer-keypoint-transformer-for-monocular-3d-pose-estimation-of-hands-and-object-in-interaction-paper"><span class="nav-number">1.3.2.0.4.</span> <span class="nav-text">•
HandsFormer: Keypoint Transformer for Monocular 3D Pose Estimation of
Hands and Object in Interaction paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#humor-3d-human-motion-model-for-robust-pose-estimation-paper"><span class="nav-number">1.3.2.0.5.</span> <span class="nav-text">•
HuMoR: 3D Human Motion Model for Robust Pose Estimation paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#multi-person-extreme-motion-prediction-with-cross-interaction-attention-paper"><span class="nav-number">1.3.2.0.6.</span> <span class="nav-text">•
Multi-Person Extreme Motion Prediction with Cross-Interaction Attention
paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#voxeltrack-multi-person-3d-human-pose-estimation-and-tracking-in-the-wild-paper"><span class="nav-number">1.3.2.0.7.</span> <span class="nav-text">•
VoxelTrack: Multi-Person 3D Human Pose Estimation and Tracking in the
Wild paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#gravity-aware-monocular-3d-human-object-reconstruction-papercode"><span class="nav-number">1.3.2.0.8.</span> <span class="nav-text">•
Gravity-Aware Monocular 3D Human-Object Reconstruction paper&amp;code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#densepose-3d-lifting-canonical-surface-maps-of-articulated-objects-to-the-third-dimension-paper"><span class="nav-number">1.3.2.0.9.</span> <span class="nav-text">•
DensePose 3D: Lifting Canonical Surface Maps of Articulated Objects to
the Third Dimension paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#graph-based-3d-multi-person-pose-estimation-using-multi-view-images-paper"><span class="nav-number">1.3.2.0.10.</span> <span class="nav-text">•
Graph-Based 3D Multi-Person Pose Estimation Using Multi-View Images paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#learning-dynamical-human-joint-affinity-for-3d-pose-estimation-in-videos-paper"><span class="nav-number">1.3.2.0.11.</span> <span class="nav-text">•
Learning Dynamical Human-Joint Affinity for 3D Pose Estimation in Videos
paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#physics-based-human-motion-estimation-and-synthesis-from-videos-paper"><span class="nav-number">1.3.2.0.12.</span> <span class="nav-text">•
Physics-based Human Motion Estimation and Synthesis from Videos paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#real-time-low-cost-multi-person-3d-pose-estimation-paper"><span class="nav-number">1.3.2.0.13.</span> <span class="nav-text">•
Real-time, low-cost multi-person 3D pose estimation paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#direct-multi-view-multi-person-3d-human-pose-estimation-paper-code"><span class="nav-number">1.3.2.0.14.</span> <span class="nav-text">•
Direct Multi-view Multi-person 3D Human Pose Estimation paper code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#rethinking-keypoint-representations-modeling-keypoints-and-poses-as-objects-for-multi-person-human-pose-estimation-code"><span class="nav-number">1.3.2.0.15.</span> <span class="nav-text">•
Rethinking Keypoint Representations: Modeling Keypoints and Poses as
Objects for Multi-Person Human Pose Estimation code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#camera-distortion-aware-3d-human-pose-estimation-in-video-with-optimization-based-meta-learning-paper"><span class="nav-number">1.3.2.0.16.</span> <span class="nav-text">•
Camera Distortion-aware 3D Human Pose Estimation in Video with
Optimization-based Meta-Learning paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#in-bed-human-pose-estimation-from-unseen-and-privacy-preserving-image-domains-paper"><span class="nav-number">1.3.2.0.17.</span> <span class="nav-text">•
In-Bed Human Pose Estimation from Unseen and Privacy-Preserving Image
Domains paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#camera-motion-agnostic-3d-human-pose-estimation-paper-code"><span class="nav-number">1.3.2.0.18.</span> <span class="nav-text">•
Camera Motion Agnostic 3D Human Pose Estimation paper code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#elepose-unsupervised-3d-human-pose-estimation-by-predicting-camera-elevation-and-learning-normalizing-flows-on-2d-poses-paper"><span class="nav-number">1.3.2.0.19.</span> <span class="nav-text">•
ElePose: Unsupervised 3D Human Pose Estimation by Predicting Camera
Elevation and Learning Normalizing Flows on 2D Poses paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#multi-modal-3d-human-pose-estimation-with-2d-weak-supervision-in-autonomous-driving-paper"><span class="nav-number">1.3.2.0.20.</span> <span class="nav-text">•
Multi-modal 3D Human Pose Estimation with 2D Weak Supervision in
Autonomous Driving paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#adaptpose-cross-dataset-adaptation-for-3d-human-pose-estimation-by-learnable-motion-generation-paper"><span class="nav-number">1.3.2.0.21.</span> <span class="nav-text">•
AdaptPose: Cross-Dataset Adaptation for 3D Human Pose Estimation by
Learnable Motion Generation paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#flag-flow-based-3d-avatar-generation-from-sparse-observations-paper"><span class="nav-number">1.3.2.0.22.</span> <span class="nav-text">•
FLAG: Flow-based 3D Avatar Generation from Sparse Observations paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#pose-mum-reinforcing-key-points-relationship-for-semi-supervised-human-pose-estimation-paper"><span class="nav-number">1.3.2.0.23.</span> <span class="nav-text">•
Pose-MUM : Reinforcing Key Points Relationship for Semi-Supervised Human
Pose Estimation paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#distribution-aware-single-stage-models-for-multi-person-3d-pose-estimation-paper"><span class="nav-number">1.3.2.0.24.</span> <span class="nav-text">•
Distribution-Aware Single-Stage Models for Multi-Person 3D Pose
Estimation paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#p-stmo-pre-trained-spatial-temporal-many-to-one-model-for-3d-human-pose-estimation-paper-code"><span class="nav-number">1.3.2.0.25.</span> <span class="nav-text">•
P-STMO: Pre-Trained Spatial Temporal Many-to-One Model for 3D Human Pose
Estimation paper code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#posepipe-open-source-human-pose-estimation-pipeline-for-clinical-research-paper-code"><span class="nav-number">1.3.2.0.26.</span> <span class="nav-text">•
PosePipe: Open-Source Human Pose Estimation Pipeline for Clinical
Research paper code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#d-human-pose-estimation-using-m%C3%B6bius-graph-convolutional-networks-paper"><span class="nav-number">1.3.2.0.27.</span> <span class="nav-text">•
3D Human Pose Estimation Using Möbius Graph Convolutional Networks paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#ray3d-ray-based-3d-human-pose-estimation-for-monocular-absolute-3d-localization-paper-code"><span class="nav-number">1.3.2.0.28.</span> <span class="nav-text">•
Ray3D: ray-based 3D human pose estimation for monocular absolute 3D
localization paper
code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#yolo-pose-enhancing-yolo-for-multi-person-pose-estimation-using-object-keypoint-similarity-loss-paper-code"><span class="nav-number">1.3.2.0.29.</span> <span class="nav-text">•
YOLO-Pose: Enhancing YOLO for Multi Person Pose Estimation Using Object
Keypoint Similarity Loss paper code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#permutation-invariant-relational-network-for-multi-person-3d-pose-estimation-paper"><span class="nav-number">1.3.2.0.30.</span> <span class="nav-text">•
Permutation-Invariant Relational Network for Multi-person 3D Pose
Estimation paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#non-local-latent-relation-distillation-for-self-adaptive-3d-human-pose-estimation-paper"><span class="nav-number">1.3.2.0.31.</span> <span class="nav-text">•
Non-Local Latent Relation Distillation for Self-Adaptive 3D Human Pose
Estimation paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#aligning-silhouette-topology-for-self-adaptive-3d-human-pose-recovery-paper"><span class="nav-number">1.3.2.0.32.</span> <span class="nav-text">•
Aligning Silhouette Topology for Self-Adaptive 3D Human Pose Recovery paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#dite-hrnet-dynamic-lightweight-high-resolution-network-for-human-pose-estimation-paper"><span class="nav-number">1.3.2.0.33.</span> <span class="nav-text">•
Dite-HRNet: Dynamic Lightweight High-Resolution Network for Human Pose
Estimation paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#pedrecnet-multi-task-deep-neural-network-for-full-3d-human-pose-and-orientation-estimation-paper"><span class="nav-number">1.3.2.0.34.</span> <span class="nav-text">•
PedRecNet: Multi-task deep neural network for full 3D human pose and
orientation estimation paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#teaching-independent-parts-separately-tips-gan-improving-accuracy-and-stability-in-unsupervised-adversarial-2d-to-3d-human-pose-estimation-paper"><span class="nav-number">1.3.2.0.35.</span> <span class="nav-text">•
&quot;Teaching Independent Parts Separately&quot; (TIPS-GAN) : Improving Accuracy
and Stability in Unsupervised Adversarial 2D to 3D Human Pose Estimation
paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#lightweight-human-pose-estimation-using-heatmap-weighting-loss-paper"><span class="nav-number">1.3.2.0.36.</span> <span class="nav-text">•Lightweight
Human Pose Estimation Using Heatmap-Weighting Loss paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#vtp-volumetric-transformer-for-multi-view-multi-person-3d-pose-estimation-paper"><span class="nav-number">1.3.2.0.37.</span> <span class="nav-text">•VTP:
Volumetric Transformer for Multi-view Multi-person 3D Pose Estimation paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#location-free-human-pose-estimation-paper"><span class="nav-number">1.3.2.0.38.</span> <span class="nav-text">•Location-free Human
Pose Estimation paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#trajectory-optimization-for-physics-based-reconstruction-of-3d-human-pose-from-monocular-video-paper"><span class="nav-number">1.3.2.0.39.</span> <span class="nav-text">•Trajectory
Optimization for Physics-Based Reconstruction of 3d Human Pose from
Monocular Video paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#spgnet-spatial-projection-guided-3d-human-pose-estimation-in-low-dimensional-space-paper"><span class="nav-number">1.3.2.0.40.</span> <span class="nav-text">•SPGNet:
Spatial Projection Guided 3D Human Pose Estimation in Low Dimensional
Space paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#graphmlp-a-graph-mlp-like-architecture-for-3d-human-pose-estimation-paper"><span class="nav-number">1.3.2.0.41.</span> <span class="nav-text">•GraphMLP:
A Graph MLP-Like Architecture for 3D Human Pose Estimation paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#blazepose-ghum-holistic-real-time-3d-human-landmarks-and-pose-estimation-paper"><span class="nav-number">1.3.2.0.42.</span> <span class="nav-text">•BlazePose
GHUM Holistic: Real-time 3D Human Landmarks and Pose Estimation paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#mutual-adaptive-reasoning-for-monocular-3d-multi-person-pose-estimation-paper"><span class="nav-number">1.3.2.0.43.</span> <span class="nav-text">•Mutual
Adaptive Reasoning for Monocular 3D Multi-Person Pose Estimation paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#human-keypoint-detection-for-close-proximity-human-robot-interaction-paper"><span class="nav-number">1.3.2.0.44.</span> <span class="nav-text">•Human
keypoint detection for close proximity human-robot interaction paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#virtualpose-learning-generalizable-3d-human-pose-models-from-virtual-data-paper"><span class="nav-number">1.3.2.0.45.</span> <span class="nav-text">•VirtualPose:
Learning Generalizable 3D Human Pose Models from Virtual Data paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#d-clothed-human-reconstruction-in-the-wild-paper-code"><span class="nav-number">1.3.2.0.46.</span> <span class="nav-text">•3D
Clothed Human Reconstruction in the Wild paper code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#efficient-and-accurate-skeleton-based-two-person-interaction-recognition-using-inter--and-intra-body-graphs-paper"><span class="nav-number">1.3.2.0.47.</span> <span class="nav-text">•EFFICIENT
AND ACCURATE SKELETON-BASED TWO-PERSON INTERACTION RECOGNITION USING
INTER- AND INTRA-BODY GRAPHS paper</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#registration"><span class="nav-number">1.3.3.</span> <span class="nav-text">registration</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#loopreg-self-supervised-learning-of-implicit-surface-correspondences-pose-and-shape-for-3d-human-mesh-registration-paper-code"><span class="nav-number">1.3.3.0.1.</span> <span class="nav-text">•
LoopReg: Self-supervised Learning of Implicit Surface Correspondences,
Pose and Shape for 3D Human Mesh Registration paper
code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#neural-deformation-graphs-for-globally-consistent-non-rigid-reconstruction-paper-code"><span class="nav-number">1.3.3.0.2.</span> <span class="nav-text">•
Neural Deformation Graphs for Globally-consistent Non-rigid
Reconstruction paper
code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#farm-functional-automatic-registration-method-for-3d-human-bodies-paper-code"><span class="nav-number">1.3.3.0.3.</span> <span class="nav-text">•
FARM: Functional Automatic Registration Method for 3D Human Bodies paper code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#locally-aware-piecewise-transformation-fields-for-3d-human-mesh-registration-paper"><span class="nav-number">1.3.3.0.4.</span> <span class="nav-text">•
Locally Aware Piecewise Transformation Fields for 3D Human Mesh
Registration paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#unsupervised-3d-human-mesh-recovery-from-noisy-point-clouds-paper"><span class="nav-number">1.3.3.0.5.</span> <span class="nav-text">•
Unsupervised 3D Human Mesh Recovery from Noisy Point Clouds paper</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#correspondence"><span class="nav-number">1.3.4.</span> <span class="nav-text">correspondence</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#humangps-geodesic-preserving-feature-for-dense-human-correspondences-paper"><span class="nav-number">1.3.4.0.1.</span> <span class="nav-text">•
HumanGPS: Geodesic PreServing Feature for Dense Human Correspondences paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#bodymap-learning-full-body-dense-correspondence-map-paper"><span class="nav-number">1.3.4.0.2.</span> <span class="nav-text">•BodyMap:
Learning Full-Body Dense Correspondence Map paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#corri2p-deep-image-to-point-cloud-registration-via-dense-correspondence-paper"><span class="nav-number">1.3.4.0.3.</span> <span class="nav-text">•CorrI2P:
Deep Image-to-Point Cloud Registration via Dense Correspondence paper</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#application"><span class="nav-number">1.3.5.</span> <span class="nav-text">application</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#one-shot-free-view-neural-talking-head-synthesis-for-video-conferencing-paper"><span class="nav-number">1.3.5.0.1.</span> <span class="nav-text">•
One-Shot Free-View Neural Talking-Head Synthesis for Video Conferencing
paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#lipsync3d-data-efficient-learning-of-personalized-3d-talking-faces-from-video-using-pose-and-lighting-normalization-paper"><span class="nav-number">1.3.5.0.2.</span> <span class="nav-text">•
LipSync3D: Data-Efficient Learning of Personalized 3D Talking Faces from
Video using Pose and Lighting Normalization paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#arshoe-real-time-augmented-reality-shoe-try-on-system-on-smartphones-paper"><span class="nav-number">1.3.5.0.3.</span> <span class="nav-text">•
ARShoe: Real-Time Augmented Reality Shoe Try-on System on Smartphones paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#a-neural-anthropometer-learning-from-body-dimensions-computed-on-human-3d-meshes-paper"><span class="nav-number">1.3.5.0.4.</span> <span class="nav-text">•
A Neural Anthropometer Learning from Body Dimensions Computed on Human
3D Meshes paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#robust-3d-garment-digitization-from-monocular-2d-images-for-3d-virtual-try-on-systems-paper"><span class="nav-number">1.3.5.0.5.</span> <span class="nav-text">•
Robust 3D Garment Digitization from Monocular 2D Images for 3D Virtual
Try-On Systems paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#single-image-human-body-reshaping-with-deep-neural-networks-paper"><span class="nav-number">1.3.5.0.6.</span> <span class="nav-text">•
Single-image Human-body Reshaping with Deep Neural Networks paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#style-based-global-appearance-flow-for-virtual-try-on-paper"><span class="nav-number">1.3.5.0.7.</span> <span class="nav-text">•
Style-Based Global Appearance Flow for Virtual Try-On paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#monitoring-of-pigmented-skin-lesions-using-3d-whole-body-imaging-paper"><span class="nav-number">1.3.5.0.8.</span> <span class="nav-text">•
Monitoring of Pigmented Skin Lesions Using 3D Whole Body Imaging paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#estimation-of-3d-body-shape-and-clothing-measurements-from-frontaland-side-view-images-paper"><span class="nav-number">1.3.5.0.9.</span> <span class="nav-text">•
ESTIMATION OF 3D BODY SHAPE AND CLOTHING MEASUREMENTS FROM FRONTALAND
SIDE-VIEW IMAGES paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#dressing-avatars-deep-photorealistic-appearance-for-physically-simulated-clothing-paper"><span class="nav-number">1.3.5.0.10.</span> <span class="nav-text">•
Dressing Avatars: Deep Photorealistic Appearance for Physically
Simulated Clothing paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#aifit-automatic-3d-human-interpretable-feedback-models-for-fitness-training-paper"><span class="nav-number">1.3.5.0.11.</span> <span class="nav-text">•
AIFit: Automatic 3D Human-Interpretable Feedback Models for Fitness
Training paper</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#texture"><span class="nav-number">1.3.6.</span> <span class="nav-text">texture</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#spatiotemporal-texture-reconstruction-for-dynamic-objects-using-a-single-rgb-d-camera-paper"><span class="nav-number">1.3.6.0.1.</span> <span class="nav-text">•
Spatiotemporal Texture Reconstruction for Dynamic Objects Using a Single
RGB-D Camera paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#semi-supervised-synthesis-of-high-resolution-editable-textures-for-3d-humans-paper"><span class="nav-number">1.3.6.0.2.</span> <span class="nav-text">•
Semi-supervised Synthesis of High-Resolution Editable Textures for 3D
Humans paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#stylepeople-a-generative-model-of-fullbody-human-avatars-paper-code"><span class="nav-number">1.3.6.0.3.</span> <span class="nav-text">•
StylePeople: A Generative Model of Fullbody Human Avatars paper code</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#skin"><span class="nav-number">1.3.7.</span> <span class="nav-text">skin</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#heterskinnet-a-heterogeneous-network-for-skin-weights-prediction-paper"><span class="nav-number">1.3.7.0.1.</span> <span class="nav-text">•
HeterSkinNet: A Heterogeneous Network for Skin Weights Prediction paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#snarf-differentiable-forward-skinning-for-animating-non-rigid-neural-implicit-shapes-paper"><span class="nav-number">1.3.7.0.2.</span> <span class="nav-text">•
SNARF: Differentiable Forward Skinning for Animating Non-Rigid Neural
Implicit Shapes paper</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#uncategorized"><span class="nav-number">1.3.8.</span> <span class="nav-text">uncategorized</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#fully-convolutional-graph-neural-networks-for-parametric-virtual-try-on-paper"><span class="nav-number">1.3.8.0.1.</span> <span class="nav-text">•
Fully Convolutional Graph Neural Networks for Parametric Virtual Try-On
paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#tailornet-predicting-clothing-in-3d-as-a-function-of-human-pose-shape-and-garment-style-paper-code"><span class="nav-number">1.3.8.0.2.</span> <span class="nav-text">•
TailorNet: Predicting Clothing in 3D as a Function of Human Pose, Shape
and Garment Style paper
code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#dbooster-3d-body-shape-and-texture-recovery-paper"><span class="nav-number">1.3.8.0.3.</span> <span class="nav-text">• 3DBooSTeR:
3D Body Shape and Texture Recovery paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#neural-3d-clothes-retargeting-from-a-single-image-paper"><span class="nav-number">1.3.8.0.4.</span> <span class="nav-text">•
Neural 3D Clothes Retargeting from a Single Image paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#single-image-full-body-human-relighting-paper"><span class="nav-number">1.3.8.0.5.</span> <span class="nav-text">• Single-image
Full-body Human Relighting paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#ibutter-neural-interactive-bullet-time-generator-for-human-free-viewpoint-rendering-paper"><span class="nav-number">1.3.8.0.6.</span> <span class="nav-text">•
iButter: Neural Interactive Bullet Time Generator for Human
Free-viewpoint Rendering paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#a-riemannian-framework-for-analysis-of-human-body-surface-paper"><span class="nav-number">1.3.8.0.7.</span> <span class="nav-text">• A
Riemannian Framework for Analysis of Human Body Surface paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#the-power-of-points-for-modeling-humans-in-clothing-papercode"><span class="nav-number">1.3.8.0.8.</span> <span class="nav-text">•
The Power of Points for Modeling Humans in Clothing paper&amp;code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#neural-human-deformation-transfer-paper"><span class="nav-number">1.3.8.0.9.</span> <span class="nav-text">• Neural Human
Deformation Transfer paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#d-human-texture-estimation-from-a-single-image-with-transformers-paper"><span class="nav-number">1.3.8.0.10.</span> <span class="nav-text">•
3D Human Texture Estimation from a Single Image with Transformers paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#learning-to-predict-diverse-human-motions-from-a-single-image-via-mixture-density-networks-paper"><span class="nav-number">1.3.8.0.11.</span> <span class="nav-text">•
Learning to Predict Diverse Human Motions from a Single Image via
Mixture Density Networks paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#zflow-gated-appearance-flow-based-virtual-try-on-with-3d-priors-paper"><span class="nav-number">1.3.8.0.12.</span> <span class="nav-text">•
ZFlow: Gated Appearance Flow-based Virtual Try-on with 3D Priors paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#a-shading-guided-generative-implicit-model-for-shape-accurate-3d-aware-image-synthesis-paper-code"><span class="nav-number">1.3.8.0.13.</span> <span class="nav-text">•
A Shading-Guided Generative Implicit Model for Shape-Accurate 3D-Aware
Image Synthesis paper code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#action2video-generating-videos-of-human-3d-actions-paper"><span class="nav-number">1.3.8.0.14.</span> <span class="nav-text">•
Action2video: Generating Videos of Human 3D Actions paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#garment4d-garment-reconstruction-from-point-cloud-sequences-paper-code"><span class="nav-number">1.3.8.0.15.</span> <span class="nav-text">•
Garment4D: Garment Reconstruction from Point Cloud Sequences paper
code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#adg-pose-automated-dataset-generation-for-real-world-human-pose-estimation-paper-code"><span class="nav-number">1.3.8.0.16.</span> <span class="nav-text">•
ADG-Pose: Automated Dataset Generation for Real-World Human Pose
Estimation paper code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#diffusionnet-discretization-agnostic-learning-on-surfaces-paper"><span class="nav-number">1.3.8.0.17.</span> <span class="nav-text">•
DiffusionNet: Discretization Agnostic Learning on Surfaces paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#text-and-image-guided-3d-avatar-generation-and-manipulation-paper-code"><span class="nav-number">1.3.8.0.18.</span> <span class="nav-text">•
Text and Image Guided 3D Avatar Generation and Manipulation paper code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#quantification-of-occlusion-handling-capability-of-a-3d-human-pose-estimation-framework-paper"><span class="nav-number">1.3.8.0.19.</span> <span class="nav-text">•
Quantification of Occlusion Handling Capability of a 3D Human Pose
Estimation Framework paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#motron-multimodal-probabilistic-human-motion-forecasting-paper"><span class="nav-number">1.3.8.0.20.</span> <span class="nav-text">•
Motron: Multimodal Probabilistic Human Motion Forecasting paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#fexgan-meta-facial-expression-generation-with-meta-humans-paper"><span class="nav-number">1.3.8.0.21.</span> <span class="nav-text">•
FEXGAN-META: FACIAL EXPRESSION GENERATION WITH META HUMANS paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#actformer-a-gan-transformer-framework-towards-general-action-conditioned-3d-human-motion-generation"><span class="nav-number">1.3.8.0.22.</span> <span class="nav-text">•
ActFormer: A GAN Transformer Framework towards General
Action-Conditioned 3D Human Motion Generation</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#domain-adaptive-hand-keypoint-and-pixel-localization-in-the-wild-paper"><span class="nav-number">1.3.8.0.23.</span> <span class="nav-text">•
Domain Adaptive Hand Keypoint and Pixel Localization in the Wild paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#portrait-eyeglasses-and-shadow-removal-by-leveraging-3d-synthetic-data-paper"><span class="nav-number">1.3.8.0.24.</span> <span class="nav-text">•
Portrait Eyeglasses and Shadow Removal by Leveraging 3D Synthetic Data
paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#recognition-of-freely-selected-keypoints-on-human-limbs-paper"><span class="nav-number">1.3.8.0.25.</span> <span class="nav-text">•
Recognition of Freely Selected Keypoints on Human Limbs paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#whats-in-your-hands-3d-reconstruction-of-generic-objects-in-hands-paper-code"><span class="nav-number">1.3.8.0.26.</span> <span class="nav-text">•
What’s in your hands? 3D Reconstruction of Generic Objects in Hands paper code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#chore-contact-human-and-object-reconstruction-from-a-single-rgb-image-paper"><span class="nav-number">1.3.8.0.27.</span> <span class="nav-text">•
CHORE: Contact, Human and Object REconstruction from a single RGB image
paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#snug-self-supervised-neural-dynamic-garments-paper"><span class="nav-number">1.3.8.0.28.</span> <span class="nav-text">• SNUG:
Self-Supervised Neural Dynamic Garments paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#d-magic-mirror-clothing-reconstruction-from-a-single-image-via-a-causal-perspective-paper"><span class="nav-number">1.3.8.0.29.</span> <span class="nav-text">•
3D Magic Mirror: Clothing Reconstruction from a Single Image via a
Causal Perspective paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#fake-it-till-you-make-it-face-analysis-in-the-wild-using-synthetic-data-alone-homepage"><span class="nav-number">1.3.8.0.30.</span> <span class="nav-text">•
Fake it till you make it: face analysis in the wild using synthetic data
alone homepage</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#avatarclip-zero-shot-text-driven-generation-and-animation-of-3d-avatars-paper-code"><span class="nav-number">1.3.8.0.31.</span> <span class="nav-text">•
AvatarCLIP: Zero-Shot Text-Driven Generation and Animation of 3D Avatars
paper code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#scene-aware-person-image-generation-through-global-contextual-conditioning-paper"><span class="nav-number">1.3.8.0.32.</span> <span class="nav-text">•
Scene Aware Person Image Generation through Global Contextual
Conditioning paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#hairfit-pose-invariant-hairstyle-transfer-via-flow-based-hair-alignment-and-semantic-region-aware-inpainting-paper"><span class="nav-number">1.3.8.0.33.</span> <span class="nav-text">•
HairFIT: Pose-Invariant Hairstyle Transfer via Flow-based Hair Alignment
and Semantic-Region-Aware Inpainting paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#from-a-few-accurate-2d-correspondences-to-3d-point-clouds-paper"><span class="nav-number">1.3.8.0.34.</span> <span class="nav-text">•
From a few Accurate 2D Correspondences to 3D Point Clouds paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#convolutional-neural-network-based-partial-face-detection-paper"><span class="nav-number">1.3.8.0.35.</span> <span class="nav-text">•
Convolutional Neural Network Based Partial Face Detection paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#spsn-superpixel-prototype-sampling-network-for-rgb-d-salient-object-detection-paper-code"><span class="nav-number">1.3.8.0.36.</span> <span class="nav-text">•
SPSN: Superpixel Prototype Sampling Network for RGB-D Salient Object
Detection paper code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#detecting-humans-in-rgb-d-data-with-cnns-paper"><span class="nav-number">1.3.8.0.37.</span> <span class="nav-text">• Detecting
Humans in RGB-D Data with CNNs paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#animation-from-blur-multi-modal-blur-decomposition-with-motion-guidance-paper"><span class="nav-number">1.3.8.0.38.</span> <span class="nav-text">•
Animation from Blur: Multi-modal Blur Decomposition with Motion Guidance
paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#d-shape-sequence-of-human-comparison-and-classification-using-current-and-varifolds-paper-code"><span class="nav-number">1.3.8.0.39.</span> <span class="nav-text">•
3D Shape Sequence of Human Comparison and Classification using Current
and Varifolds paper
code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#kinepose-a-temporally-optimized-inverse-kinematics-technique-for-6dof-human-pose-estimation-with-biomechanical-constraints-paper-code"><span class="nav-number">1.3.8.0.40.</span> <span class="nav-text">•
KinePose: A temporally optimized inverse kinematics technique for 6DOF
human pose estimation with biomechanical constraints paper code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#skeleton-free-pose-transfer-for-stylized-3d-character-paper"><span class="nav-number">1.3.8.0.41.</span> <span class="nav-text">•
Skeleton-free Pose Transfer for Stylized 3D Character paper</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#parametric-model"><span class="nav-number">1.4.</span> <span class="nav-text">parametric model</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#body"><span class="nav-number">1.4.1.</span> <span class="nav-text">body</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#smpl-a-skinned-multi-person-linear-model-paper-code"><span class="nav-number">1.4.1.0.1.</span> <span class="nav-text">• SMPL: A
Skinned Multi-Person Linear Model paper
code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#expressive-body-capture-3d-hands-face-and-body-from-a-single-image-paper-code"><span class="nav-number">1.4.1.0.2.</span> <span class="nav-text">•
Expressive Body Capture: 3D Hands, Face, and Body from a Single Image paper code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#star-sparse-trained-articulated-human-body-regressor-paper-code"><span class="nav-number">1.4.1.0.3.</span> <span class="nav-text">•
STAR: Sparse Trained Articulated Human Body Regressor paper code</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#face"><span class="nav-number">1.4.2.</span> <span class="nav-text">face</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#basel-face-model-2009-website"><span class="nav-number">1.4.2.0.1.</span> <span class="nav-text">• Basel Face Model 2009 website</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#basel-face-model-2017-website"><span class="nav-number">1.4.2.0.2.</span> <span class="nav-text">• Basel Face Model 2017 website</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#large-scale-3d-morphable-model-website"><span class="nav-number">1.4.2.0.3.</span> <span class="nav-text">• Large Scale 3D
Morphable Model website</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#head"><span class="nav-number">1.4.3.</span> <span class="nav-text">head</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#flame-articulated-expressive-head-model-website"><span class="nav-number">1.4.3.0.1.</span> <span class="nav-text">• FLAME:
Articulated Expressive Head Model website</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#hand"><span class="nav-number">1.4.4.</span> <span class="nav-text">hand</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#mano-paper-website"><span class="nav-number">1.4.4.0.1.</span> <span class="nav-text">• MANO paper
website</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#nimble-a-non-rigid-hand-model-with-bones-and-muscles-paper"><span class="nav-number">1.4.4.0.2.</span> <span class="nav-text">•
NIMBLE: A Non-rigid Hand Model with Bones and Muscles paper</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#method"><span class="nav-number">1.4.5.</span> <span class="nav-text">method</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#ghum-ghuml-generative-3d-human-shape-and-articulated-pose-models-github"><span class="nav-number">1.4.5.0.1.</span> <span class="nav-text">•GHUM
&amp; GHUML: Generative 3D Human Shape and Articulated Pose Models github</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#large-scale-3d-morphable-models-paper-code"><span class="nav-number">1.4.5.0.2.</span> <span class="nav-text">•Large Scale 3D
Morphable Models paper
code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#morphable-face-models---an-open-framework-paper-code"><span class="nav-number">1.4.5.0.3.</span> <span class="nav-text">•Morphable
Face Models - An Open Framework paper code</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#dataset"><span class="nav-number">1.5.</span> <span class="nav-text">dataset</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#face-1"><span class="nav-number">1.5.1.</span> <span class="nav-text">face</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#maad-face-a-massively-annotated-attribute-dataset-for-face-images-paper"><span class="nav-number">1.5.1.0.1.</span> <span class="nav-text">•
MAAD-Face: A Massively Annotated Attribute Dataset for Face Images paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#facescape-3d-facial-dataset-and-benchmark-for-single-view-3d-face-reconstruction-paper"><span class="nav-number">1.5.1.0.2.</span> <span class="nav-text">•
FaceScape: 3D Facial Dataset and Benchmark for Single-View 3D Face
Reconstruction paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#w-lp-aflw2000-3d-homepage"><span class="nav-number">1.5.1.0.3.</span> <span class="nav-text">• 300W-LP &amp; AFLW2000-3D homepage</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#aflw-website"><span class="nav-number">1.5.1.0.4.</span> <span class="nav-text">• AFLW website</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#realy-rethinking-the-evaluation-of-3d-face-reconstruction-paper-website"><span class="nav-number">1.5.1.0.5.</span> <span class="nav-text">•
REALY: Rethinking the Evaluation of 3D Face Reconstruction paper website</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#faceverse-high-quality-3d-face-dataset-github"><span class="nav-number">1.5.1.0.6.</span> <span class="nav-text">• FaceVerse-High
Quality 3D Face Dataset github</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#dad-3dheads-a-large-scale-dense-accurate-and-diverse-dataset-for-3d-head-alignment-from-a-single-image-paper-github"><span class="nav-number">1.5.1.0.7.</span> <span class="nav-text">•
DAD-3DHeads: A Large-scale Dense, Accurate and Diverse Dataset for 3D
Head Alignment from a Single Image paper github</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#multiface-a-dataset-for-neural-face-rendering-paper"><span class="nav-number">1.5.1.0.8.</span> <span class="nav-text">•
Multiface: A Dataset for Neural Face Rendering paper</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#hand-1"><span class="nav-number">1.5.2.</span> <span class="nav-text">hand</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#grab-a-dataset-of-whole-body-human-grasping-of-objects-github"><span class="nav-number">1.5.2.0.1.</span> <span class="nav-text">•
GRAB: A Dataset of Whole-Body Human Grasping of Objects github</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#reconstructing-hand-object-interactions-in-the-wild-github"><span class="nav-number">1.5.2.0.2.</span> <span class="nav-text">•
Reconstructing Hand-Object Interactions in the Wild github</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#ego2handspose-a-dataset-for-egocentric-two-hand-3d-global-pose-estimation-paper"><span class="nav-number">1.5.2.0.3.</span> <span class="nav-text">•
Ego2HandsPose: A Dataset for Egocentric Two-hand 3D Global Pose
Estimation paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#interhand2.6m"><span class="nav-number">1.5.2.0.4.</span> <span class="nav-text">• Interhand2.6M</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#body-1"><span class="nav-number">1.5.3.</span> <span class="nav-text">body</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#ntu60-x-towards-skeleton-based-recognition-of-subtle-human-actions-code"><span class="nav-number">1.5.3.0.1.</span> <span class="nav-text">•
NTU60-X: TOWARDS SKELETON-BASED RECOGNITION OF SUBTLE HUMAN ACTIONS code</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#agora-avatars-in-geography-optimized-for-regression-analysis-homepage"><span class="nav-number">1.5.3.0.2.</span> <span class="nav-text">•
AGORA: Avatars in Geography Optimized for Regression Analysis homepage</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#amass-archive-of-motion-capture-as-surface-shapes-paper-github"><span class="nav-number">1.5.3.0.3.</span> <span class="nav-text">•
AMASS: Archive of Motion Capture as Surface Shapes paper github</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#asl-skeleton3d-and-asl-phono-two-novel-datasets-for-the-american-sign-language-paper"><span class="nav-number">1.5.3.0.4.</span> <span class="nav-text">•
ASL-Skeleton3D and ASL-Phono: Two Novel Datasets for the American Sign
Language paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#mpi-inf-3dhp-homepage"><span class="nav-number">1.5.3.0.5.</span> <span class="nav-text">•MPI-INF-3DHP homepage</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#human3.6m-website"><span class="nav-number">1.5.3.0.6.</span> <span class="nav-text">•Human3.6M website</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#dpw-website"><span class="nav-number">1.5.3.0.7.</span> <span class="nav-text">•3DPW website</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#pennaction-website"><span class="nav-number">1.5.3.0.8.</span> <span class="nav-text">•PennAction website</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#insta-variety-github"><span class="nav-number">1.5.3.0.9.</span> <span class="nav-text">•Insta Variety github</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#posetrack-homapage"><span class="nav-number">1.5.3.0.10.</span> <span class="nav-text">•PoseTrack homapage</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#kinetics-400-website"><span class="nav-number">1.5.3.0.11.</span> <span class="nav-text">•Kinetics-400 website</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#renderpeople-website"><span class="nav-number">1.5.3.0.12.</span> <span class="nav-text">•RenderPeople website</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#buff-website"><span class="nav-number">1.5.3.0.13.</span> <span class="nav-text">•BUFF website</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#people-snapshot-dataset-homepage"><span class="nav-number">1.5.3.0.14.</span> <span class="nav-text">•People Snapshot Dataset homepage</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#multi-garment-homepage"><span class="nav-number">1.5.3.0.15.</span> <span class="nav-text">•Multi-Garment homepage</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#iper-website"><span class="nav-number">1.5.3.0.16.</span> <span class="nav-text">•iPER website</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#zju-mocap-homepage"><span class="nav-number">1.5.3.0.17.</span> <span class="nav-text">•ZJU-MoCap homepage</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#smartportraits-depth-powered-handheld-smartphone-dataset-of-human-portraits-for-state-estimation-reconstruction-and-synthesis-paper"><span class="nav-number">1.5.3.0.18.</span> <span class="nav-text">•SmartPortraits:
Depth Powered Handheld Smartphone Dataset of Human Portraits for State
Estimation, Reconstruction and Synthesis paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#mvp-human-dataset-for-3d-human-avatar-reconstruction-from-unconstrained-frames-paper"><span class="nav-number">1.5.3.0.19.</span> <span class="nav-text">•MVP-Human
Dataset for 3D Human Avatar Reconstruction from Unconstrained Frames paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#humman-multi-modal-4d-human-dataset-for-versatile-sensing-and-modeling-paper"><span class="nav-number">1.5.3.0.20.</span> <span class="nav-text">•HuMMan:
Multi-Modal 4D Human Dataset for Versatile Sensing and Modeling paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#dmpb-dataset-github"><span class="nav-number">1.5.3.0.21.</span> <span class="nav-text">•3DMPB-dataset github</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#imar_vision_datasets_tools-github"><span class="nav-number">1.5.3.0.22.</span> <span class="nav-text">•imar_vision_datasets_tools
github</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#rich-real-scenes-interaction-contacts-and-humans-github"><span class="nav-number">1.5.3.0.23.</span> <span class="nav-text">•RICH:
Real scenes, Interaction, Contacts and Humans github</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#method-1"><span class="nav-number">1.5.4.</span> <span class="nav-text">method</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#neuralannot-neural-annotator-for-3d-human-mesh-training-sets-paper"><span class="nav-number">1.5.4.0.1.</span> <span class="nav-text">•NeuralAnnot:
Neural Annotator for 3D Human Mesh Training Sets paper</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#others"><span class="nav-number">1.5.5.</span> <span class="nav-text">others</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#k-hairstyle-a-large-scale-korean-hairstyle-dataset-for-virtual-hair-editing-and-hairstyle-classification-homepage"><span class="nav-number">1.5.5.0.1.</span> <span class="nav-text">•
K-Hairstyle: A Large-scale Korean hairstyle dataset for virtual hair
editing and hairstyle classification homepage</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#simulated-garment-dataset-for-virtual-try-on-address"><span class="nav-number">1.5.5.0.2.</span> <span class="nav-text">•
Simulated garment dataset for virtual try-on address</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#deepfashion-homepage"><span class="nav-number">1.5.5.0.3.</span> <span class="nav-text">• DeepFashion homepage</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#delving-into-high-quality-synthetic-face-occlusion-segmentation-datasets-paper"><span class="nav-number">1.5.5.0.4.</span> <span class="nav-text">•
Delving into High-Quality Synthetic Face Occlusion Segmentation Datasets
paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#a-gan-facial-flow-for-face-animation-with-generative-adversarial-networks-paper"><span class="nav-number">1.5.5.0.5.</span> <span class="nav-text">•
3A-GAN: Facial Flow for Face Animation with Generative Adversarial
Networks paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#scanned-objects-by-google-research-website"><span class="nav-number">1.5.5.0.6.</span> <span class="nav-text">• Scanned Objects by
Google Research website</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#labs"><span class="nav-number">1.6.</span> <span class="nav-text">labs</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#max-planck-institute-website"><span class="nav-number">1.6.0.0.1.</span> <span class="nav-text">• max planck institute website</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#yebin-liu-website"><span class="nav-number">1.6.0.0.2.</span> <span class="nav-text">• Yebin Liu website</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#zju3dv-github"><span class="nav-number">1.6.0.0.3.</span> <span class="nav-text">• ZJU3DV github</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#hujun-bao-google-scholar"><span class="nav-number">1.6.0.0.4.</span> <span class="nav-text">• Hujun Bao google
scholar</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#ustc-3dv-homepage"><span class="nav-number">1.6.0.0.5.</span> <span class="nav-text">• USTC-3DV homepage</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#hao_li-homepage"><span class="nav-number">1.6.0.0.6.</span> <span class="nav-text">• Hao_Li homepage</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#other-related-awesome"><span class="nav-number">1.7.</span> <span class="nav-text">other related awesome</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#awesome-clothed-human-github"><span class="nav-number">1.7.0.0.1.</span> <span class="nav-text">• awesome-clothed-human github</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#curated-list-of-awesome-3d-morphable-model-software-and-data-github"><span class="nav-number">1.7.0.0.2.</span> <span class="nav-text">•
curated-list-of-awesome-3D-Morphable-Model-software-and-data github</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#awesome-hand-pose-estimation-github"><span class="nav-number">1.7.0.0.3.</span> <span class="nav-text">•
awesome-hand-pose-estimation github</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#awesome-3d-body-papers-github"><span class="nav-number">1.7.0.0.4.</span> <span class="nav-text">• Awesome 3D Body Papers github</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#body_reconstruction_references-github"><span class="nav-number">1.7.0.0.5.</span> <span class="nav-text">•
Body_Reconstruction_References github</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#d-face-reconstruction-paper-list-github"><span class="nav-number">1.7.0.0.6.</span> <span class="nav-text">•
3D-face-reconstruction-paper-list github</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#awesome_talking_face_generation-github"><span class="nav-number">1.7.0.0.7.</span> <span class="nav-text">•
awesome_talking_face_generation github</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#cg3dv-twitter-github"><span class="nav-number">1.7.0.0.8.</span> <span class="nav-text">• CG&amp;3DV Twitter github</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#survey"><span class="nav-number">1.8.</span> <span class="nav-text">survey</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#recovering-3d-human-mesh-from-monocular-images-a-survey-paper-github"><span class="nav-number">1.8.0.0.1.</span> <span class="nav-text">•
Recovering 3D Human Mesh from Monocular Images: A Survey paper github</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#d-human-pose-estimation-a-survey-paper"><span class="nav-number">1.8.0.0.2.</span> <span class="nav-text">• 2D Human Pose
Estimation: A Survey paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#a-survey-of-non-rigid-3d-registration-paper"><span class="nav-number">1.8.0.0.3.</span> <span class="nav-text">• A Survey of
Non-Rigid 3D Registration paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#d-face-reconstruction-in-deep-learning-era-a-survey-paper"><span class="nav-number">1.8.0.0.4.</span> <span class="nav-text">• 3D
Face Reconstruction in Deep Learning Era: A Survey paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#towards-efficient-and-photorealistic-3d-human-reconstruction-a-brief-survey-paper"><span class="nav-number">1.8.0.0.5.</span> <span class="nav-text">•
Towards efficient and photorealistic 3D human reconstruction: A brief
survey paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#survey-on-3d-face-reconstruction-from-uncalibrated-images-paper"><span class="nav-number">1.8.0.0.6.</span> <span class="nav-text">•Survey
on 3D face reconstruction from uncalibrated images paper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#state-of-the-art-on-3d-reconstruction-with-rgb-d-cameras-paper"><span class="nav-number">1.8.0.0.7.</span> <span class="nav-text">•State
of the Art on 3D Reconstruction with RGB-D Cameras paper</span></a></li></ol></li></ol></li></ol></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="ZJU_XS"
      src="/images/IMG_0045.JPG">
  <p class="site-author-name" itemprop="name">ZJU_XS</p>
  <div class="site-description" itemprop="description">记录生活的点滴 探索未知的领域</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">10</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/The-FleetingLove" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;The-FleetingLove" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:shenxie@zju.edu.cn" title="E-Mail → mailto:shenxie@zju.edu.cn" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license site-overview-item animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://the-fleetinglove.github.io/2022/09/30/%E8%AE%BA%E6%96%87%E5%90%88%E9%9B%86--Awesome%203D%20Human%20Reconstruction/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/IMG_0045.JPG">
      <meta itemprop="name" content="ZJU_XS">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Starry Light">
      <meta itemprop="description" content="记录生活的点滴 探索未知的领域">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="3D Human Reconstruction Paper List | Starry Light">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          3D Human Reconstruction Paper List
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2022-09-30 12:50:05 / 修改时间：19:38:32" itemprop="dateCreated datePublished" datetime="2022-09-30T12:50:05+08:00">2022-09-30</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Face-Reconstruction/" itemprop="url" rel="index"><span itemprop="name">Face Reconstruction</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Face-Reconstruction/Paper-List/" itemprop="url" rel="index"><span itemprop="name">Paper List</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Changyan：</span>
    
    <a title="3D Human Reconstruction Paper List" href="/2022/09/30/%E8%AE%BA%E6%96%87%E5%90%88%E9%9B%86--Awesome%203D%20Human%20Reconstruction/#SOHUCS" itemprop="discussionUrl">
      <span id="sourceId::a47a9290817e16c5b2382daa872e44fb" class="cy_cmt_count" itemprop="commentCount"></span>
    </a>
  </span>
  
  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>29k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>48 分钟</span>
    </span>
</div>


          
            <i class="fa fa-thumb-tack"></i>
            <font color=green>置顶</font>
            <span class="post-meta-divider">|</span>
            
        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="awesome-3d-human-reconstruction">Awesome 3D Human
Reconstruction</h1>
<p>A curated list of related resources for 3d human reconstruction. <a target="_blank" rel="noopener" href="https://github.com/rlczddl/awesome-3d-human-reconstruction">Link</a></p>
<h2 id="contents">Contents</h2>
<ul>
<li><a href="#3d-human">papers</a></li>
<li><a href="#related">related papers</a></li>
<li><a href="#parametric-model">parametric model</a></li>
<li><a href="#dataset">dataset</a></li>
<li><a href="#labs">labs</a></li>
<li><a href="#other-related-awesome">other related awesome</a></li>
<li><a href="#survey">survey</a></li>
</ul>
<p><span id="more"></span></p>
<h2 id="d-human">3d human</h2>
<h3 id="nerf-or-pifu">nerf or pifu</h3>
<h5 id="stereopifu-depth-aware-clothed-human-digitization-via-stereo-vision-paper">•
StereoPIFu: Depth Aware Clothed Human Digitization via Stereo Vision <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2104.05289.pdf">paper</a></h5>
<h5 id="learning-implicit-3d-representations-of-dressed-humans-from-sparse-views-paper">•
Learning Implicit 3D Representations of Dressed Humans from Sparse Views
<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2104.08013v1.pdf">paper</a></h5>
<h5 id="animatable-neural-radiance-fields-for-human-body-modeling-paper">•
Animatable Neural Radiance Fields for Human Body Modeling <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2105.02872.pdf">paper</a></h5>
<h5 id="pamir-parametric-model-conditioned-implicit-representation-for-image-based-human-reconstruction-paper-code">•
PaMIR: Parametric Model-Conditioned Implicit Representation for
Image-based Human Reconstruction <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2007.03858.pdf">paper</a> <a target="_blank" rel="noopener" href="https://github.com/ZhengZerong/PaMIR">code</a></h5>
<h5 id="neural-actor-neural-free-view-synthesis-of-human-actors-with-pose-control-papercode">•
Neural Actor: Neural Free-view Synthesis of Human Actors with Pose
Control <a target="_blank" rel="noopener" href="http://gvv.mpi-inf.mpg.de/projects/NeuralActor/">paper&amp;code</a></h5>
<h5 id="moco-flow-neural-motion-consensus-flow-for-dynamic-humans-in-stationary-monocular-cameras-paper">•
MoCo-Flow: Neural Motion Consensus Flow for Dynamic Humans in Stationary
Monocular Cameras <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2106.04477v1.pdf">paper</a></h5>
<h5 id="doublefield-bridging-the-neural-surface-and-radiance-fields-for-high-fidelity-human-rendering-paper">•
DoubleField: Bridging the Neural Surface and Radiance Fields for
High-fidelity Human Rendering <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2106.03798.pdf">paper</a></h5>
<h5 id="bridge-the-gap-between-model-based-and-model-free-human-reconstruction-paper">•
Bridge the Gap Between Model-based and Model-free Human Reconstruction
<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2106.06415v1.pdf">paper</a></h5>
<h5 id="metaavatar-learning-animatable-clothed-human-models-from-few-depth-images-paper">•
MetaAvatar: Learning Animatable Clothed Human Models from Few Depth
Images <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2106.11944v1.pdf">paper</a></h5>
<h5 id="animatable-neural-radiance-fields-from-monocular-rgb-video-paper">•
Animatable Neural Radiance Fields from Monocular RGB Video <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2106.13629v1.pdf">paper</a></h5>
<h5 id="few-shot-neural-human-performance-rendering-from-sparse-rgbd-videos-paper">•
Few-shot Neural Human Performance Rendering from Sparse RGBD Videos <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2107.06505v1.pdf">paper</a></h5>
<h5 id="relightable-neural-video-portrait-paper">• Relightable Neural
Video Portrait <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2107.14735v1.pdf">paper</a></h5>
<h5 id="flame-in-nerf-neural-control-of-radiance-fields-for-free-view-face-animation-paper">•
FLAME-in-NeRF : Neural control of Radiance Fields for Free View Face
Animation <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2108.04913v1.pdf">paper</a></h5>
<h5 id="arch-animation-ready-clothed-human-reconstruction-revisited-paper">•
ARCH++: Animation-Ready Clothed Human Reconstruction Revisited <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2108.07845.pdf">paper</a></h5>
<h5 id="neural-gif-neural-generalized-implicit-functions-for-animating-people-in-clothing-paper">•
Neural-GIF: Neural Generalized Implicit Functions for Animating People
in Clothing <a target="_blank" rel="noopener" href="https://virtualhumans.mpi-inf.mpg.de/neuralgif/">paper</a></h5>
<h5 id="neural-human-performer-learning-generalizable-radiance-fields-for-human-performance-rendering-paper-code">•
Neural Human Performer: Learning Generalizable Radiance Fields for Human
Performance Rendering <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2109.07448v1.pdf">paper</a> <a target="_blank" rel="noopener" href="https://youngjoongunc.github.io/nhp/">code</a></h5>
<h5 id="topologically-consistent-multi-view-face-inference-using-volumetric-sampling-paper">•
Topologically Consistent Multi-View Face Inference Using Volumetric
Sampling <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2110.02948.pdf">paper</a></h5>
<h5 id="creating-and-reenacting-controllable-3d-humans-with-differentiable-rendering-paper">•
Creating and Reenacting Controllable 3D Humans with Differentiable
Rendering <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2110.11746v1.pdf">paper</a></h5>
<h5 id="h-nerf-neural-radiance-fields-for-rendering-and-temporal-reconstruction-of-humans-in-motion-paper">•
H-NeRF: Neural Radiance Fields for Rendering and Temporal Reconstruction
of Humans in Motion <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2110.13746v1.pdf">paper</a></h5>
<h5 id="fenerf-face-editing-in-neural-radiance-fields-paper">• FENeRF:
Face Editing in Neural Radiance Fields <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2111.15490v1.pdf">paper</a></h5>
<h5 id="latenthuman-shape-and-pose-disentangled-latent-representation-for-human-bodies-papercode">•
LatentHuman: Shape-and-Pose Disentangled Latent Representation for Human
Bodies <a target="_blank" rel="noopener" href="https://latenthuman.github.io/">paper&amp;code</a></h5>
<h5 id="neural-head-avatars-from-monocular-rgb-videos-paper">• Neural
Head Avatars from Monocular RGB Videos <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2112.01554v1.pdf">paper</a></h5>
<h5 id="humannerf-generalizable-neural-human-radiance-field-from-sparse-inputs-paper">•
HumanNeRF: Generalizable Neural Human Radiance Field from Sparse Inputs
<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2112.02789v1.pdf">paper</a></h5>
<h5 id="implicit-neural-deformation-for-multi-view-face-reconstruction-paper">•
Implicit Neural Deformation for Multi-View Face Reconstruction <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2112.02494v1.pdf">paper</a></h5>
<h5 id="mofanerf-morphable-facial-neural-radiance-field-paper-code">•
MoFaNeRF: Morphable Facial Neural Radiance Field <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2112.02308.pdf">paper</a> <a target="_blank" rel="noopener" href="https://github.com/zhuhao-nju/mofanerf">code</a></h5>
<h5 id="geometry-guided-progressive-nerf-for-generalizable-and-efficient-neural-human-rendering-paper">•
Geometry-Guided Progressive NeRF for Generalizable and Efficient Neural
Human Rendering <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2112.04312v1.pdf">paper</a></h5>
<h5 id="headnerf-a-real-time-nerf-based-parametric-head-model-paper-code">•
HeadNeRF: A Real-time NeRF-based Parametric Head Model <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2112.05637v1.pdf">paper</a> <a target="_blank" rel="noopener" href="https://github.com/CrisHY1995/headnerf">code</a></h5>
<h5 id="i-m-avatar-implicit-morphable-head-avatars-from-videos-paper-code">•
I M Avatar: Implicit Morphable Head Avatars from Videos <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2112.07471v1.pdf">paper</a> <a target="_blank" rel="noopener" href="https://github.com/zhengyuf/IMavatar">code</a></h5>
<h5 id="lookingoodπ-real-time-person-independent-neural-re-rendering-for-high-quality-human-performance-capture-paper">•
LookinGoodπ: Real-time Person-independent Neural Re-rendering for
High-quality Human Performance Capture <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2112.08037v1.pdf">paper</a></h5>
<h5 id="icon-implicit-clothed-humans-obtained-from-normals-paper-code">•
ICON: Implicit Clothed humans Obtained from Normals <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2112.09127v1.pdf">paper</a> <a target="_blank" rel="noopener" href="https://github.com/YuliangXiu/ICON">code</a></h5>
<h5 id="dd-nerf-double-diffusion-neural-radiance-field-as-a-generalizable-implicit-body-representation-paper">•
DD-NeRF: Double-Diffusion Neural Radiance Field as a Generalizable
Implicit Body Representation <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2112.12390v1.pdf">paper</a></h5>
<h5 id="human-view-synthesis-using-a-single-sparse-rgb-d-input-paper">•
Human View Synthesis using a Single Sparse RGB-D Input <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2112.13889v1.pdf">paper</a></h5>
<h5 id="surface-aligned-neural-radiance-fields-for-controllable-3d-human-synthesis-paper">•
Surface-Aligned Neural Radiance Fields for Controllable 3D Human
Synthesis <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2201.01683v1.pdf">paper</a></h5>
<h5 id="embodied-hands-modeling-and-capturing-hands-and-bodies-together-paper">•
Embodied Hands: Modeling and Capturing Hands and Bodies Together <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2201.02610v1.pdf">paper</a></h5>
<h5 id="humannerf-free-viewpoint-rendering-of-moving-people-from-monocular-video-paper-code">•
HumanNeRF: Free-viewpoint Rendering of Moving People from Monocular
Video <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2201.04127v1.pdf">paper</a> <a target="_blank" rel="noopener" href="https://github.com/chungyiweng/humannerf">code</a></h5>
<h5 id="gdna-towards-generative-detailed-neural-avatars-paper-code">•
gDNA: Towards Generative Detailed Neural Avatars <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2201.04123v1.pdf">paper</a> <a target="_blank" rel="noopener" href="https://github.com/xuchen-ethz/gdna">code</a></h5>
<h5 id="selfrecon-self-reconstruction-your-digital-avatar-from-monocular-video-paper">•
SelfRecon: Self Reconstruction Your Digital Avatar from Monocular Video
<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2201.12792v1.pdf">paper</a></h5>
<h5 id="neuvv-neural-volumetric-videos-with-immersive-rendering-and-editing-paper">•
NeuVV: Neural Volumetric Videos with Immersive Rendering and Editing <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2202.06088v1.pdf">paper</a></h5>
<h5 id="animatable-neural-radiance-fields-for-modeling-dynamic-human-bodies-paper-code">•
Animatable Neural Radiance Fields for Modeling Dynamic Human Bodies <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2105.02872">paper</a> <a target="_blank" rel="noopener" href="https://github.com/zju3dv/animatable_nerf">code</a></h5>
<h5 id="deepmulticap-performance-capture-of-multiple-characters-using-sparse-multiview-cameras-paper-code">•
DeepMultiCap: Performance Capture of Multiple Characters Using Sparse
Multiview Cameras <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2105.00261">paper</a>
<a target="_blank" rel="noopener" href="https://github.com/DSaurus/DeepMultiCap">code</a></h5>
<h5 id="neuralfusion-neural-volumetric-rendering-under-human-object-interactions-paper">•
NeuralFusion: Neural Volumetric Rendering under Human-object
Interactions <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2202.12825v1.pdf">paper</a></h5>
<h5 id="pina-learning-a-personalized-implicit-neural-avatar-from-a-single-rgb-d-video-sequence-paper-code">•
PINA: Learning a Personalized Implicit Neural Avatar from a Single RGB-D
Video Sequence <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2203.01754v1.pdf">paper</a> <a target="_blank" rel="noopener" href="https://github.com/zj-dong/pina/tree/page">code</a></h5>
<h5 id="neuman-neural-human-radiance-field-from-a-single-video-paper">•
NeuMan: Neural Human Radiance Field from a Single Video <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2203.12575v1.pdf">paper</a></h5>
<h5 id="m-avatar-implicit-morphable-head-avatars-from-videos-paper-code">• M
Avatar: Implicit Morphable Head Avatars from Videos <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2112.07471.pdf">paper</a> <a target="_blank" rel="noopener" href="https://github.com/zhengyuf/IMavatar">code</a></h5>
<h5 id="imface-a-nonlinear-3d-morphable-face-model-with-implicit-neural-representations-paper">•
ImFace: A Nonlinear 3D Morphable Face Model with Implicit Neural
Representations <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2203.14510.pdf">paper</a></h5>
<h5 id="coap-compositional-articulated-occupancy-of-people-paper-code">•
COAP: Compositional Articulated Occupancy of People <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2204.06184v1.pdf">paper</a> <a target="_blank" rel="noopener" href="https://github.com/markomih/COAP">code</a></h5>
<h5 id="sunstage-portrait-reconstruction-and-relighting-using-the-sun-as-a-light-stage-paper">•
SunStage: Portrait Reconstruction and Relighting using the Sun as a
Light Stage <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2204.03648v1.pdf">paper</a></h5>
<h5 id="lisa-learning-implicit-shape-and-appearance-of-hands-paper">•
LISA: Learning Implicit Shape and Appearance of Hands <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2204.01695v1.pdf">paper</a></h5>
<h5 id="animatable-neural-radiance-fields-from-monocular-rgb-d-paper">•
Animatable Neural Radiance Fields from Monocular RGB-D <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2204.01218v1.pdf">paper</a></h5>
<h5 id="jiff-jointly-aligned-implicit-face-function-for-high-quality-single-view-clothed-human-reconstruction-paper">•
JIFF: Jointly-aligned Implicit Face Function for High Quality Single
View Clothed Human Reconstruction <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2204.10549v1.pdf">paper</a></h5>
<h5 id="generalizable-neural-performer-learning-robust-radiance-fields-for-human-novel-view-synthesis-paper">•
Generalizable Neural Performer: Learning Robust Radiance Fields for
Human Novel View Synthesis <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2204.11798v1.pdf">paper</a></h5>
<h5 id="danbo-disentangled-articulated-neural-body-representations-via-graph-neural-networks-paper">•
DANBO: Disentangled Articulated Neural Body Representations via Graph
Neural Networks <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2205.01666.pdf">paper</a></h5>
<h5 id="single-view-3d-body-and-cloth-reconstruction-under-complex-poses-paper">•
Single-view 3D Body and Cloth Reconstruction under Complex Poses <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2205.04087v1.pdf">paper</a></h5>
<h5 id="keypointnerf-generalizing-image-based-volumetric-avatars-using-relative-spatial-encoding-of-keypoints-paper">•
KeypointNeRF: Generalizing Image-based Volumetric Avatars using Relative
Spatial Encoding of Keypoints <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2205.04992.pdf">paper</a></h5>
<h5 id="h3d-net-few-shot-high-fidelity-3d-head-reconstruction-paper-code">•
H3D-Net: Few-Shot High-Fidelity 3D Head Reconstruction <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2107.12512.pdf">paper</a> <a target="_blank" rel="noopener" href="https://github.com/MaxPolak97/H3D-Net-reproduction">code</a></h5>
<h5 id="photorealistic-monocular-3d-reconstruction-of-humans-wearing-clothing-paper">•
Photorealistic Monocular 3D Reconstruction of Humans Wearing Clothing <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2204.08906">paper</a></h5>
<h5 id="high-fidelity-human-avatars-from-a-single-rgb-camera-paper">•
High-Fidelity Human Avatars from a Single RGB Camera <a target="_blank" rel="noopener" href="https://github.com/hzhao1997/HF-Avatar">paper</a></h5>
<h5 id="uv-volumes-for-real-time-rendering-of-editable-free-view-human-performance-code">•
UV Volumes for Real-time Rendering of Editable Free-view Human
Performance <a target="_blank" rel="noopener" href="https://github.com/fanegg/UV-Volumes">code</a></h5>
<h5 id="fof-learning-fourier-occupancy-field-for-monocular-real-time-human-reconstruction-paper">•
FOF: Learning Fourier Occupancy Field for Monocular Real-time Human
Reconstruction <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2206.02194v1.pdf">paper</a></h5>
<h5 id="nemf-neural-motion-fields-for-kinematic-animation-paper">• NeMF:
Neural Motion Fields for Kinematic Animation <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2206.03287v1.pdf">paper</a></h5>
<h5 id="rignerf-fully-controllable-neural-3d-portraits-paper">• RigNeRF:
Fully Controllable Neural 3D Portraits <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2206.06481v1.pdf">paper</a></h5>
<h5 id="eyenerf-a-hybrid-representation-for-photorealistic-synthesis-animation-and-relighting-of-human-eyes-paper">•
EyeNeRF: A Hybrid Representation for Photorealistic Synthesis, Animation
and Relighting of Human Eyes <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2206.08428v1.pdf">paper</a></h5>
<h5 id="tava-template-free-animatable-volumetric-actors-paper">• TAVA:
Template-free Animatable Volumetric Actors <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2206.08929v1.pdf">paper</a></h5>
<h5 id="neural-surface-reconstruction-of-dynamic-scenes-with-monocular-rgb-d-camera-homepage">•
Neural Surface Reconstruction of Dynamic Scenes with Monocular RGB-D
Camera <a target="_blank" rel="noopener" href="https://ustc3dv.github.io/ndr/">homepage</a></h5>
<h5 id="generative-neural-articulated-radiance-fields-paper">•
Generative Neural Articulated Radiance Fields <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2206.14314v1.pdf">paper</a></h5>
<h5 id="neural-parameterization-for-dynamic-human-head-editing-paper">•
Neural Parameterization for Dynamic Human Head Editing <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2207.00210v1.pdf">paper</a></h5>
<h5 id="avatarcap-animatable-avatar-conditioned-monocular-human-volumetric-capture-paper-code">•
AvatarCap: Animatable Avatar Conditioned Monocular Human Volumetric
Capture <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2207.02031v1.pdf">paper</a> <a target="_blank" rel="noopener" href="https://github.com/lizhe00/AvatarCap">code</a></h5>
<h5 id="learning-implicit-templates-for-point-based-clothed-human-modeling-homepage">•
Learning Implicit Templates for Point-Based Clothed Human Modeling <a target="_blank" rel="noopener" href="https://jsnln.github.io/fite/">homepage</a></h5>
<h5 id="relighting4d-neural-relightable-human-from-videos-code">•Relighting4D:
Neural Relightable Human from Videos <a target="_blank" rel="noopener" href="https://github.com/FrozenBurning/Relighting4D">code</a></h5>
<h5 id="high-quality-human-reconstruction-via-diffusion-based-stereo-using-sparse-cameras-paper">•High
Quality Human Reconstruction via Diffusion-based Stereo Using Sparse
Cameras <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2207.08000v1.pdf">paper</a></h5>
<h5 id="crosshuman-learning-cross-guidance-from-multi-frame-images-for-human-reconstruction-paper">•CrossHuman:
Learning Cross-Guidance from Multi-Frame Images for Human Reconstruction
<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2207.09735v1.pdf">paper</a></h5>
<h5 id="unif-united-neural-implicit-functions-for-clothed-human-reconstruction-and-animation-paper-code">•UNIF:
United Neural Implicit Functions for Clothed Human Reconstruction and
Animation <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2207.09835v1.pdf">paper</a> <a target="_blank" rel="noopener" href="https://github.com/ShenhanQian/UNIF">code</a></h5>
<h5 id="drivable-volumetric-avatars-using-texel-aligned-features-paper">•Drivable
Volumetric Avatars using Texel-Aligned Features <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2207.09774v1.pdf">paper</a></h5>
<h5 id="the-one-where-they-reconstructed-3d-humans-and-environments-in-tv-shows-homepage">•The
One Where They Reconstructed 3D Humans and Environments in TV Shows <a target="_blank" rel="noopener" href="https://ethanweber.me/sitcoms3D/">homepage</a></h5>
<h5 id="avatargen-a-3d-generative-model-for-animatable-human-avatars-paper">•AvatarGen:
a 3D Generative Model for Animatable Human Avatars <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2208.00561.pdf">paper</a></h5>
<h5 id="voltemorph-realtime-controllable-and-generalisable-animation-of-volumetric-representations-paper">•VolTeMorph:
Realtime, Controllable and Generalisable Animation of Volumetric
Representations <a target="_blank" rel="noopener" href="https://www.arxiv-vanity.com/papers/2208.00949/?continueFlag=acd9680585ca1db48ed3cbc277e4da97">paper</a></h5>
<h5 id="multi-neus-3d-head-portraits-from-single-image-with-neural-implicit-functions-paper">•Multi-NeuS:
3D Head Portraits from Single Image with Neural Implicit Functions <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2209.04436.pdf">paper</a></h5>
<h5 id="learning-to-relight-portrait-images-via-a-virtual-light-stage-and-synthetic-to-real-adaptation-paper">•Learning
to Relight Portrait Images via a Virtual Light Stage and
Synthetic-to-Real Adaptation <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2209.10510.pdf">paper</a></h5>
<h3 id="geo-fusion">geo fusion</h3>
<h5 id="doublefusion-real-time-capture-of-human-performances-with-inner-body-shapes-from-a-single-depth-sensor-paper">•
DoubleFusion: Real-time Capture of Human Performances with Inner Body
Shapes from a Single Depth Sensor <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1804.06023">paper</a></h5>
<h5 id="robust-3d-self-portraits-in-seconds-paper">• Robust 3D
Self-portraits in Seconds <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2004.02460">paper</a></h5>
<h5 id="accurate-human-body-reconstruction-for-volumetric-video-paper">•
ACCURATE HUMAN BODY RECONSTRUCTION FOR VOLUMETRIC VIDEO <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2202.13118v1.pdf">paper</a></h5>
<h5 id="occlusionfusion-occlusion-aware-motion-estimation-for-real-time-dynamic-3d-reconstruction-paper-code">•
OcclusionFusion: Occlusion-aware Motion Estimation for Real-time Dynamic
3D Reconstruction <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2203.07977v1.pdf">paper</a> <a target="_blank" rel="noopener" href="https://github.com/wenbin-lin/OcclusionFusion/">code</a></h5>
<h3 id="photo">photo</h3>
<h5 id="portrait-reconstruction-and-relighting-using-the-sun-as-a-light-stage-homepage">•
Portrait Reconstruction and Relighting using the Sun as a Light Stage <a target="_blank" rel="noopener" href="https://grail.cs.washington.edu/projects/sunstage/">homepage</a></h5>
<h3 id="d-human-whole-body">3D human whole body</h3>
<h5 id="monocular-expressive-body-regression-through-body-driven-attention-paper-code">•
Monocular Expressive Body Regression through Body-Driven Attention <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2008.09062">paper</a> <a target="_blank" rel="noopener" href="https://github.com/vchoutas/expose">code</a></h5>
<h5 id="frankmocap-fast-monocular-3d-hand-and-body-motion-capture-by-regression-and-integration-paper-code">•
FrankMocap: Fast Monocular 3D Hand and Body Motion Capture by Regression
and Integration <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2008.08324.pdf">paper</a>
<a target="_blank" rel="noopener" href="https://github.com/facebookresearch/frankmocap">code</a></h5>
<h5 id="collaborative-regression-of-expressive-bodies-using-moderation-papercode">•
Collaborative Regression of Expressive Bodies using Moderation <a target="_blank" rel="noopener" href="https://pixie.is.tue.mpg.de/">paper&amp;code</a></h5>
<h5 id="monocular-real-time-full-body-capture-with-inter-part-correlations-papercode">•
Monocular Real-time Full Body Capture with Inter-part Correlations <a target="_blank" rel="noopener" href="https://calciferzh.github.io/publications/zhou2021monocular">paper&amp;code</a></h5>
<h5 id="monocular-real-time-hand-shape-and-motion-capture-using-multi-modal-data-papercode">•
Monocular Real-time Hand Shape and Motion Capture using Multi-modal Data
<a target="_blank" rel="noopener" href="https://calciferzh.github.io/publications/zhou2020monocular">paper&amp;code</a></h5>
<h5 id="real-time-rgbd-based-extended-body-pose-estimation-paper-code">•
Real-time RGBD-based Extended Body Pose Estimation <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2103.03663.pdf">paper</a> <a target="_blank" rel="noopener" href="https://github.com/rmbashirov/rgbd-kinect-pose">code</a></h5>
<h5 id="detailed-avatar-recovery-from-single-image-paper">• Detailed
Avatar Recovery from Single Image <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2108.02931v1.pdf">paper</a></h5>
<h5 id="lightweight-multi-person-total-motion-capture-using-sparse-multi-view-cameras-paper">•
Lightweight Multi-person Total Motion Capture Using Sparse Multi-view
Cameras <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2108.10378v1.pdf">paper</a></h5>
<h5 id="imposing-temporal-consistency-on-deep-monocular-body-shape-and-pose-estimation-paper">•
Imposing Temporal Consistency on Deep Monocular Body Shape and Pose
Estimation <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2202.03074v1.pdf">paper</a></h5>
<h5 id="piano-a-parametric-hand-bone-model-from-magnetic-resonance-imaging-github">•
PIANO: A Parametric Hand Bone Model from Magnetic Resonance Imaging <a target="_blank" rel="noopener" href="https://github.com/reyuwei/PIANO_model">github</a></h5>
<h5 id="goal-generating-4d-whole-body-motion-for-hand-object-grasping-paper-code">•
GOAL: Generating 4D Whole-Body Motion for Hand-Object Grasping <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2112.11454.pdf">paper</a> <a target="_blank" rel="noopener" href="https://github.com/otaheri/GOAL">code</a></h5>
<h5 id="pymaf-x-towards-well-aligned-full-body-model-regression-from-monocular-images-paper">•
PyMAF-X: Towards Well-aligned Full-body Model Regression from Monocular
Images <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2207.06400v1.pdf">paper</a></h5>
<h3 id="d-human-body">3D human body</h3>
<h5 id="pose2mesh-graph-convolutional-network-for-3d-human-pose-and-mesh-recovery-from-a-2d-human-pose-paper-code">•
Pose2Mesh: Graph Convolutional Network for 3D Human Pose and Mesh
Recovery from a 2D Human Pose <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2008.09047">paper</a> <a target="_blank" rel="noopener" href="https://github.com/hongsukchoi/Pose2Mesh_RELEASE">code</a></h5>
<h5 id="monocular-real-time-volumetric-performance-capture-paper-code">•
Monocular Real-Time Volumetric Performance Capture <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2007.13988">paper</a> <a target="_blank" rel="noopener" href="https://github.com/Project-Splinter/MonoPort">code</a></h5>
<h5 id="full-body-awareness-from-partial-observations-paper-code">•
Full-Body Awareness from Partial Observations <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2008.06046">paper</a> <a target="_blank" rel="noopener" href="https://github.com/crockwell/partial_humans">code</a></h5>
<h5 id="centerhmr-a-bottom-up-single-shot-method-for-multi-person-3d-mesh-recovery-from-a-single-image-paper-code">•
CenterHMR: a Bottom-up Single-shot Method for Multi-person 3D Mesh
Recovery from a Single Image <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2008.12272.pdf">paper</a> <a target="_blank" rel="noopener" href="https://github.com/Arthur151/CenterHMR">code</a></h5>
<h5 id="reconstructing-nba-players-paper-code">• Reconstructing NBA
players <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2007.13303.pdf">paper</a> <a target="_blank" rel="noopener" href="https://github.com/luyangzhu/NBA-Players">code</a></h5>
<h5 id="going-beyond-free-viewpoint-creating-animatable-volumetric-video-of-human-performances-paper">•
Going beyond Free Viewpoint: Creating Animatable Volumetric Video of
Human Performances <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2009.00922">paper</a></h5>
<h5 id="synthetic-training-for-accurate-3d-human-pose-and-shape-estimation-in-the-wild-paper-code">•
Synthetic Training for Accurate 3D Human Pose and Shape Estimation in
the Wild <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2009.10013.pdf">paper</a> <a target="_blank" rel="noopener" href="https://github.com/akashsengupta1997/STRAPS-3DHumanShapePose">code</a></h5>
<h5 id="monoclothcap-towards-temporally-coherent-clothing-capture-from-monocular-rgb-video-paper">•
MonoClothCap: Towards Temporally Coherent Clothing Capture from
Monocular RGB Video <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2009.10711.pdf">paper</a></h5>
<h5 id="multi-view-consistency-loss-for-improved-single-image-3d-reconstruction-of-clothed-people-paper-code">•
Multi-View Consistency Loss for Improved Single-Image 3D Reconstruction
of Clothed People <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2009.14162">paper</a>
<a target="_blank" rel="noopener" href="https://akincaliskan3d.github.io/MV3DH/">code</a></h5>
<h5 id="synthetic-training-for-monocular-human-mesh-recovery-paper">•
Synthetic Training for Monocular Human Mesh Recovery <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2010.14036">paper</a></h5>
<h5 id="pose2pose-3d-positional-pose-guided-3d-rotational-pose-prediction-for-expressive-3d-human-pose-and-mesh-estimation-paper">•
Pose2Pose: 3D Positional Pose-Guided 3D Rotational Pose Prediction for
Expressive 3D Human Pose and Mesh Estimation <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2011.11534.pdf">paper</a></h5>
<h5 id="deep-physics-aware-inference-of-cloth-deformation-for-monocular-human-performance-capture">•
Deep Physics-aware Inference of Cloth Deformation for Monocular Human
Performance Capture</h5>
<h5 id="d-human-body-capture-from-egocentric-video-via-3d-scene-grounding-paper">•
4D Human Body Capture from Egocentric Video via 3D Scene Grounding <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2011.13341">paper</a></h5>
<h5 id="hybrik-a-hybrid-analytical-neural-inverse-kinematics-solution-for-3d-human-pose-and-shape-estimation-paper-code">•
HybrIK: A Hybrid Analytical-Neural Inverse Kinematics Solution for 3D
Human Pose and Shape Estimation <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2011.14672.pdf">paper</a> <a target="_blank" rel="noopener" href="https://github.com/Jeff-sjtu/HybrIK">code</a></h5>
<h5 id="we-are-more-than-our-joints-predicting-how-3d-bodies-move-paper">•
We are More than Our Joints: Predicting how 3D Bodies Move <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2012.00619.pdf">paper</a></h5>
<h5 id="smply-benchmarking-3d-human-pose-estimation-in-the-wild-paper">•
SMPLy Benchmarking 3D Human Pose Estimation in the Wild <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2012.02743v1.pdf">paper</a></h5>
<h5 id="synthesizing-long-term-3d-human-motion-and-interaction-in-3d-paper">•
Synthesizing Long-Term 3D Human Motion and Interaction in 3D <a target="_blank" rel="noopener" href="https://jiashunwang.github.io/Long-term-Motion-in-3D-Scenes/">paper</a></h5>
<h5 id="detailed-3d-human-body-reconstruction-from-multi-view-images-combining-voxel-super-resolution-and-learned-implicit-representation">•
Detailed 3D Human Body Reconstruction from Multi-view Images Combining
Voxel Super-Resolution and Learned Implicit Representation</h5>
<h5 id="a-novel-joint-points-and-silhouette-based-method-to-estimate-3d-human-pose-and-shape">•
A novel joint points and silhouette-based method to estimate 3D human
pose and shape</h5>
<h5 id="facedet3d-facial-expressions-with-3d-geometric-detail-prediction-paper">•
FaceDet3D: Facial Expressions with 3D Geometric Detail Prediction <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2012.07999.pdf">paper</a></h5>
<h5 id="nerface-dynamic-neural-radiance-fields-for-monocular-4d-facial-avatar-reconstruction-code">•
NerFACE: Dynamic Neural Radiance Fields for Monocular 4D Facial Avatar
Reconstruction <a target="_blank" rel="noopener" href="https://github.com/gafniguy/4D-Facial-Avatars">code</a></h5>
<h5 id="learning-complex-3d-human-self-contact-paper">• Learning Complex
3D Human Self-Contact <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2012.10366.pdf">paper</a></h5>
<h5 id="populating-3d-scenes-by-learning-human-scene-interaction-paper">•
Populating 3D Scenes by Learning Human-Scene Interaction <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2012.11581.pdf">paper</a></h5>
<h5 id="anr-articulated-neural-rendering-for-virtual-avatars-paper">•
ANR: Articulated Neural Rendering for Virtual Avatars <a target="_blank" rel="noopener" href="https://anr-avatars.github.io/">paper</a></h5>
<h5 id="human-mesh-recovery-from-multiple-shots-paper">• Human Mesh
Recovery from Multiple Shots <a target="_blank" rel="noopener" href="https://geopavlakos.github.io/multishot/">paper</a></h5>
<h5 id="lifting-2d-stylegan-for-3d-aware-face-generation-paper">•
Lifting 2D StyleGAN for 3D-Aware Face Generation <a href="S3:%20Neural%20Shape,%20Skeleton,%20and%20Skinning%20Fields%20for%203D%20Human%20Modeling">paper</a></h5>
<h5 id="capturing-detailed-deformations-of-moving-human-bodies-paper">•
Capturing Detailed Deformations of Moving Human Bodies <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2102.07343.pdf">paper</a></h5>
<h5 id="d-human-pose-shape-and-texture-from-low-resolution-images-and-videos-paper">•
3D Human Pose, Shape and Texture from Low-Resolution Images and Videos
<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2103.06498v1.pdf">paper</a></h5>
<h5 id="challencap-monocular-3d-capture-of-challenging-human-performances-using-multi-modal-references-paper">•
ChallenCap: Monocular 3D Capture of Challenging Human Performances using
Multi-Modal References <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2103.06747v1.pdf">paper</a></h5>
<h5 id="smplicit-topology-aware-generative-model-for-clothed-people-papercode">•
SMPLicit: Topology-aware Generative Model for Clothed People <a target="_blank" rel="noopener" href="http://www.iri.upc.edu/people/ecorona/smplicit/">paper&amp;code</a></h5>
<h5 id="learning-high-fidelity-depths-of-dressed-humans-by-watching-social-media-dance-videos-paper">•
Learning High Fidelity Depths of Dressed Humans by Watching Social Media
Dance Videos <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2103.03319.pdf">paper</a></h5>
<h5 id="neuralhumanfvv-real-time-neural-volumetric-human-performance-rendering-using-rgb-cameras-paper">•
NeuralHumanFVV: Real-Time Neural Volumetric Human Performance Rendering
using RGB Cameras <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2103.07700v1.pdf">paper</a></h5>
<h5 id="probabilistic-3d-human-shape-and-pose-estimation-from-multiple-unconstrained-images-in-the-wild-paper">•
Probabilistic 3D Human Shape and Pose Estimation from Multiple
Unconstrained Images in the Wild <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2103.10978v1.pdf">paper</a></h5>
<h5 id="dcrowdnet-2d-human-pose-guided-3d-crowd-human-pose-and-shape-estimation-in-the-wild-paper">•
3DCrowdNet: 2D Human Pose-Guided 3D Crowd Human Pose and Shape
Estimation in the Wild <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2104.07300v1.pdf">paper</a></h5>
<h5 id="scale-modeling-clothed-humans-with-a-surface-codec-of-articulated-local-elements-paper">•
SCALE: Modeling Clothed Humans with a Surface Codec of Articulated Local
Elements <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2104.07660.pdf">paper</a></h5>
<h5 id="multi-person-implicit-reconstruction-from-a-single-image-paper">•
Multi-person Implicit Reconstruction from a Single Image <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2104.09283v1.pdf">paper</a></h5>
<h5 id="pare-part-attention-regressor-for-3d-human-body-estimation-paper-code">•
PARE: Part Attention Regressor for 3D Human Body Estimation <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2104.08527v1.pdf">paper</a> <a target="_blank" rel="noopener" href="https://pare.is.tue.mpg.de/">code</a></h5>
<h5 id="temporal-consistency-loss-for-high-resolution-textured-and-clothed-3d-human-reconstruction-from-monocular-video-paper">•
Temporal Consistency Loss for High Resolution Textured and Clothed 3D
Human Reconstruction from Monocular Video <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2104.09259.pdf">paper</a></h5>
<h5 id="function4d-real-time-human-volumetric-capture-from-very-sparse-consumer-rgbd-sensors-paper">•
Function4D: Real-time Human Volumetric Capture from Very Sparse Consumer
RGBD Sensors <a target="_blank" rel="noopener" href="http://www.liuyebin.com/Function4D/Function4D.html">paper</a></h5>
<h5 id="end-to-end-human-pose-and-mesh-reconstruction-with-transformers-paper">•
End-to-End Human Pose and Mesh Reconstruction with Transformers <a target="_blank" rel="noopener" href="https://github.com/microsoft/MeshTransformer">paper</a></h5>
<h5 id="revitalizing-optimization-for-3d-human-pose-and-shape-estimation-a-sparse-constrained-formulation-paper">•
Revitalizing Optimization for 3D Human Pose and Shape Estimation: A
Sparse Constrained Formulation <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2105.13965v1.pdf">paper</a></h5>
<h5 id="sharp-shape-aware-reconstruction-of-people-in-loose-clothing-paper">•
SHARP: Shape-Aware Reconstruction of People In Loose Clothing <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2106.04778v1.pdf">paper</a></h5>
<h5 id="thundr-transformer-based-3d-human-reconstruction-with-markers-paper">•
THUNDR: Transformer-based 3D HUmaN Reconstruction with Markers <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2106.09336v1.pdf">paper</a></h5>
<h5 id="deep3dpose-realtime-reconstruction-of-arbitrarily-posed-human-bodies-from-single-rgb-images-paper">•
Deep3DPose: Realtime Reconstruction of Arbitrarily Posed Human Bodies
from Single RGB Images <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2106.11536v1.pdf">paper</a></h5>
<h5 id="learning-local-recurrent-models-for-human-mesh-recovery-paper">•
Learning Local Recurrent Models for Human Mesh Recovery <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2107.12847v1.pdf">paper</a></h5>
<h5 id="posefusion2-simultaneous-background-reconstruction-and-human-shape-recovery-in-real-time-paper">•
PoseFusion2: Simultaneous Background Reconstruction and Human Shape
Recovery in Real-time <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2108.00695v1.pdf">paper</a></h5>
<h5 id="lasor-learning-accurate-3d-human-pose-and-shape-via-synthetic-occlusion-aware-data-and-neural-mesh-rendering-paper">•
LASOR: Learning Accurate 3D Human Pose and Shape Via Synthetic
Occlusion-Aware Data and Neural Mesh Rendering <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2108.00351v1.pdf">paper</a></h5>
<h5 id="learning-motion-priors-for-4d-human-body-capture-in-3d-scenes-paper-code">•
Learning Motion Priors for 4D Human Body Capture in 3D Scenes <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2108.10399v1.pdf">paper</a> <a target="_blank" rel="noopener" href="https://github.com/sanweiliti/LEMO">code</a></h5>
<h5 id="probabilistic-modeling-for-human-mesh-recovery-papercode">•
Probabilistic Modeling for Human Mesh Recovery <a target="_blank" rel="noopener" href="https://www.seas.upenn.edu/~nkolot/projects/prohmr/">paper&amp;code</a></h5>
<h5 id="dc-gnet-deep-mesh-relation-capturing-graph-convolution-network-for-3d-human-shape-reconstruction-paper">•
DC-GNet: Deep Mesh Relation Capturing Graph Convolution Network for 3D
Human Shape Reconstruction <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2108.12384v1.pdf">paper</a></h5>
<h5 id="encoder-decoder-with-multi-level-attention-for-3d-human-shape-and-pose-estimation-paper-code">•
Encoder-decoder with Multi-level Attention for 3D Human Shape and Pose
Estimation <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2109.02303v1.pdf">paper</a> <a target="_blank" rel="noopener" href="https://github.com/ziniuwan/maed">code</a></h5>
<h5 id="action-conditioned-3d-human-motion-synthesis-with-transformer-vae-code">•
Action-Conditioned 3D Human Motion Synthesis with Transformer VAE <a target="_blank" rel="noopener" href="https://github.com/Mathux/ACTOR">code</a></h5>
<h5 id="learning-to-regress-bodies-from-images-using-differentiable-semantic-rendering-paper">•
Learning to Regress Bodies from Images using Differentiable Semantic
Rendering <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2110.03480v1.pdf">paper</a></h5>
<h5 id="deep-two-stream-video-inference-for-human-body-pose-and-shape-estimation-paper">•
Deep Two-Stream Video Inference for Human Body Pose and Shape Estimation
<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2110.11680v1.pdf">paper</a></h5>
<h5 id="ultrapose-synthesizing-dense-pose-with-1-billion-points-by-human-body-decoupling-3d-model-paper">•
UltraPose: Synthesizing Dense Pose with 1 Billion Points by Human-body
Decoupling 3D Model <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2110.15267v1.pdf">paper</a></h5>
<h5 id="body-size-and-depth-disambiguation-in-multi-person-reconstruction-from-single-images-paper-code">•
Body Size and Depth Disambiguation in Multi-Person Reconstruction from
Single Images <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2111.01884v1.pdf">paper</a>
<a target="_blank" rel="noopener" href="https://github.com/nicolasugrinovic/size_depth_disambiguation">code</a></h5>
<h5 id="unified-3d-mesh-recovery-of-humans-and-animals-by-learning-animal-exercise-paper">•
Unified 3D Mesh Recovery of Humans and Animals by Learning Animal
Exercise <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2111.02450v1.pdf">paper</a></h5>
<h5 id="d-human-shape-and-pose-from-a-single-low-resolution-image-with-self-supervised-learning-paper-code">•
3D Human Shape and Pose from a Single Low-Resolution Image with
Self-Supervised Learning <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2007.13666.pdf">paper</a> <a target="_blank" rel="noopener" href="https://github.com/xuxy09/RSC-Net">code</a></h5>
<h5 id="out-of-domain-human-mesh-reconstruction-via-bilevel-online-adaptation-paper-code">•
Out-of-Domain Human Mesh Reconstruction via Bilevel Online Adaptation <a target="_blank" rel="noopener" href="https://drive.google.com/file/d/1b6e3rMrVn_xNhM-MitqpLtulARdl4M9F/view?usp=sharing">paper</a>
<a target="_blank" rel="noopener" href="https://www.google.com/url?q=https%3A%2F%2Fgithub.com%2Fsyguan96%2FDynaBOA&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNHYmgSyYqdKGYNp7W-bAO2MrHfp1w">code</a></h5>
<h4 id="meshletemp-leveraging-the-learnable-vertex-vertex-relationship-to-generalize-human-pose-and-mesh-reconstruction-for-in-the-wild-scenes-paper">•
MeshLeTemp: Leveraging the Learnable Vertex-Vertex Relationship to
Generalize Human Pose and Mesh Reconstruction for In-the-Wild Scenes <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2202.07228v1.pdf">paper</a></h4>
<h5 id="monocular-human-shape-and-pose-with-dense-mesh-borne-local-image-features-paper">•
Monocular Human Shape and Pose with Dense Mesh-borne Local Image
Features <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2111.05319v1.pdf">paper</a></h5>
<h5 id="human-performance-capture-from-monocular-video-in-the-wild-paper">•
Human Performance Capture from Monocular Video in the Wild <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2111.14672.pdf">paper</a></h5>
<h5 id="probabilistic-estimation-of-3d-human-shape-and-pose-with-a-semantic-local-parametric-model-paper">•
Probabilistic Estimation of 3D Human Shape and Pose with a Semantic
Local Parametric Model <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2111.15404v1.pdf">paper</a></h5>
<h5 id="glamr-global-occlusion-aware-human-mesh-recovery-with-dynamic-cameras-paper-code">•
GLAMR: Global Occlusion-Aware Human Mesh Recovery with Dynamic Cameras
<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2112.01524v1.pdf">paper</a> <a target="_blank" rel="noopener" href="https://github.com/NVlabs/GLAMR">code</a></h5>
<h5 id="metaavatar-learning-animatable-clothed-human-models-from-few-depth-images-paper-code">•
MetaAvatar: Learning Animatable Clothed Human Models from Few Depth
Images} <a target="_blank" rel="noopener" href="https://openreview.net/forum?id=Q-PA3D1OsDz">paper</a>
<a target="_blank" rel="noopener" href="https://github.com/taconite/MetaAvatar-release">code</a></h5>
<h5 id="egobody-human-body-shape-motion-and-social-interactions-from-head-mounted-devices-paper-code">•
EgoBody: Human Body Shape, Motion and Social Interactions from
Head-Mounted Devices <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2112.07642v1.pdf">paper</a> <a target="_blank" rel="noopener" href="https://sanweiliti.github.io/egobody/egobody.html">code</a></h5>
<h5 id="putting-people-in-their-place-monocular-regression-of-3d-people-in-depth-paper">•
Putting People in their Place: Monocular Regression of 3D People in
Depth <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2112.08274v1.pdf">paper</a></h5>
<h5 id="multi-initialization-optimization-network-for-accurate-3d-human-pose-and-shape-estimation-paper">•
Multi-initialization Optimization Network for Accurate 3D Human Pose and
Shape Estimation <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2112.12917v1.pdf">paper</a></h5>
<h5 id="moothnet-a-plug-and-play-network-for-refining-human-poses-in-videos-paper">•
moothNet: A Plug-and-Play Network for Refining Human Poses in Videos <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2112.13715v1.pdf">paper</a></h5>
<h5 id="hspace-synthetic-parametric-humans-animated-in-complex-environments-paper">•
HSPACE: Synthetic Parametric Humans Animated in Complex Environments <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2112.12867v1.pdf">paper</a></h5>
<h5 id="votehmr-occlusion-aware-voting-network-for-robust-3d-human-mesh-recovery-from-partial-point-clouds-paper-code">•
VoteHMR: Occlusion-Aware Voting Network for Robust 3D Human Mesh
Recovery from Partial Point Clouds <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2110.08729.pdf">paper</a> <a target="_blank" rel="noopener" href="https://github.com/hanabi7/VoteHMR">code</a></h5>
<h5 id="h4d-human-4d-modeling-by-learning-neural-compositional-representation-paper">•
H4D: Human 4D Modeling by Learning Neural Compositional Representation
<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2203.01247v1.pdf">paper</a></h5>
<h5 id="capturing-humans-in-motion-temporal-attentive-3d-human-pose-and-shape-estimation-from-monocular-video-paper-code">•
Capturing Humans in Motion: Temporal-Attentive 3D Human Pose and Shape
Estimation from Monocular Video <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2203.08534v1.pdf">paper</a> <a target="_blank" rel="noopener" href="https://mps-net.github.io/MPS-Net/">code</a></h5>
<h5 id="hybridcap-inertia-aid-monocular-capture-of-challenging-human-motions-paper">•
HybridCap: Inertia-aid Monocular Capture of Challenging Human Motions <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2203.09287v1.pdf">paper</a></h5>
<h5 id="learning-to-estimate-robust-3d-human-mesh-from-in-the-wild-crowded-scenes-paper-code">•
Learning to Estimate Robust 3D Human Mesh from In-the-Wild Crowded
Scenes <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2104.07300">paper</a> <a target="_blank" rel="noopener" href="https://github.com/hongsukchoi/3DCrowdNet_RELEASE">code</a></h5>
<h5 id="bodyslam-joint-camera-localisation-mapping-and-human-motion-tracking-paper">•
BodySLAM: Joint Camera Localisation, Mapping, and Human Motion Tracking
<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2205.02301v1.pdf">paper</a></h5>
<h5 id="hulc-3d-human-motion-capture-with-pose-manifold-sampling-and-dense-contact-guidance-paper">•
HULC: 3D Human Motion Capture with Pose Manifold Sampling and Dense
Contact Guidance <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2205.05677v1.pdf">paper</a></h5>
<h5 id="learned-vertex-descent-a-new-direction-for-3d-human-model-fitting-paper-code">•
Learned Vertex Descent: A New Direction for 3D Human Model Fitting <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2205.06254v1.pdf">paper</a> <a target="_blank" rel="noopener" href="https://github.com/enriccorona/LVD">code</a></h5>
<h5 id="mug-multi-human-graph-network-for-3d-mesh-reconstruction-from-2d-pose-paper">•
MUG: Multi-human Graph Network for 3D Mesh Reconstruction from 2D Pose
<a target="_blank" rel="noopener" href="https://www.arxivdaily.com/thread/26950">paper</a></h5>
<h5 id="accurate-3d-body-shape-regression-using-metric-and-semantic-attributes-paper">•
Accurate 3D Body Shape Regression using Metric and Semantic Attributes
<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2206.07036v1.pdf">paper</a></h5>
<h5 id="capturing-and-inferring-dense-full-body-human-scene-contact-homepage">•
Capturing and Inferring Dense Full-Body Human-Scene Contact <a target="_blank" rel="noopener" href="https://rich.is.tue.mpg.de/index.html">homepage</a></h5>
<h5 id="occluded-human-body-capture-with-self-supervised-spatial-temporal-motion-prior-paper">•
Occluded Human Body Capture with Self-Supervised Spatial-Temporal Motion
Prior <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2207.05375v1.pdf">paper</a></h5>
<h5 id="live-stream-temporally-embedded-3d-human-body-pose-and-shape-estimation-paper-code">•
Live Stream Temporally Embedded 3D Human Body Pose and Shape Estimation
<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2207.12537v1.pdf">paper</a> <a target="_blank" rel="noopener" href="https://github.com/ostadabbas/TePose">code</a></h5>
<h5 id="cliff-carrying-location-information-in-full-frames-into-human-pose-and-shape-estimation-paper">•
CLIFF: Carrying Location Information in Full Frames into Human Pose and
Shape Estimation <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2208.00571.pdf">paper</a></h5>
<h3 id="d-human-face">3d human face</h3>
<h5 id="high-fidelity-3d-digital-human-creation-from-rgb-d-selfies-paper-code">•
High-Fidelity 3D Digital Human Creation from RGB-D Selfies <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2010.05562.pdf">paper</a> <a target="_blank" rel="noopener" href="https://github.com/tencent-ailab/hifi3dface">code</a></h5>
<h5 id="styleuv-diverse-and-high-quality-uv-map-generative-model-paper">•
StyleUV: Diverse and High-quality UV Map Generative Model <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2011.12893.pdf">paper</a></h5>
<h5 id="i3dmm-deep-implicit-3d-morphable-model-of-human-heads-paper">•
i3DMM: Deep Implicit 3D Morphable Model of Human Heads <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2011.14143v1.pdf">paper</a></h5>
<h5 id="relightable-3d-head-portraits-from-a-smartphone-video-paper">•
Relightable 3D Head Portraits from a Smartphone Video <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2012.09963.pdf">paper</a></h5>
<h5 id="learning-compositional-radiance-fields-of-dynamic-human-heads-paper">•
Learning Compositional Radiance Fields of Dynamic Human Heads <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2012.09955.pdf">paper</a></h5>
<h5 id="sider-single-image-neural-optimization-for-facial-geometric-detail-recovery-paper">•
SIDER : Single-Image Neural Optimization for Facial Geometric Detail
Recovery <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2108.05465v1.pdf">paper</a></h5>
<h5 id="synergy-between-3dmm-and-3d-landmarks-for-accurate-3d-facial-geometry-paper-code">•
Synergy between 3DMM and 3D Landmarks for Accurate 3D Facial Geometry <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2110.09772.pdf">paper</a> <a target="_blank" rel="noopener" href="https://github.com/choyingw/SynergyNet">code</a></h5>
<h5 id="high-quality-real-time-facial-capture-based-on-single-camera-paper">•
HIGH-QUALITY REAL TIME FACIAL CAPTURE BASED ON SINGLE CAMERA <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2111.07556v1.pdf">paper</a></h5>
<h5 id="self-supervised-high-fidelity-and-re-renderable-3d-facial-reconstruction-from-a-single-imag-paper">•
Self-supervised High-fidelity and Re-renderable 3D Facial Reconstruction
from a Single Imag <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2111.08282.pdf">paper</a></h5>
<h5 id="generating-diverse-3d-reconstructions-from-a-single-occluded-face-image-paper">•
Generating Diverse 3D Reconstructions from a Single Occluded Face Image
<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2112.00879v1.pdf">paper</a></h5>
<h5 id="self-supervised-robustifying-guidance-for-monocular-3d-face-reconstruction-paper">•
Self-Supervised Robustifying Guidance for Monocular 3D Face
Reconstruction <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2112.14382v1.pdf">paper</a></h5>
<h5 id="babynet-reconstructing-3d-faces-of-babies-from-uncalibrated-photographs-paper">•
BabyNet: Reconstructing 3D faces of babies from uncalibrated photographs
<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2203.05908v1.pdf">paper</a></h5>
<h5 id="s2f2-self-supervised-high-fidelity-face-reconstruction-from-monocular-image-paper">•
S2F2: Self-Supervised High Fidelity Face Reconstruction from Monocular
Image <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2203.07732v1.pdf">paper</a></h5>
<h5 id="facial-geometric-detail-recovery-via-implicit-representation-paper-code">•
Facial Geometric Detail Recovery via Implicit Representation <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2203.09692v1.pdf">paper</a> <a target="_blank" rel="noopener" href="https://github.com/deepinsight/insightface/tree/master/reconstruction/PBIDR">code</a></h5>
<h5 id="beyond-3dmm-learning-to-capture-high-fidelity-3d-face-shape-paper">•
Beyond 3DMM: Learning to Capture High-fidelity 3D Face Shape <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2204.04379v1.pdf">paper</a></h5>
<h5 id="from-2d-images-to-3d-model-weakly-supervised-multi-view-face-reconstruction-with-deep-fusion-paper">•
From 2D Images to 3D Model: Weakly Supervised Multi-View Face
Reconstruction with Deep Fusion <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2204.03842v1.pdf">paper</a></h5>
<h5 id="f3d-face-reconstruction-with-dense-landmarks-paper">• F3D face
reconstruction with dense landmarks <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2204.02776v1.pdf">paper</a></h5>
<h5 id="emoca-emotion-driven-monocular-face-capture-and-animation-paper">•
EMOCA: Emotion Driven Monocular Face Capture and Animation <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2204.11312v1.pdf">paper</a></h5>
<h5 id="single-image-3d-face-reconstruction-under-perspective-projection-paper">•
Single-Image 3D Face Reconstruction under Perspective Projection <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2205.04126v1.pdf">paper</a></h5>
<h3 id="d-human-head">3d human head</h3>
<h5 id="deca-detailed-expression-capture-and-animation-paper-code">•
DECA: Detailed Expression Capture and Animation <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2012.04012.pdf">paper</a> <a target="_blank" rel="noopener" href="https://github.com/YadiraF/DECA?utm_source=catalyzex.com">code</a></h5>
<h5 id="pixel-codec-avatars-paper">• Pixel Codec Avatars <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2104.04638.pdf">paper</a></h5>
<h5 id="h3d-net-few-shot-high-fidelity-3d-head-reconstruction-paper">•
H3D-Net: Few-Shot High-Fidelity 3D Head Reconstruction <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2107.12512v1.pdf">paper</a></h5>
<h5 id="towards-metrical-reconstruction-of-human-faces-paper-code">•Towards
Metrical Reconstruction of Human Faces <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2204.06607v1.pdf">paper</a> <a target="_blank" rel="noopener" href="https://zielon.github.io/mica/">code</a></h5>
<h5 id="data-driven-3d-human-head-reconstruction-paper">•Data-driven 3D
human head reconstruction <a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/abs/pii/S0097849319300317">paper</a></h5>
<h5 id="dynamic-3d-avatar-creation-from-hand-held-video-input-acm-paper">•Dynamic
3D avatar creation from hand-held video input, ACM <a target="_blank" rel="noopener" href="http://sofienbouaziz.com/pdf/Avatars_SIGG15.pdf">paper</a></h5>
<h5 id="realistic-one-shot-mesh-based-head-avatars-paper">•Realistic
One-shot Mesh-based Head Avatars <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2206.08343.pdf">paper</a></h5>
<h5 id="authentic-volumetric-avatars-from-a-phone-scan-paper">•Authentic
Volumetric Avatars from a Phone Scan <a target="_blank" rel="noopener" href="https://drive.google.com/file/d/1i4NJKAggS82wqMamCJ1OHRGgViuyoY6R/view">paper</a></h5>
<h5 id="neural-head-avatars-from-monocular-rgb-videos-homepage">•Neural
Head Avatars from Monocular RGB Videos <a target="_blank" rel="noopener" href="https://philgras.github.io/neural_head_avatars/neural_head_avatars.html">homepage</a></h5>
<h5 id="towards-metrical-reconstruction-of-human-faces-homepage">•Towards
Metrical Reconstruction of Human Faces <a target="_blank" rel="noopener" href="https://zielon.github.io/mica/">homepage</a></h5>
<h3 id="d-human-hand">3D human hand</h3>
<h5 id="active-learning-for-bayesian-3d-hand-pose-estimation-paper-code">•
Active Learning for Bayesian 3D Hand Pose Estimation <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2010.00694.pdf">paper</a> <a target="_blank" rel="noopener" href="https://github.com/razvancaramalau/al_bhpe">code</a></h5>
<h5 id="multi-view-consistency-loss-for-improved-single-image-3d-reconstruction-of-clothed-people-paper-code-1">•
Multi-View Consistency Loss for Improved Single-Image 3D Reconstruction
of Clothed People <a target="_blank" rel="noopener" href="https://akincaliskan3d.github.io/MV3DH//resources/ACCV_Cam_Ready_Multi_View_3D_Human.pdf">paper</a>
<a target="_blank" rel="noopener" href="https://github.com/akcalakcal/Multi_View_Consistent_Single_Image_3D_Human_Reconstruction">code</a></h5>
<h5 id="eventhands-real-time-neural-3d-hand-reconstruction-from-an-event-stream">•
EventHands: Real-Time Neural 3D Hand Reconstruction from an Event
Stream</h5>
<h5 id="monocular-real-time-full-body-capture-with-inter-part-correlations">•
Monocular Real-time Full Body Capture with Inter-part Correlations</h5>
<h5 id="im2mesh-gan-accurate-3d-hand-mesh-recovery-from-a-single-rgb-image">•
Im2Mesh GAN: Accurate 3D Hand Mesh Recovery from a Single RGB Image</h5>
<h5 id="handtailor-towards-high-precision-monocular-3d-hand-recovery-paper-code">•
HandTailor: Towards High-Precision Monocular 3D Hand Recovery <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2102.09244v1.pdf">paper</a> <a target="_blank" rel="noopener" href="https://github.com/LyuJ1998/HandTailor">code</a></h5>
<h5 id="camera-space-hand-mesh-recovery-via-semantic-aggregation-and-adaptive-2d-1d-registration-paper-code">•
Camera-Space Hand Mesh Recovery via Semantic Aggregation and Adaptive
2D-1D Registration <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2103.02845v1.pdf">paper</a> <a target="_blank" rel="noopener" href="https://github.com/SeanChenxy/HandMesh">code</a></h5>
<h5 id="model-based-3d-hand-reconstruction-via-self-supervised-learning-paper-code">•
Model-based 3D Hand Reconstruction via Self-Supervised Learning <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2103.11703v1.pdf">paper</a> <a target="_blank" rel="noopener" href="https://github.com/TerenceCYJ/S2HAND">code</a></h5>
<h5 id="action-conditioned-3d-human-motion-synthesis-with-transformer-vae-paper">•
Action-Conditioned 3D Human Motion Synthesis with Transformer VAE <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2104.05670.pdf">paper</a></h5>
<h5 id="semi-supervised-3d-hand-object-poses-estimation-with-interactions-in-time-paper">•
Semi-Supervised 3D Hand-Object Poses Estimation with Interactions in
Time <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2106.05266v1.pdf">paper</a></h5>
<h5 id="rgb2hands-real-time-tracking-of-3d-hand-interactions-from-monocular-rgb-video-paper">•
RGB2Hands: Real-Time Tracking of 3D Hand Interactions from Monocular RGB
Video <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2106.11589v1.pdf">paper</a></h5>
<h5 id="artiboost-boosting-articulated-3d-hand-object-pose-estimation-via-online-exploration-and-synthesis-paper-code">•
ArtiBoost: Boosting Articulated 3D Hand-Object Pose Estimation via
Online Exploration and Synthesis <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2109.05488v1.pdf">paper</a> <a target="_blank" rel="noopener" href="https://github.com/MVIG-SJTU/ArtiBoost">code</a></h5>
<h5 id="monocular-3d-reconstruction-of-interacting-hands-via-collision-aware-factorized-refinements-paper-code">•
Monocular 3D Reconstruction of Interacting Hands via Collision-Aware
Factorized Refinements <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2111.00763v1.pdf">paper</a> <a target="_blank" rel="noopener" href="https://penincillin.github.io/ihmr_3dv2021">code</a></h5>
<h5 id="dynamic-iterative-refinement-for-efficient-3d-hand-pose-estimation-paper">•
Dynamic Iterative Refinement for Efficient 3D Hand Pose Estimation <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2111.06500v1.pdf">paper</a></h5>
<h5 id="semi-supervised-3d-hand-shape-and-pose-estimation-with-label-propagation-paper">•
Semi-Supervised 3D Hand Shape and Pose Estimation with Label Propagation
<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2111.15199v1.pdf">paper</a></h5>
<h5 id="mobrecon-mobile-friendly-hand-mesh-reconstruction-from-monocular-image-paper-code">•
MobRecon: Mobile-Friendly Hand Mesh Reconstruction from Monocular Image
<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2112.02753v1.pdf">paper</a> <a target="_blank" rel="noopener" href="https://github.com/SeanChenxy/HandMesh">code</a></h5>
<h5 id="consistent-3d-hand-reconstruction-in-video-via-self-supervised-learning-paper">•
Consistent 3D Hand Reconstruction in Video via Self-Supervised Learning
<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2201.09548v1.pdf">paper</a></h5>
<h5 id="interacting-attention-graph-for-single-image-two-hand-reconstruction-paper">•
Interacting Attention Graph for Single Image Two-Hand Reconstruction <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2203.09364v1.pdf">paper</a></h5>
<h5 id="handoccnet-occlusion-robust-3d-hand-mesh-estimation-network-paper-code">•
HandOccNet: Occlusion-Robust 3D Hand Mesh Estimation Network <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2203.14564">paper</a> <a target="_blank" rel="noopener" href="https://github.com/namepllet/HandOccNet">code</a></h5>
<h5 id="toch-spatio-temporal-object-correspondence-to-hand-for-motion-refinement-paper">•
TOCH: Spatio-Temporal Object Correspondence to Hand for Motion
Refinement <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2205.07982v1.pdf">paper</a></h5>
<h5 id="end-to-end-3d-hand-pose-estimation-from-stereo-cameras-paper">•
End-to-End 3D Hand Pose Estimation from Stereo Cameras <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2206.01384v1.pdf">paper</a></h5>
<h5 id="efficient-annotation-and-learning-for-3dhand-pose-estimation-a-survey-paper">•
Efficient Annotation and Learning for 3DHand Pose Estimation: A Survey
<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2206.02257v1.pdf">paper</a></h5>
<h5 id="d-interacting-hand-pose-estimation-by-hand-de-occlusion-and-removal-code">•
3D Interacting Hand Pose Estimation by Hand De-occlusion and Removal <a target="_blank" rel="noopener" href="https://github.com/MengHao666/HDR">code</a></h5>
<h3 id="d-hair">3d hair</h3>
<h5 id="neuralhdhair-automatic-high-fidelity-hair-modeling-from-a-single-image-using-implicit-neural-representations-paper">•
NeuralHDHair: Automatic High-fidelity Hair Modeling from a Single Image
Using Implicit Neural Representations <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2205.04175v1.pdf">paper</a></h5>
<h5 id="d-hair-synthesis-using-volumetric-variational-autoencoders">•3D
hair synthesis using volumetric variational autoencoders</h5>
<h5 id="ao-cnn-filament-aware-hair-reconstruction-based-on-volumetric-vector-fields">•AO-CNN:
filament-aware hair reconstruction based on volumetric vector
fields</h5>
<h5 id="neural-strands-learning-hair-geometry-and-appearance-from-multi-view-images-paper">•Neural
Strands: Learning Hair Geometry and Appearance from Multi-View Images <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2207.14067">paper</a></h5>
<h3 id="d-teeth">3d teeth</h3>
<h5 id="model-based-teeth-reconstruction-paper">• Model-based teeth
reconstruction <a target="_blank" rel="noopener" href="https://vcai.mpi-inf.mpg.de/projects/MZ/Papers/SGASIA2016_TR/page.html">paper</a></h5>
<h3 id="d-eyelids">3d eyelids</h3>
<h5 id="real-time-3d-eyelids-tracking-from-semantic-edges-paper">•
Real-time 3D Eyelids Tracking from Semantic Edges <a target="_blank" rel="noopener" href="http://xufeng.site/publications/2017/2017_Real-time%203D%20Eyelids%20Tracking%20from%20Semantic%20Edges-min.pdf">paper</a></h5>
<h2 id="related">related</h2>
<h3 id="human-mattting">human mattting</h3>
<h5 id="real-time-high-resolution-background-matting-code">• Real-Time
High-Resolution Background Matting <a target="_blank" rel="noopener" href="https://github.com/PeterL1n/BackgroundMattingV2">code</a></h5>
<h5 id="real-time-monocular-human-depth-estimation-and-segmentation-on-embedded-systems-paper">•
Real-Time Monocular Human Depth Estimation and Segmentation on Embedded
Systems <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2108.10506v1.pdf">paper</a></h5>
<h5 id="deepsportlab-a-unified-framework-for-ball-detection-player-instance-segmentation-and-pose-estimation-in-team-sports-scenes-paper">•
DeepSportLab: a Unified Framework for Ball Detection, Player Instance
Segmentation and Pose Estimation in Team Sports Scenes <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2112.00627v1.pdf">paper</a></h5>
<h5 id="pp-humanseg-connectivity-aware-portrait-segmentation-with-a-large-scale-teleconferencing-video-dataset-paper-code">•
PP-HumanSeg: Connectivity-Aware Portrait Segmentation with a Large-Scale
Teleconferencing Video Dataset <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2112.07146v1.pdf">paper</a> <a target="_blank" rel="noopener" href="https://github.com/PaddlePaddle/PaddleSeg">code</a></h5>
<h5 id="portrait-segmentation-using-deep-learning-paper">• Portrait
Segmentation Using Deep Learning <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2202.02705v1.pdf">paper</a></h5>
<h5 id="human-instance-matting-via-mutual-guidance-and-multi-instance-refinement-paper-code">•
Human Instance Matting via Mutual Guidance and Multi-Instance Refinement
<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2205.10767v1.pdf">paper</a> <a target="_blank" rel="noopener" href="https://github.com/nowsyn/InstMatt">code</a></h5>
<h3 id="pose-estimation">pose estimation</h3>
<h5 id="canonpose-self-supervised-monocular-3d-human-pose-estimation-in-the-wild-paper">•
CanonPose: Self-Supervised Monocular 3D Human Pose Estimation in the
Wild <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2011.14679.pdf">paper</a></h5>
<h5 id="active-learning-for-bayesian-3d-hand-pose-estimation-paper-code-1">•
Active Learning for Bayesian 3D Hand Pose Estimation <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2010.00694v2.pdf">paper</a> <a target="_blank" rel="noopener" href="https://github.com/razvancaramalau/al_bhpe">code</a></h5>
<h5 id="a-nerf-surface-free-human-3d-pose-refinement-via-neural-rendering">•
A-NeRF: Surface-free Human 3D Pose Refinement via Neural Rendering</h5>
<h5 id="handsformer-keypoint-transformer-for-monocular-3d-pose-estimation-of-hands-and-object-in-interaction-paper">•
HandsFormer: Keypoint Transformer for Monocular 3D Pose Estimation of
Hands and Object in Interaction <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2104.14639v1.pdf">paper</a></h5>
<h5 id="humor-3d-human-motion-model-for-robust-pose-estimation-paper">•
HuMoR: 3D Human Motion Model for Robust Pose Estimation <a target="_blank" rel="noopener" href="https://geometry.stanford.edu/projects/humor/">paper</a></h5>
<h5 id="multi-person-extreme-motion-prediction-with-cross-interaction-attention-paper">•
Multi-Person Extreme Motion Prediction with Cross-Interaction Attention
<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2105.08825">paper</a></h5>
<h5 id="voxeltrack-multi-person-3d-human-pose-estimation-and-tracking-in-the-wild-paper">•
VoxelTrack: Multi-Person 3D Human Pose Estimation and Tracking in the
Wild <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2108.02452v1.pdf">paper</a></h5>
<h5 id="gravity-aware-monocular-3d-human-object-reconstruction-papercode">•
Gravity-Aware Monocular 3D Human-Object Reconstruction <a target="_blank" rel="noopener" href="http://4dqv.mpi-inf.mpg.de/GraviCap/">paper&amp;code</a></h5>
<h5 id="densepose-3d-lifting-canonical-surface-maps-of-articulated-objects-to-the-third-dimension-paper">•
DensePose 3D: Lifting Canonical Surface Maps of Articulated Objects to
the Third Dimension <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2109.00033v1.pdf">paper</a></h5>
<h5 id="graph-based-3d-multi-person-pose-estimation-using-multi-view-images-paper">•
Graph-Based 3D Multi-Person Pose Estimation Using Multi-View Images <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2109.05885v1.pdf">paper</a></h5>
<h5 id="learning-dynamical-human-joint-affinity-for-3d-pose-estimation-in-videos-paper">•
Learning Dynamical Human-Joint Affinity for 3D Pose Estimation in Videos
<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2109.07353v1.pdf">paper</a></h5>
<h5 id="physics-based-human-motion-estimation-and-synthesis-from-videos-paper">•
Physics-based Human Motion Estimation and Synthesis from Videos <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2109.09913.pdf">paper</a></h5>
<h5 id="real-time-low-cost-multi-person-3d-pose-estimation-paper">•
Real-time, low-cost multi-person 3D pose estimation <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2110.11414v1.pdf">paper</a></h5>
<h5 id="direct-multi-view-multi-person-3d-human-pose-estimation-paper-code">•
Direct Multi-view Multi-person 3D Human Pose Estimation <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2111.04076.pdf">paper</a> <a target="_blank" rel="noopener" href="https://github.com/sail-sg/mvp">code</a></h5>
<h5 id="rethinking-keypoint-representations-modeling-keypoints-and-poses-as-objects-for-multi-person-human-pose-estimation-code">•
Rethinking Keypoint Representations: Modeling Keypoints and Poses as
Objects for Multi-Person Human Pose Estimation <a target="_blank" rel="noopener" href="https://github.com/wmcnally/kapao">code</a></h5>
<h5 id="camera-distortion-aware-3d-human-pose-estimation-in-video-with-optimization-based-meta-learning-paper">•
Camera Distortion-aware 3D Human Pose Estimation in Video with
Optimization-based Meta-Learning <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2111.15056v1.pdf">paper</a></h5>
<h5 id="in-bed-human-pose-estimation-from-unseen-and-privacy-preserving-image-domains-paper">•
In-Bed Human Pose Estimation from Unseen and Privacy-Preserving Image
Domains <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2111.15124v1.pdf">paper</a></h5>
<h5 id="camera-motion-agnostic-3d-human-pose-estimation-paper-code">•
Camera Motion Agnostic 3D Human Pose Estimation <a target="_blank" rel="noopener" href="https://github.com/seonghyunkim1212/GMR">paper</a> <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2112.00343v1.pdf">code</a></h5>
<h5 id="elepose-unsupervised-3d-human-pose-estimation-by-predicting-camera-elevation-and-learning-normalizing-flows-on-2d-poses-paper">•
ElePose: Unsupervised 3D Human Pose Estimation by Predicting Camera
Elevation and Learning Normalizing Flows on 2D Poses <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2112.07088v1.pdf">paper</a></h5>
<h5 id="multi-modal-3d-human-pose-estimation-with-2d-weak-supervision-in-autonomous-driving-paper">•
Multi-modal 3D Human Pose Estimation with 2D Weak Supervision in
Autonomous Driving <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2112.12141v1.pdf">paper</a></h5>
<h5 id="adaptpose-cross-dataset-adaptation-for-3d-human-pose-estimation-by-learnable-motion-generation-paper">•
AdaptPose: Cross-Dataset Adaptation for 3D Human Pose Estimation by
Learnable Motion Generation <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2112.11593v1.pdf">paper</a></h5>
<h5 id="flag-flow-based-3d-avatar-generation-from-sparse-observations-paper">•
FLAG: Flow-based 3D Avatar Generation from Sparse Observations <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2203.05789v1.pdf">paper</a></h5>
<h5 id="pose-mum-reinforcing-key-points-relationship-for-semi-supervised-human-pose-estimation-paper">•
Pose-MUM : Reinforcing Key Points Relationship for Semi-Supervised Human
Pose Estimation <a href>paper</a></h5>
<h5 id="distribution-aware-single-stage-models-for-multi-person-3d-pose-estimation-paper">•
Distribution-Aware Single-Stage Models for Multi-Person 3D Pose
Estimation <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2203.07697v1.pdf">paper</a></h5>
<h5 id="p-stmo-pre-trained-spatial-temporal-many-to-one-model-for-3d-human-pose-estimation-paper-code">•
P-STMO: Pre-Trained Spatial Temporal Many-to-One Model for 3D Human Pose
Estimation <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2203.07628v1.pdf">paper</a> <a target="_blank" rel="noopener" href="https://github.com/paTRICK-swk/P-STMO">code</a></h5>
<h5 id="posepipe-open-source-human-pose-estimation-pipeline-for-clinical-research-paper-code">•
PosePipe: Open-Source Human Pose Estimation Pipeline for Clinical
Research <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2203.08792v1.pdf">paper</a> <a target="_blank" rel="noopener" href="https://github.com/peabody124/PosePipeline/">code</a></h5>
<h5 id="d-human-pose-estimation-using-möbius-graph-convolutional-networks-paper">•
3D Human Pose Estimation Using Möbius Graph Convolutional Networks <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2203.10554v1.pdf">paper</a></h5>
<h5 id="ray3d-ray-based-3d-human-pose-estimation-for-monocular-absolute-3d-localization-paper-code">•
Ray3D: ray-based 3D human pose estimation for monocular absolute 3D
localization <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2203.11471v1.pdf">paper</a>
<a target="_blank" rel="noopener" href="https://github.com/YxZhxn/Ray3D">code</a></h5>
<h5 id="yolo-pose-enhancing-yolo-for-multi-person-pose-estimation-using-object-keypoint-similarity-loss-paper-code">•
YOLO-Pose: Enhancing YOLO for Multi Person Pose Estimation Using Object
Keypoint Similarity Loss <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2204.06806v1.pdf">paper</a> <a target="_blank" rel="noopener" href="https://github.com/TexasInstruments/edgeai-yolov5">code</a></h5>
<h5 id="permutation-invariant-relational-network-for-multi-person-3d-pose-estimation-paper">•
Permutation-Invariant Relational Network for Multi-person 3D Pose
Estimation <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2204.04913v1.pdf">paper</a></h5>
<h5 id="non-local-latent-relation-distillation-for-self-adaptive-3d-human-pose-estimation-paper">•
Non-Local Latent Relation Distillation for Self-Adaptive 3D Human Pose
Estimation <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2204.01971v1.pdf">paper</a></h5>
<h5 id="aligning-silhouette-topology-for-self-adaptive-3d-human-pose-recovery-paper">•
Aligning Silhouette Topology for Self-Adaptive 3D Human Pose Recovery <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2204.01276v1.pdf">paper</a></h5>
<h5 id="dite-hrnet-dynamic-lightweight-high-resolution-network-for-human-pose-estimation-paper">•
Dite-HRNet: Dynamic Lightweight High-Resolution Network for Human Pose
Estimation <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2204.10762v1.pdf">paper</a></h5>
<h5 id="pedrecnet-multi-task-deep-neural-network-for-full-3d-human-pose-and-orientation-estimation-paper">•
PedRecNet: Multi-task deep neural network for full 3D human pose and
orientation estimation <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2204.11548v1.pdf">paper</a></h5>
<h5 id="teaching-independent-parts-separately-tips-gan-improving-accuracy-and-stability-in-unsupervised-adversarial-2d-to-3d-human-pose-estimation-paper">•
"Teaching Independent Parts Separately" (TIPS-GAN) : Improving Accuracy
and Stability in Unsupervised Adversarial 2D to 3D Human Pose Estimation
<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2205.05980v1.pdf">paper</a></h5>
<h5 id="lightweight-human-pose-estimation-using-heatmap-weighting-loss-paper">•Lightweight
Human Pose Estimation Using Heatmap-Weighting Loss <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2205.10611v1.pdf">paper</a></h5>
<h5 id="vtp-volumetric-transformer-for-multi-view-multi-person-3d-pose-estimation-paper">•VTP:
Volumetric Transformer for Multi-view Multi-person 3D Pose Estimation <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2205.12602v1.pdf">paper</a></h5>
<h5 id="location-free-human-pose-estimation-paper">•Location-free Human
Pose Estimation <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2205.12619v1.pdf">paper</a></h5>
<h5 id="trajectory-optimization-for-physics-based-reconstruction-of-3d-human-pose-from-monocular-video-paper">•Trajectory
Optimization for Physics-Based Reconstruction of 3d Human Pose from
Monocular Video <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2205.12292v1.pdf">paper</a></h5>
<h5 id="spgnet-spatial-projection-guided-3d-human-pose-estimation-in-low-dimensional-space-paper">•SPGNet:
Spatial Projection Guided 3D Human Pose Estimation in Low Dimensional
Space <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2206.01867v1.pdf">paper</a></h5>
<h5 id="graphmlp-a-graph-mlp-like-architecture-for-3d-human-pose-estimation-paper">•GraphMLP:
A Graph MLP-Like Architecture for 3D Human Pose Estimation <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2206.06420v1.pdf">paper</a></h5>
<h5 id="blazepose-ghum-holistic-real-time-3d-human-landmarks-and-pose-estimation-paper">•BlazePose
GHUM Holistic: Real-time 3D Human Landmarks and Pose Estimation <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2206.11678v1.pdf">paper</a></h5>
<h5 id="mutual-adaptive-reasoning-for-monocular-3d-multi-person-pose-estimation-paper">•Mutual
Adaptive Reasoning for Monocular 3D Multi-Person Pose Estimation <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2207.07900v1.pdf">paper</a></h5>
<h5 id="human-keypoint-detection-for-close-proximity-human-robot-interaction-paper">•Human
keypoint detection for close proximity human-robot interaction <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2207.07742v1.pdf">paper</a></h5>
<h5 id="virtualpose-learning-generalizable-3d-human-pose-models-from-virtual-data-paper">•VirtualPose:
Learning Generalizable 3D Human Pose Models from Virtual Data <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2207.09949v1.pdf">paper</a></h5>
<h5 id="d-clothed-human-reconstruction-in-the-wild-paper-code">•3D
Clothed Human Reconstruction in the Wild <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2207.10053v1.pdf">paper</a> <a target="_blank" rel="noopener" href="https://github.com/hygenie1228/ClothWild_RELEASE">code</a></h5>
<h5 id="efficient-and-accurate-skeleton-based-two-person-interaction-recognition-using-inter--and-intra-body-graphs-paper">•EFFICIENT
AND ACCURATE SKELETON-BASED TWO-PERSON INTERACTION RECOGNITION USING
INTER- AND INTRA-BODY GRAPHS <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2207.12648v1.pdf">paper</a></h5>
<h3 id="registration">registration</h3>
<h5 id="loopreg-self-supervised-learning-of-implicit-surface-correspondences-pose-and-shape-for-3d-human-mesh-registration-paper-code">•
LoopReg: Self-supervised Learning of Implicit Surface Correspondences,
Pose and Shape for 3D Human Mesh Registration <a target="_blank" rel="noopener" href="https://virtualhumans.mpi-inf.mpg.de/papers/bhatnagar2020loopreg/bhatnagar2020loopreg.pdf">paper</a>
<a target="_blank" rel="noopener" href="https://github.com/bharat-b7/LoopReg">code</a></h5>
<h5 id="neural-deformation-graphs-for-globally-consistent-non-rigid-reconstruction-paper-code">•
Neural Deformation Graphs for Globally-consistent Non-rigid
Reconstruction <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2012.01451.pdf">paper</a>
<a target="_blank" rel="noopener" href="https://github.com/AljazBozic/NeuralGraph">code</a></h5>
<h5 id="farm-functional-automatic-registration-method-for-3d-human-bodies-paper-code">•
FARM: Functional Automatic Registration Method for 3D Human Bodies <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1807.10517">paper</a> <a target="_blank" rel="noopener" href="https://github.com/riccardomarin/FARM">code</a></h5>
<h5 id="locally-aware-piecewise-transformation-fields-for-3d-human-mesh-registration-paper">•
Locally Aware Piecewise Transformation Fields for 3D Human Mesh
Registration <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2104.08160v1.pdf">paper</a></h5>
<h5 id="unsupervised-3d-human-mesh-recovery-from-noisy-point-clouds-paper">•
Unsupervised 3D Human Mesh Recovery from Noisy Point Clouds <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2107.07539v1.pdf">paper</a></h5>
<h3 id="correspondence">correspondence</h3>
<h5 id="humangps-geodesic-preserving-feature-for-dense-human-correspondences-paper">•
HumanGPS: Geodesic PreServing Feature for Dense Human Correspondences <a target="_blank" rel="noopener" href="https://feitongt.github.io/HumanGPS/paper.pdf">paper</a></h5>
<h5 id="bodymap-learning-full-body-dense-correspondence-map-paper">•BodyMap:
Learning Full-Body Dense Correspondence Map <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2205.09111v1.pdf">paper</a></h5>
<h5 id="corri2p-deep-image-to-point-cloud-registration-via-dense-correspondence-paper">•CorrI2P:
Deep Image-to-Point Cloud Registration via Dense Correspondence <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2207.05483v1.pdf">paper</a></h5>
<h3 id="application">application</h3>
<h5 id="one-shot-free-view-neural-talking-head-synthesis-for-video-conferencing-paper">•
One-Shot Free-View Neural Talking-Head Synthesis for Video Conferencing
<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2011.15126.pdf">paper</a></h5>
<h5 id="lipsync3d-data-efficient-learning-of-personalized-3d-talking-faces-from-video-using-pose-and-lighting-normalization-paper">•
LipSync3D: Data-Efficient Learning of Personalized 3D Talking Faces from
Video using Pose and Lighting Normalization <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2106.04185v1.pdf">paper</a></h5>
<h5 id="arshoe-real-time-augmented-reality-shoe-try-on-system-on-smartphones-paper">•
ARShoe: Real-Time Augmented Reality Shoe Try-on System on Smartphones <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2108.10515v1.pdf">paper</a></h5>
<h5 id="a-neural-anthropometer-learning-from-body-dimensions-computed-on-human-3d-meshes-paper">•
A Neural Anthropometer Learning from Body Dimensions Computed on Human
3D Meshes <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2110.04064v1.pdf">paper</a></h5>
<h5 id="robust-3d-garment-digitization-from-monocular-2d-images-for-3d-virtual-try-on-systems-paper">•
Robust 3D Garment Digitization from Monocular 2D Images for 3D Virtual
Try-On Systems <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2111.15140v1.pdf">paper</a></h5>
<h5 id="single-image-human-body-reshaping-with-deep-neural-networks-paper">•
Single-image Human-body Reshaping with Deep Neural Networks <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2203.10496v1.pdf">paper</a></h5>
<h5 id="style-based-global-appearance-flow-for-virtual-try-on-paper">•
Style-Based Global Appearance Flow for Virtual Try-On <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2204.01046v1.pdf">paper</a></h5>
<h5 id="monitoring-of-pigmented-skin-lesions-using-3d-whole-body-imaging-paper">•
Monitoring of Pigmented Skin Lesions Using 3D Whole Body Imaging <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2205.07085v1.pdf">paper</a></h5>
<h5 id="estimation-of-3d-body-shape-and-clothing-measurements-from-frontaland-side-view-images-paper">•
ESTIMATION OF 3D BODY SHAPE AND CLOTHING MEASUREMENTS FROM FRONTALAND
SIDE-VIEW IMAGES <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2205.14347v1.pdf">paper</a></h5>
<h5 id="dressing-avatars-deep-photorealistic-appearance-for-physically-simulated-clothing-paper">•
Dressing Avatars: Deep Photorealistic Appearance for Physically
Simulated Clothing <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2206.15470.pdf">paper</a></h5>
<h5 id="aifit-automatic-3d-human-interpretable-feedback-models-for-fitness-training-paper">•
AIFit: Automatic 3D Human-Interpretable Feedback Models for Fitness
Training <a target="_blank" rel="noopener" href="http://vision.imar.ro/fit3d/">paper</a></h5>
<h3 id="texture">texture</h3>
<h5 id="spatiotemporal-texture-reconstruction-for-dynamic-objects-using-a-single-rgb-d-camera-paper">•
Spatiotemporal Texture Reconstruction for Dynamic Objects Using a Single
RGB-D Camera <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2108.09007v1.pdf">paper</a></h5>
<h5 id="semi-supervised-synthesis-of-high-resolution-editable-textures-for-3d-humans-paper">•
Semi-supervised Synthesis of High-Resolution Editable Textures for 3D
Humans <a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2021/papers/Chaudhuri_Semi-Supervised_Synthesis_of_High-Resolution_Editable_Textures_for_3D_Humans_CVPR_2021_paper.pdf">paper</a></h5>
<h5 id="stylepeople-a-generative-model-of-fullbody-human-avatars-paper-code">•
StylePeople: A Generative Model of Fullbody Human Avatars <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2104.08363.pdf">paper</a> <a target="_blank" rel="noopener" href="https://github.com/saic-vul/style-people">code</a></h5>
<h3 id="skin">skin</h3>
<h5 id="heterskinnet-a-heterogeneous-network-for-skin-weights-prediction-paper">•
HeterSkinNet: A Heterogeneous Network for Skin Weights Prediction <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2103.10602.pdf">paper</a></h5>
<h5 id="snarf-differentiable-forward-skinning-for-animating-non-rigid-neural-implicit-shapes-paper">•
SNARF: Differentiable Forward Skinning for Animating Non-Rigid Neural
Implicit Shapes <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2104.03953.pdf">paper</a></h5>
<h3 id="uncategorized">uncategorized</h3>
<h5 id="fully-convolutional-graph-neural-networks-for-parametric-virtual-try-on-paper">•
Fully Convolutional Graph Neural Networks for Parametric Virtual Try-On
<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2009.04592.pdf">paper</a></h5>
<h5 id="tailornet-predicting-clothing-in-3d-as-a-function-of-human-pose-shape-and-garment-style-paper-code">•
TailorNet: Predicting Clothing in 3D as a Function of Human Pose, Shape
and Garment Style <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2003.04583">paper</a>
<a target="_blank" rel="noopener" href="https://github.com/chaitanya100100/TailorNet">code</a></h5>
<h5 id="dbooster-3d-body-shape-and-texture-recovery-paper">• 3DBooSTeR:
3D Body Shape and Texture Recovery <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2010.12670.pdf">paper</a></h5>
<h5 id="neural-3d-clothes-retargeting-from-a-single-image-paper">•
Neural 3D Clothes Retargeting from a Single Image <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2102.00062v1.pdf">paper</a></h5>
<h5 id="single-image-full-body-human-relighting-paper">• Single-image
Full-body Human Relighting <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2107.07259.pdf">paper</a></h5>
<h5 id="ibutter-neural-interactive-bullet-time-generator-for-human-free-viewpoint-rendering-paper">•
iButter: Neural Interactive Bullet Time Generator for Human
Free-viewpoint Rendering <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2108.05577.pdf">paper</a></h5>
<h5 id="a-riemannian-framework-for-analysis-of-human-body-surface-paper">• A
Riemannian Framework for Analysis of Human Body Surface <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2108.11449v1.pdf">paper</a></h5>
<h5 id="the-power-of-points-for-modeling-humans-in-clothing-papercode">•
The Power of Points for Modeling Humans in Clothing <a target="_blank" rel="noopener" href="https://qianlim.github.io/POP.html">paper&amp;code</a></h5>
<h5 id="neural-human-deformation-transfer-paper">• Neural Human
Deformation Transfer <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2109.01588v1.pdf">paper</a></h5>
<h5 id="d-human-texture-estimation-from-a-single-image-with-transformers-paper">•
3D Human Texture Estimation from a Single Image with Transformers <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2109.02563v1.pdf">paper</a></h5>
<h5 id="learning-to-predict-diverse-human-motions-from-a-single-image-via-mixture-density-networks-paper">•
Learning to Predict Diverse Human Motions from a Single Image via
Mixture Density Networks <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2109.05776v1.pdf">paper</a></h5>
<h5 id="zflow-gated-appearance-flow-based-virtual-try-on-with-3d-priors-paper">•
ZFlow: Gated Appearance Flow-based Virtual Try-on with 3D Priors <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2109.07001v1.pdf">paper</a></h5>
<h5 id="a-shading-guided-generative-implicit-model-for-shape-accurate-3d-aware-image-synthesis-paper-code">•
A Shading-Guided Generative Implicit Model for Shape-Accurate 3D-Aware
Image Synthesis <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2110.15678">paper</a> <a target="_blank" rel="noopener" href="https://github.com/XingangPan/ShadeGAN">code</a></h5>
<h5 id="action2video-generating-videos-of-human-3d-actions-paper">•
Action2video: Generating Videos of Human 3D Actions <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2111.06925v1.pdf">paper</a></h5>
<h5 id="garment4d-garment-reconstruction-from-point-cloud-sequences-paper-code">•
Garment4D: Garment Reconstruction from Point Cloud Sequences <a target="_blank" rel="noopener" href="https://papers.nips.cc/paper/2021/file/eb160de1de89d9058fcb0b968dbbbd68-Paper.pdf">paper</a>
<a target="_blank" rel="noopener" href="https://github.com/hongfz16/Garment4D">code</a></h5>
<h5 id="adg-pose-automated-dataset-generation-for-real-world-human-pose-estimation-paper-code">•
ADG-Pose: Automated Dataset Generation for Real-World Human Pose
Estimation <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2202.00753v1.pdf">paper</a> <a target="_blank" rel="noopener" href="https://github.com/TeCSAR-UNCC/ADG-Pose">code</a></h5>
<h5 id="diffusionnet-discretization-agnostic-learning-on-surfaces-paper">•
DiffusionNet: Discretization Agnostic Learning on Surfaces <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2012.00888.pdf">paper</a></h5>
<h5 id="text-and-image-guided-3d-avatar-generation-and-manipulation-paper-code">•
Text and Image Guided 3D Avatar Generation and Manipulation <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2202.06079v1.pdf">paper</a> <a target="_blank" rel="noopener" href="https://catlab-team.github.io/">code</a></h5>
<h5 id="quantification-of-occlusion-handling-capability-of-a-3d-human-pose-estimation-framework-paper">•
Quantification of Occlusion Handling Capability of a 3D Human Pose
Estimation Framework <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2203.04113v1.pdf">paper</a></h5>
<h5 id="motron-multimodal-probabilistic-human-motion-forecasting-paper">•
Motron: Multimodal Probabilistic Human Motion Forecasting <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2203.04132v1.pdf">paper</a></h5>
<h5 id="fexgan-meta-facial-expression-generation-with-meta-humans-paper">•
FEXGAN-META: FACIAL EXPRESSION GENERATION WITH META HUMANS <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2203.05975v1.pdf">paper</a></h5>
<h5 id="actformer-a-gan-transformer-framework-towards-general-action-conditioned-3d-human-motion-generation">•
ActFormer: A GAN Transformer Framework towards General
Action-Conditioned 3D Human Motion Generation</h5>
<h5 id="domain-adaptive-hand-keypoint-and-pixel-localization-in-the-wild-paper">•
Domain Adaptive Hand Keypoint and Pixel Localization in the Wild <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2203.08344v1.pdf">paper</a></h5>
<h5 id="portrait-eyeglasses-and-shadow-removal-by-leveraging-3d-synthetic-data-paper">•
Portrait Eyeglasses and Shadow Removal by Leveraging 3D Synthetic Data
<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2203.10474v1.pdf">paper</a></h5>
<h5 id="recognition-of-freely-selected-keypoints-on-human-limbs-paper">•
Recognition of Freely Selected Keypoints on Human Limbs <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2204.06326v1.pdf">paper</a></h5>
<h5 id="whats-in-your-hands-3d-reconstruction-of-generic-objects-in-hands-paper-code">•
What’s in your hands? 3D Reconstruction of Generic Objects in Hands <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2204.07153v1.pdf">paper</a> <a target="_blank" rel="noopener" href="https://github.com/JudyYe/ihoi">code</a></h5>
<h5 id="chore-contact-human-and-object-reconstruction-from-a-single-rgb-image-paper">•
CHORE: Contact, Human and Object REconstruction from a single RGB image
<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2204.02445v1.pdf">paper</a></h5>
<h5 id="snug-self-supervised-neural-dynamic-garments-paper">• SNUG:
Self-Supervised Neural Dynamic Garments <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2204.02219v1.pdf">paper</a></h5>
<h5 id="d-magic-mirror-clothing-reconstruction-from-a-single-image-via-a-causal-perspective-paper">•
3D Magic Mirror: Clothing Reconstruction from a Single Image via a
Causal Perspective <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2204.13096v1.pdf">paper</a></h5>
<h5 id="fake-it-till-you-make-it-face-analysis-in-the-wild-using-synthetic-data-alone-homepage">•
Fake it till you make it: face analysis in the wild using synthetic data
alone <a target="_blank" rel="noopener" href="https://microsoft.github.io/FaceSynthetics/">homepage</a></h5>
<h5 id="avatarclip-zero-shot-text-driven-generation-and-animation-of-3d-avatars-paper-code">•
AvatarCLIP: Zero-Shot Text-Driven Generation and Animation of 3D Avatars
<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2205.08535v1.pdf">paper</a> <a target="_blank" rel="noopener" href="https://github.com/hongfz16/AvatarCLIP">code</a></h5>
<h5 id="scene-aware-person-image-generation-through-global-contextual-conditioning-paper">•
Scene Aware Person Image Generation through Global Contextual
Conditioning <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2206.02717v1.pdf">paper</a></h5>
<h5 id="hairfit-pose-invariant-hairstyle-transfer-via-flow-based-hair-alignment-and-semantic-region-aware-inpainting-paper">•
HairFIT: Pose-Invariant Hairstyle Transfer via Flow-based Hair Alignment
and Semantic-Region-Aware Inpainting <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2206.08585v1.pdf">paper</a></h5>
<h5 id="from-a-few-accurate-2d-correspondences-to-3d-point-clouds-paper">•
From a few Accurate 2D Correspondences to 3D Point Clouds <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2206.08749v1.pdf">paper</a></h5>
<h5 id="convolutional-neural-network-based-partial-face-detection-paper">•
Convolutional Neural Network Based Partial Face Detection <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2206.14350v1.pdf">paper</a></h5>
<h5 id="spsn-superpixel-prototype-sampling-network-for-rgb-d-salient-object-detection-paper-code">•
SPSN: Superpixel Prototype Sampling Network for RGB-D Salient Object
Detection <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2207.07898v1.pdf">paper</a> <a target="_blank" rel="noopener" href="https://github.com/Hydragon516/SPSN">code</a></h5>
<h5 id="detecting-humans-in-rgb-d-data-with-cnns-paper">• Detecting
Humans in RGB-D Data with CNNs <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2207.08064v1.pdf">paper</a></h5>
<h5 id="animation-from-blur-multi-modal-blur-decomposition-with-motion-guidance-paper">•
Animation from Blur: Multi-modal Blur Decomposition with Motion Guidance
<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2207.10123.pdf">paper</a></h5>
<h5 id="d-shape-sequence-of-human-comparison-and-classification-using-current-and-varifolds-paper-code">•
3D Shape Sequence of Human Comparison and Classification using Current
and Varifolds <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2207.12485v1.pdf">paper</a>
<a target="_blank" rel="noopener" href="https://github.com/CRISTAL-3DSAM/HumanComparisonVarifolds">code</a></h5>
<h5 id="kinepose-a-temporally-optimized-inverse-kinematics-technique-for-6dof-human-pose-estimation-with-biomechanical-constraints-paper-code">•
KinePose: A temporally optimized inverse kinematics technique for 6DOF
human pose estimation with biomechanical constraints <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2207.12841v1.pdf">paper</a> <a target="_blank" rel="noopener" href="https://github.com/KevGildea/KinePose">code</a></h5>
<h5 id="skeleton-free-pose-transfer-for-stylized-3d-character-paper">•
Skeleton-free Pose Transfer for Stylized 3D Character <a target="_blank" rel="noopener" href="https://www.arxiv-vanity.com/papers/2208.00790/?continueFlag=acd9680585ca1db48ed3cbc277e4da97">paper</a></h5>
<h2 id="parametric-model">parametric model</h2>
<h3 id="body">body</h3>
<h5 id="smpl-a-skinned-multi-person-linear-model-paper-code">• SMPL: A
Skinned Multi-Person Linear Model <a target="_blank" rel="noopener" href="https://files.is.tue.mpg.de/black/papers/SMPL2015.pdf">paper</a>
<a target="_blank" rel="noopener" href="https://github.com/CalciferZh/SMPL">code</a></h5>
<h5 id="expressive-body-capture-3d-hands-face-and-body-from-a-single-image-paper-code">•
Expressive Body Capture: 3D Hands, Face, and Body from a Single Image <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.05866">paper</a> <a target="_blank" rel="noopener" href="https://github.com/vchoutas/smplx">code</a></h5>
<h5 id="star-sparse-trained-articulated-human-body-regressor-paper-code">•
STAR: Sparse Trained Articulated Human Body Regressor <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2008.08535">paper</a> <a target="_blank" rel="noopener" href="https://github.com/ahmedosman/STAR">code</a></h5>
<h3 id="face">face</h3>
<h5 id="basel-face-model-2009-website">• Basel Face Model 2009 <a target="_blank" rel="noopener" href="http://faces.cs.unibas.ch/bfm/?nav=1-0&amp;id=basel_face_model">website</a></h5>
<h5 id="basel-face-model-2017-website">• Basel Face Model 2017 <a target="_blank" rel="noopener" href="http://faces.cs.unibas.ch/bfm/bfm2017.html">website</a></h5>
<h5 id="large-scale-3d-morphable-model-website">• Large Scale 3D
Morphable Model <a target="_blank" rel="noopener" href="https://xip.uclb.com/i/healthcare_tools/LSFM.html">website</a></h5>
<h3 id="head">head</h3>
<h5 id="flame-articulated-expressive-head-model-website">• FLAME:
Articulated Expressive Head Model <a target="_blank" rel="noopener" href="http://flame.is.tue.mpg.de/">website</a></h5>
<h3 id="hand">hand</h3>
<h5 id="mano-paper-website">• MANO <a target="_blank" rel="noopener" href="https://ps.is.mpg.de/uploads_file/attachment/attachment/392/Embodied_Hands_SiggraphAsia2017.pdf">paper</a>
<a target="_blank" rel="noopener" href="https://mano.is.tue.mpg.de/">website</a></h5>
<h5 id="nimble-a-non-rigid-hand-model-with-bones-and-muscles-paper">•
NIMBLE: A Non-rigid Hand Model with Bones and Muscles <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2202.04533v1.pdf">paper</a></h5>
<h3 id="method">method</h3>
<h5 id="ghum-ghuml-generative-3d-human-shape-and-articulated-pose-models-github">•GHUM
&amp; GHUML: Generative 3D Human Shape and Articulated Pose Models <a target="_blank" rel="noopener" href="https://github.com/google-research/google-research/tree/master/ghum">github</a></h5>
<h5 id="large-scale-3d-morphable-models-paper-code">•Large Scale 3D
Morphable Models <a target="_blank" rel="noopener" href="https://link.springer.com/article/10.1007/s11263-017-1009-7">paper</a>
<a target="_blank" rel="noopener" href="https://github.com/menpo/lsfm">code</a></h5>
<h5 id="morphable-face-models---an-open-framework-paper-code">•Morphable
Face Models - An Open Framework <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1709.08398">paper</a> <a target="_blank" rel="noopener" href="https://github.com/unibas-gravis/basel-face-pipeline">code</a></h5>
<h2 id="dataset">dataset</h2>
<h3 id="face-1">face</h3>
<h5 id="maad-face-a-massively-annotated-attribute-dataset-for-face-images-paper">•
MAAD-Face: A Massively Annotated Attribute Dataset for Face Images <a target="_blank" rel="noopener" href="https://github.com/pterhoer/MAAD-Face">paper</a></h5>
<h5 id="facescape-3d-facial-dataset-and-benchmark-for-single-view-3d-face-reconstruction-paper">•
FaceScape: 3D Facial Dataset and Benchmark for Single-View 3D Face
Reconstruction <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2111.01082v1.pdf">paper</a></h5>
<h5 id="w-lp-aflw2000-3d-homepage">• 300W-LP &amp; AFLW2000-3D <a target="_blank" rel="noopener" href="http://www.cbsr.ia.ac.cn/users/xiangyuzhu/projects/3DDFA/main.htm">homepage</a></h5>
<h5 id="aflw-website">• AFLW <a target="_blank" rel="noopener" href="https://www.tugraz.at/institute/icg/research/team-bischof/lrs/downloads/aflw/">website</a></h5>
<h5 id="realy-rethinking-the-evaluation-of-3d-face-reconstruction-paper-website">•
REALY: Rethinking the Evaluation of 3D Face Reconstruction <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2203.09729v1.pdf">paper</a> <a target="_blank" rel="noopener" href="https://www.realy3dface.com/">website</a></h5>
<h5 id="faceverse-high-quality-3d-face-dataset-github">• FaceVerse-High
Quality 3D Face Dataset <a target="_blank" rel="noopener" href="https://github.com/LizhenWangT/FaceVerse-Dataset">github</a></h5>
<h5 id="dad-3dheads-a-large-scale-dense-accurate-and-diverse-dataset-for-3d-head-alignment-from-a-single-image-paper-github">•
DAD-3DHeads: A Large-scale Dense, Accurate and Diverse Dataset for 3D
Head Alignment from a Single Image <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2204.03688v1.pdf">paper</a> <a target="_blank" rel="noopener" href="https://github.com/PinataFarms/DAD-3DHeads">github</a></h5>
<h5 id="multiface-a-dataset-for-neural-face-rendering-paper">•
Multiface: A Dataset for Neural Face Rendering <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2207.11243.pdf">paper</a></h5>
<h3 id="hand-1">hand</h3>
<h5 id="grab-a-dataset-of-whole-body-human-grasping-of-objects-github">•
GRAB: A Dataset of Whole-Body Human Grasping of Objects <a target="_blank" rel="noopener" href="https://github.com/otaheri/GRAB">github</a></h5>
<h5 id="reconstructing-hand-object-interactions-in-the-wild-github">•
Reconstructing Hand-Object Interactions in the Wild <a target="_blank" rel="noopener" href="https://github.com/ZheC/MOW">github</a></h5>
<h5 id="ego2handspose-a-dataset-for-egocentric-two-hand-3d-global-pose-estimation-paper">•
Ego2HandsPose: A Dataset for Egocentric Two-hand 3D Global Pose
Estimation <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2206.04927v1.pdf">paper</a></h5>
<h5 id="interhand2.6m">• Interhand2.6M</h5>
<h3 id="body-1">body</h3>
<h5 id="ntu60-x-towards-skeleton-based-recognition-of-subtle-human-actions-code">•
NTU60-X: TOWARDS SKELETON-BASED RECOGNITION OF SUBTLE HUMAN ACTIONS <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2101.11529.pdf">code</a></h5>
<h5 id="agora-avatars-in-geography-optimized-for-regression-analysis-homepage">•
AGORA: Avatars in Geography Optimized for Regression Analysis <a target="_blank" rel="noopener" href="https://agora.is.tue.mpg.de/">homepage</a></h5>
<h5 id="amass-archive-of-motion-capture-as-surface-shapes-paper-github">•
AMASS: Archive of Motion Capture as Surface Shapes <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1904.03278.pdf">paper</a> <a target="_blank" rel="noopener" href="https://github.com/nghorbani/amass">github</a></h5>
<h5 id="asl-skeleton3d-and-asl-phono-two-novel-datasets-for-the-american-sign-language-paper">•
ASL-Skeleton3D and ASL-Phono: Two Novel Datasets for the American Sign
Language <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2201.02065v1.pdf">paper</a></h5>
<h5 id="mpi-inf-3dhp-homepage">•MPI-INF-3DHP <a target="_blank" rel="noopener" href="https://vcai.mpi-inf.mpg.de/3dhp-dataset/">homepage</a></h5>
<h5 id="human3.6m-website">•Human3.6M <a target="_blank" rel="noopener" href="http://vision.imar.ro/human3.6m/description.php">website</a></h5>
<h5 id="dpw-website">•3DPW <a target="_blank" rel="noopener" href="https://virtualhumans.mpi-inf.mpg.de/3DPW/">website</a></h5>
<h5 id="pennaction-website">•PennAction <a target="_blank" rel="noopener" href="http://dreamdragon.github.io/PennAction/">website</a></h5>
<h5 id="insta-variety-github">•Insta Variety <a target="_blank" rel="noopener" href="https://github.com/akanazawa/human_dynamics/blob/master/doc/insta_variety.md">github</a></h5>
<h5 id="posetrack-homapage">•PoseTrack <a target="_blank" rel="noopener" href="https://posetrack.net/">homapage</a></h5>
<h5 id="kinetics-400-website">•Kinetics-400 <a target="_blank" rel="noopener" href="https://deepmind.com/research/open-source/kinetics">website</a></h5>
<h5 id="renderpeople-website">•RenderPeople <a target="_blank" rel="noopener" href="https://renderpeople.com/">website</a></h5>
<h5 id="buff-website">•BUFF <a target="_blank" rel="noopener" href="https://buff.is.tue.mpg.de/">website</a></h5>
<h5 id="people-snapshot-dataset-homepage">•People Snapshot Dataset <a target="_blank" rel="noopener" href="https://graphics.tu-bs.de/people-snapshot">homepage</a></h5>
<h5 id="multi-garment-homepage">•Multi-Garment <a target="_blank" rel="noopener" href="https://virtualhumans.mpi-inf.mpg.de/mgn/">homepage</a></h5>
<h5 id="iper-website">•iPER <a target="_blank" rel="noopener" href="https://svip-lab.github.io/dataset/iPER_dataset.html">website</a></h5>
<h5 id="zju-mocap-homepage">•ZJU-MoCap <a target="_blank" rel="noopener" href="https://chingswy.github.io/Dataset-Demo/">homepage</a></h5>
<h5 id="smartportraits-depth-powered-handheld-smartphone-dataset-of-human-portraits-for-state-estimation-reconstruction-and-synthesis-paper">•SmartPortraits:
Depth Powered Handheld Smartphone Dataset of Human Portraits for State
Estimation, Reconstruction and Synthesis <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2204.10211v1.pdf">paper</a></h5>
<h5 id="mvp-human-dataset-for-3d-human-avatar-reconstruction-from-unconstrained-frames-paper">•MVP-Human
Dataset for 3D Human Avatar Reconstruction from Unconstrained Frames <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2204.11184v1.pdf">paper</a></h5>
<h5 id="humman-multi-modal-4d-human-dataset-for-versatile-sensing-and-modeling-paper">•HuMMan:
Multi-Modal 4D Human Dataset for Versatile Sensing and Modeling <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2204.13686v1.pdf">paper</a></h5>
<h5 id="dmpb-dataset-github">•3DMPB-dataset <a target="_blank" rel="noopener" href="https://github.com/boycehbz/3DMPB-dataset">github</a></h5>
<h5 id="imar_vision_datasets_tools-github">•imar_vision_datasets_tools
<a target="_blank" rel="noopener" href="https://github.com/sminchisescu-research/imar_vision_datasets_tools">github</a></h5>
<h5 id="rich-real-scenes-interaction-contacts-and-humans-github">•RICH:
Real scenes, Interaction, Contacts and Humans <a target="_blank" rel="noopener" href="https://github.com/paulchhuang/rich_toolkit">github</a></h5>
<h3 id="method-1">method</h3>
<h5 id="neuralannot-neural-annotator-for-3d-human-mesh-training-sets-paper">•NeuralAnnot:
Neural Annotator for 3D Human Mesh Training Sets <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2011.11232.pdf">paper</a></h5>
<h3 id="others">others</h3>
<h5 id="k-hairstyle-a-large-scale-korean-hairstyle-dataset-for-virtual-hair-editing-and-hairstyle-classification-homepage">•
K-Hairstyle: A Large-scale Korean hairstyle dataset for virtual hair
editing and hairstyle classification <a target="_blank" rel="noopener" href="https://www.arxiv-vanity.com/papers/2102.06288/">homepage</a></h5>
<h5 id="simulated-garment-dataset-for-virtual-try-on-address">•
Simulated garment dataset for virtual try-on <a target="_blank" rel="noopener" href="https://github.com/isantesteban/vto-dataset">address</a></h5>
<h5 id="deepfashion-homepage">• DeepFashion <a target="_blank" rel="noopener" href="https://liuziwei7.github.io/projects/DeepFashion.html">homepage</a></h5>
<h5 id="delving-into-high-quality-synthetic-face-occlusion-segmentation-datasets-paper">•
Delving into High-Quality Synthetic Face Occlusion Segmentation Datasets
<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2205.06218v1.pdf">paper</a></h5>
<h5 id="a-gan-facial-flow-for-face-animation-with-generative-adversarial-networks-paper">•
3A-GAN: Facial Flow for Face Animation with Generative Adversarial
Networks <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2205.06204v1.pdf">paper</a></h5>
<h5 id="scanned-objects-by-google-research-website">• Scanned Objects by
Google Research <a target="_blank" rel="noopener" href="https://ai.googleblog.com/2022/06/scanned-objects-by-google-research.html?continueFlag=69eb10990d1859f21dd21d22d96e2b22">website</a></h5>
<h2 id="labs">labs</h2>
<h5 id="max-planck-institute-website">• max planck institute <a target="_blank" rel="noopener" href="https://ps.is.tuebingen.mpg.de/publications">website</a></h5>
<h5 id="yebin-liu-website">• Yebin Liu <a target="_blank" rel="noopener" href="http://www.liuyebin.com/">website</a></h5>
<h5 id="zju3dv-github">• ZJU3DV <a target="_blank" rel="noopener" href="https://github.com/zju3dv">github</a></h5>
<h5 id="hujun-bao-google-scholar">• Hujun Bao <a target="_blank" rel="noopener" href="https://scholar.google.com/citations?hl=zh-CN&amp;user=AZCcDmsAAAAJ&amp;view_op=list_works&amp;sortby=pubdate">google
scholar</a></h5>
<h5 id="ustc-3dv-homepage">• USTC-3DV <a target="_blank" rel="noopener" href="http://staff.ustc.edu.cn/~juyong/index.html">homepage</a></h5>
<h5 id="hao_li-homepage">• Hao_Li <a target="_blank" rel="noopener" href="http://www.hao-li.com/Hao_Li/Hao_Li_-_about_me.html">homepage</a></h5>
<h2 id="other-related-awesome">other related awesome</h2>
<h5 id="awesome-clothed-human-github">• awesome-clothed-human <a target="_blank" rel="noopener" href="https://github.com/weihaox/awesome-clothed-human">github</a></h5>
<h5 id="curated-list-of-awesome-3d-morphable-model-software-and-data-github">•
curated-list-of-awesome-3D-Morphable-Model-software-and-data <a target="_blank" rel="noopener" href="https://github.com/3d-morphable-models/curated-list-of-awesome-3D-Morphable-Model-software-and-data">github</a></h5>
<h5 id="awesome-hand-pose-estimation-github">•
awesome-hand-pose-estimation <a target="_blank" rel="noopener" href="https://github.com/xinghaochen/awesome-hand-pose-estimation">github</a></h5>
<h5 id="awesome-3d-body-papers-github">• Awesome 3D Body Papers <a target="_blank" rel="noopener" href="https://github.com/3DFaceBody/awesome-3dbody-papers">github</a></h5>
<h5 id="body_reconstruction_references-github">•
Body_Reconstruction_References <a target="_blank" rel="noopener" href="https://github.com/chenweikai/Body_Reconstruction_References#data-and-code">github</a></h5>
<h5 id="d-face-reconstruction-paper-list-github">•
3D-face-reconstruction-paper-list <a target="_blank" rel="noopener" href="https://github.com/czh-98/3D-face-reconstruction-paper-list">github</a></h5>
<h5 id="awesome_talking_face_generation-github">•
awesome_talking_face_generation <a target="_blank" rel="noopener" href="https://github.com/YunjinPark/awesome_talking_face_generation">github</a></h5>
<h5 id="cg3dv-twitter-github">• CG&amp;3DV Twitter <a target="_blank" rel="noopener" href="https://github.com/USTC3DV/Truck_of_Twitter_Messages">github</a></h5>
<h2 id="survey">survey</h2>
<h5 id="recovering-3d-human-mesh-from-monocular-images-a-survey-paper-github">•
Recovering 3D Human Mesh from Monocular Images: A Survey <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2203.01923v1.pdf">paper</a> <a target="_blank" rel="noopener" href="https://github.com/tinatiansjz/hmr-survey">github</a></h5>
<h5 id="d-human-pose-estimation-a-survey-paper">• 2D Human Pose
Estimation: A Survey <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2204.07370v1.pdf">paper</a></h5>
<h5 id="a-survey-of-non-rigid-3d-registration-paper">• A Survey of
Non-Rigid 3D Registration <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2203.07858v1.pdf">paper</a></h5>
<h5 id="d-face-reconstruction-in-deep-learning-era-a-survey-paper">• 3D
Face Reconstruction in Deep Learning Era: A Survey <a target="_blank" rel="noopener" href="https://link.springer.com/article/10.1007/s11831-021-09705-4">paper</a></h5>
<h5 id="towards-efficient-and-photorealistic-3d-human-reconstruction-a-brief-survey-paper">•
Towards efficient and photorealistic 3D human reconstruction: A brief
survey <a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/pii/S2468502X21000413">paper</a></h5>
<h5 id="survey-on-3d-face-reconstruction-from-uncalibrated-images-paper">•Survey
on 3D face reconstruction from uncalibrated images <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2011.05740">paper</a></h5>
<h5 id="state-of-the-art-on-3d-reconstruction-with-rgb-d-cameras-paper">•State
of the Art on 3D Reconstruction with RGB-D Cameras <a target="_blank" rel="noopener" href="https://zollhoefer.com/papers/EG18_RecoSTAR/paper.pdf">paper</a></h5>

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>ZJU_XS
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="http://the-fleetinglove.github.io/2022/09/30/%E8%AE%BA%E6%96%87%E5%90%88%E9%9B%86--Awesome%203D%20Human%20Reconstruction/" title="3D Human Reconstruction Paper List">http://the-fleetinglove.github.io/2022/09/30/论文合集--Awesome 3D Human Reconstruction/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2022/09/29/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0--Self-Supervised%203D%20Face%20Reconstruction%20via%20Conditional%20Estimation/" rel="prev" title="论文笔记--Self-Supervised 3D Face Reconstruction via Conditional Estimation">
                  <i class="fa fa-chevron-left"></i> 论文笔记--Self-Supervised 3D Face Reconstruction via Conditional Estimation
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2022/10/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0--Authentic%20Volumetric%20Avatars%20from%20a%20Phone%20Scan/" rel="next" title="Authentic Volumetric Avatars from a Phone Scan">
                  Authentic Volumetric Avatars from a Phone Scan <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments" id="SOHUCS" sid="a47a9290817e16c5b2382daa872e44fb"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">ZJU_XS</span>
</div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="站点总字数">44k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">1:13</span>
  </span>
</div>
<div class="busuanzi-count">
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/next-boot.js"></script>

  
<script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.0/search.js" integrity="sha256-vXZMYLEqsROAXkEw93GGIvaB2ab+QW6w3+1ahD9nXXA=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>





  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


<script class="next-config" data-name="changyan" type="application/json">{"enable":true,"appid":"cywihBJTC","appkey":"6c5dadcb45f62385d61e3abaf1ffd812"}</script>
<script src="/js/third-party/comments/changyan.js"></script>

</body>
</html>
