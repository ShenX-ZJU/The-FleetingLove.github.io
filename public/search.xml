<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Authentic Volumetric Avatars from a Phone Scan</title>
      <link href="/2022/10/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0--Authentic%20Volumetric%20Avatars%20from%20a%20Phone%20Scan/"/>
      <url>/2022/10/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0--Authentic%20Volumetric%20Avatars%20from%20a%20Phone%20Scan/</url>
      
        <content type="html"><![CDATA[<p><strong>目录</strong></p><ul><li><a href="#摘要">摘要</a></li><li><a href="#介绍">介绍</a></li><li><a href="#创新点">创新点</a></li><li><a href="#方法">方法</a><ul><li><a href="#method-overview">Method Overview</a></li><li><a href="#universal-prior-model">Universal Prior Model</a></li><li><a href="#dataset">Dataset</a></li><li><a href="#training-and-losses">Training and Losses</a></li><li><a href="#conditioning-data-acquisition">Conditioning DataAcquisition</a></li><li><a href="#personalized-decoder-generation">Personalized DecoderGeneration</a></li><li><a href="#finetuning-a-personalized-decoder">Finetuning aPersonalized Decoder</a></li></ul></li><li><a href="#局限性">局限性</a></li></ul><h2 id="摘要">摘要</h2><p>创造出一个逼真级的人脸头像需要大量的面部捕捉数据，但是这种条件只有视觉特效企业才能做到，无法普及到大众群体。本文的工作就是利用手机捕捉数据来得到一个真实匹配人脸的头像。</p><p>不同于以往直接对整个人脸外观进行建模的方法，本文的方法仅使用少量的数据就可以应对各种人脸生成一个头像模型。</p><p>这个模型摒弃了会产生幻觉身份的低维潜在空间信息，相反，它使用了可以提取不同尺度person-specific信息的条件表示</p><p>本文方法的输出不仅是一个与人的面部形状和外观相匹配的高保真3D头像，而且还可以使用一个具有对注视方向的分离控制的共享全局表情空间来驱动人脸。<span id="more"></span></p><h2 id="介绍">介绍</h2><p>人脸是身份认证的重要属性，人脸外表，结构和姿态的细微变化都会降低人脸头像的重建质量。过去的方法为了解决这一问题都需要大量的数据驱动和预处理操作，损耗的算力和时间都是巨大的。</p><p>从有限的数据自动创建头像的核心挑战在于先验和验证之间的权衡。需要先验来补充有关人的外观，几何形状和运动的有限信息，这些信息可以以轻量级的方式获得。然而，尽管近年来取得了重大进展，但以高分辨率学习人脸的多种情况仍然具有挑战性。对分布的长尾进行建模，这对于捕获特定雀斑，纹身或疤痕等个人特质是必要的，可能需要具有更高维潜在空间的模型，因此，比目前用于训练此类模型的数据要多得多。</p><p>我们以前的架构是基于这样的观察，即面部外观和结构的长尾方面在于细节，最好直接从一个人的条件数据中提取，而不是从低维的身份嵌入中重建。早期方法从低维身份embeddings重建方法很快会陷入停滞，难以捕获到person-specific信息。我们的模型是一种超网络，它将用户中性面孔的数据作为输入，并以偏差图的形式为个性化解码器生成参数。</p><p>在这项工作中，我们通过消除对不存在的人产生幻觉的能力来打破先验和验证之间的权衡，相反，我们使用易于获得的真实人物的手机数据来适应。本文的方法分为3个要素：</p><ol type="1"><li>在多视角观测下上百个人的视频数据里训练得到的普适性先验模型</li><li>a registration technique for conditioning the model on a phone scanof the user’s neutral expression</li><li>一种<strong>逆向渲染</strong>方式去微调额外情感数据上的个性化人脸模型</li></ol><h2 id="创新点">创新点</h2><p><strong>Key Words</strong>: hypernetwork, inverse rendering</p><p>本文主要贡献如下：</p><ol type="1"><li>一种用于产生逼真头像的系统，与以往方法相比，质量有大幅提高</li><li>一种新颖网络架构，网络模型鲁棒性很高。产生的头像拥有一致的对视角点，表情和注视方向分离控制的表情空间</li><li>逆向渲染策略，在给定的额外手机正面拍摄数据下，将头像的表情空间特征化于用户，同时保证视角点的泛化性和潜在空间的语义</li></ol><h2 id="方法">方法</h2><h3 id="method-overview">Method Overview</h3><p><img src="/2022/10/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0--Authentic%20Volumetric%20Avatars%20from%20a%20Phone%20Scan/Method%20Overview.png"></p><h3 id="universal-prior-model">Universal Prior Model</h3><p><img src="/2022/10/05/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0--Authentic%20Volumetric%20Avatars%20from%20a%20Phone%20Scan/Universal%20Prior%20Model%20Architecture.png"></p><p><span class="math display">\[e =\mathcal{E}_{exp}(\vartriangle{T_{exp}},\vartriangle{G_{exp}};\Phi_{exp})\]</span><span class="math display">\[\Theta_{id} = \mathcal{E}_{id}(T_{neu},G_{neu};\Phi_{id})\]</span><span class="math display">\[\mathcal{M} = \mathcal{D}(e,v,g;\Theta_{id},\Phi_{dec})\]</span><span class="math display">\[\vartriangle{T_{exp}} = T_{exp} - T_{neu}\]</span><span class="math display">\[\vartriangle{G_{exp}} = G_{exp} - G_{neu}\]</span></p><p><span class="math inline">\(e\)</span>表示情感系数expressioncode；<span class="math inline">\(\Theta_{id}\)</span>表示biasmaps；<span class="math inline">\(\mathcal{M}\)</span>表示volumetricprimitives slabs，用于后续Ray March操作；<span class="math inline">\(\Phi_{exp}、\Phi_{id}、\Phi_{dec}\)</span>是可训练的参数，我认为是网络权重。</p><h3 id="dataset">Dataset</h3><p>跟踪网格(tracked meshes)的生成分为三个步骤：Capture Dome, CaptureScript, Tracking Pipeline。</p><p><strong>Capture Dome：</strong>为了捕获面部表演的同步多视图视频，本文构建了一个多摄像机捕获系统。摄像机系统含有几十个彩色和单色摄像机，摄像机以4096*2668的分辨率拍摄，快门速度为2.222ms，每秒90帧。350个点光源均匀分布在整个结构中，以均匀地照亮参与者。</p><p><strong>Capture Script：</strong>捕获脚本的目标是在最短的时间内系统地引导参与者完成各种面部表情。参与者被要求完成以下练习：模仿65个不同的面部运动片段；执行自由形式的面部运动范围部分；向25个不同的方向看以表示不同的凝视角度；阅读50个语音平衡的句子。总共有255名参与者使用此捕获脚本的数据采集，并且每个参与者平均记录了12k个子采样帧，总共有310万帧图片。</p><p><strong>Tracking Pipeline：</strong> 为了有效地生成超过 310万帧的跟踪网格，我们实施了高度可扩展的两相方法。我们的方法可以独立处理每个帧，从而完全并行处理。在第一阶段，我们训练一个高覆盖率的地标探测器，产生一组320个地标，这些地标均匀分布在脸上，覆盖突出特征（如眼角）以及更均匀的区域（如脸颊和前额）。</p><h3 id="training-and-losses">Training and Losses</h3><p>通用先验模型参数为<span class="math inline">\(\Phi=[\Phi_{exp},\Phi_{id},\Phi_{dec}]\)</span>。优化的算法如下：</p><p><span class="math display">\[\Phi^{*} = argmin \sum_{i\in{\mathcal{N}_{\mathcal{I}}}}\sum_{f\in{\mathcal{F}_{\mathcal{i}}}}\sum_{c\in{\mathcal{N}_{\mathcal{C}}}}\mathcal{L}_{total}(\Phi;\mathcal{I}_{f}^{i,c})\]</span></p><p><span class="math display">\[ \mathcal{N}_{\mathcal{I}} : different{\}identities \]</span></p><p><span class="math display">\[ \mathcal{N}_{\mathcal{F}_i} : frames\]</span></p><p><span class="math display">\[ \mathcal{N}_{\mathcal{C}} : different{\}camera{\ }views \]</span></p><p>整体损失函数算式如下： <span class="math display">\[\mathcal{L}_{total}(\Phi;\mathcal{I}_{f}^{i,c}) =\mathcal{L}_{rec}(\Phi;\mathcal{I}_{f}^{i,c}) +\mathcal{L}_{mvp}(\Phi;\mathcal{I}_{f}^{i,c}) + \mathcal{L}_{seg}(\Phi)\]</span></p><p>重建损失函数算式如下： <span class="math display">\[\mathcal{L}_{rec}(\Phi;\mathcal{I}_p) =\mathcal{L}_{pho}(\Phi;\mathcal{I}_{f}^{i,c}) +\mathcal{L}_{vgg}(\Phi;\mathcal{I}_{f}^{i,c}) +\mathcal{L}_{gan}(\Phi;\mathcal{I}_{f}^{i,c})\]</span><span class="math display">\[\mathcal{L}_{pho}(\Phi;\mathcal{I}_p) = \lambda_{pho}\frac{1}{\mathcal{N}_{\mathcal{P}}} \sum_{p \in{\mathcal{P}}}||\mathcal{I}_{f}^{i,c}(p) - \tilde{\mathcal{I}}_{f}^{i,c}(p)||_1\]</span></p><p>分割损失函数如下： <span class="math display">\[\mathcal{L}_{seg}(\Phi;\mathcal{I}_{p}) = \lambda_{seg}\frac{1}{\mathcal{N}_{\mathcal{P}}} \sum_{p \in{\mathcal{P}}}||\mathcal{O}_{f}^{i,c}(p) - \mathcal{S}_{f}^{i,c}(p)||_1\]</span></p><h3 id="conditioning-data-acquisition">Conditioning DataAcquisition</h3><p>要为用户重建逼真的头像，我们首先获取UPM所需的训练数据。对于捕获数据过程，要求用户在将手机从左到右，然后上下移动时保持固定的中性表情，以完全捕获整个头部，包括头发。</p><p>获取数据流程如下： 1.利用iPhone12的深度相机对用户中性表情进行各角度拍摄； 2.每张照片都检测出人脸landmarks； 3.对照片进行分割，得到分割掩码和轮廓痕迹 4.使用PCA模型，优化PCA系数，头部旋转角度，平移等参数，得到mesh 5.mesh经过unwarp得到texture 6. 利用之前得到的mesh和texture，渲染得到3DFace</p><h3 id="personalized-decoder-generation">Personalized DecoderGeneration</h3><p>Personalize Reconstruct流程：将获取到的mesh转化成geometryimage，geometry image和neutraltexture一并输入到UMP，最后渲染输出avatar。</p><p>目前存在弊端就是输入数据存在domaingap，其一是训练UMP的数据及实在静态且亮度均匀的环境下采集的，而手机数据是在自然环境光下获取的，其二是由于物理限制，手机捕获数据仅覆盖头部的前半球面样貌。</p><p>为了弥补这个差距，采用两步措施。一是手机面部数据的mesh拟合算法应用到实验室采集数据上，用离散个别相机照片等效于手持相机拍摄的效果；二是纹理归一化，选取误差最小的图片。</p><h3 id="finetuning-a-personalized-decoder">Finetuning a PersonalizedDecoder</h3><p><span class="math display">\[\mathcal{L}_{ref}(\Phi;\mathcal{I}_{f}) =\mathcal{L}_{rec}(\Phi;\mathcal{I}_{f}) +\mathcal{L}_{hole}(\Phi;\mathcal{I}_{f}) +\mathcal{L}_{seg}(\Phi;\mathcal{I}_{f})\]</span><span class="math display">\[\mathcal{L}_{hole}(\Phi;\mathcal{I}_{f}) =\lambda_{hole}||max(\mathcal{T}_{f} -\mathcal{O}_{f},0)·\mathcal{T}_{f}||_1\]</span></p><p><span class="math inline">\(\mathcal{I}_{f}\)</span>: expressionframes;   <span class="math inline">\(\mathcal{T}_{f}\)</span>: renderedmask;   <span class="math inline">\(\mathcal{O}_{f}\)</span>: theintegrated opacity computed during ray marching</p><h2 id="局限性">局限性</h2><ol type="1"><li>需要大量时间处理数据，数据不合适会造成过拟合现象。</li><li>用于构建UPM的数据与真实世界数据之间的域差距。</li><li>无法重建出完整身体，发型上的重构仍有待改进，包含眼镜的人脸重建效果有待改进。</li></ol>]]></content>
      
      
      <categories>
          
          <category> Face Reconstruction </category>
          
          <category> Paper Notes </category>
          
          <category> TOG </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Face Reconstruction </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>3D Human Reconstruction Paper List</title>
      <link href="/2022/09/30/%E8%AE%BA%E6%96%87%E5%90%88%E9%9B%86--Awesome%203D%20Human%20Reconstruction/"/>
      <url>/2022/09/30/%E8%AE%BA%E6%96%87%E5%90%88%E9%9B%86--Awesome%203D%20Human%20Reconstruction/</url>
      
        <content type="html"><![CDATA[<h1 id="awesome-3d-human-reconstruction">Awesome 3D HumanReconstruction</h1><p>A curated list of related resources for 3d human reconstruction. <a href="https://github.com/rlczddl/awesome-3d-human-reconstruction">Link</a></p><h2 id="contents">Contents</h2><ul><li><a href="#3d-human">papers</a></li><li><a href="#related">related papers</a></li><li><a href="#parametric-model">parametric model</a></li><li><a href="#dataset">dataset</a></li><li><a href="#labs">labs</a></li><li><a href="#other-related-awesome">other related awesome</a></li><li><a href="#survey">survey</a></li></ul><p><span id="more"></span></p><h2 id="d-human">3d human</h2><h3 id="nerf-or-pifu">nerf or pifu</h3><h5 id="stereopifu-depth-aware-clothed-human-digitization-via-stereo-vision-paper">•StereoPIFu: Depth Aware Clothed Human Digitization via Stereo Vision <a href="https://arxiv.org/pdf/2104.05289.pdf">paper</a></h5><h5 id="learning-implicit-3d-representations-of-dressed-humans-from-sparse-views-paper">•Learning Implicit 3D Representations of Dressed Humans from Sparse Views<a href="https://arxiv.org/pdf/2104.08013v1.pdf">paper</a></h5><h5 id="animatable-neural-radiance-fields-for-human-body-modeling-paper">•Animatable Neural Radiance Fields for Human Body Modeling <a href="https://arxiv.org/pdf/2105.02872.pdf">paper</a></h5><h5 id="pamir-parametric-model-conditioned-implicit-representation-for-image-based-human-reconstruction-paper-code">•PaMIR: Parametric Model-Conditioned Implicit Representation forImage-based Human Reconstruction <a href="https://arxiv.org/pdf/2007.03858.pdf">paper</a> <a href="https://github.com/ZhengZerong/PaMIR">code</a></h5><h5 id="neural-actor-neural-free-view-synthesis-of-human-actors-with-pose-control-papercode">•Neural Actor: Neural Free-view Synthesis of Human Actors with PoseControl <a href="http://gvv.mpi-inf.mpg.de/projects/NeuralActor/">paper&amp;code</a></h5><h5 id="moco-flow-neural-motion-consensus-flow-for-dynamic-humans-in-stationary-monocular-cameras-paper">•MoCo-Flow: Neural Motion Consensus Flow for Dynamic Humans in StationaryMonocular Cameras <a href="https://arxiv.org/pdf/2106.04477v1.pdf">paper</a></h5><h5 id="doublefield-bridging-the-neural-surface-and-radiance-fields-for-high-fidelity-human-rendering-paper">•DoubleField: Bridging the Neural Surface and Radiance Fields forHigh-fidelity Human Rendering <a href="https://arxiv.org/pdf/2106.03798.pdf">paper</a></h5><h5 id="bridge-the-gap-between-model-based-and-model-free-human-reconstruction-paper">•Bridge the Gap Between Model-based and Model-free Human Reconstruction<a href="https://arxiv.org/pdf/2106.06415v1.pdf">paper</a></h5><h5 id="metaavatar-learning-animatable-clothed-human-models-from-few-depth-images-paper">•MetaAvatar: Learning Animatable Clothed Human Models from Few DepthImages <a href="https://arxiv.org/pdf/2106.11944v1.pdf">paper</a></h5><h5 id="animatable-neural-radiance-fields-from-monocular-rgb-video-paper">•Animatable Neural Radiance Fields from Monocular RGB Video <a href="https://arxiv.org/pdf/2106.13629v1.pdf">paper</a></h5><h5 id="few-shot-neural-human-performance-rendering-from-sparse-rgbd-videos-paper">•Few-shot Neural Human Performance Rendering from Sparse RGBD Videos <a href="https://arxiv.org/pdf/2107.06505v1.pdf">paper</a></h5><h5 id="relightable-neural-video-portrait-paper">• Relightable NeuralVideo Portrait <a href="https://arxiv.org/pdf/2107.14735v1.pdf">paper</a></h5><h5 id="flame-in-nerf-neural-control-of-radiance-fields-for-free-view-face-animation-paper">•FLAME-in-NeRF : Neural control of Radiance Fields for Free View FaceAnimation <a href="https://arxiv.org/pdf/2108.04913v1.pdf">paper</a></h5><h5 id="arch-animation-ready-clothed-human-reconstruction-revisited-paper">•ARCH++: Animation-Ready Clothed Human Reconstruction Revisited <a href="https://arxiv.org/pdf/2108.07845.pdf">paper</a></h5><h5 id="neural-gif-neural-generalized-implicit-functions-for-animating-people-in-clothing-paper">•Neural-GIF: Neural Generalized Implicit Functions for Animating Peoplein Clothing <a href="https://virtualhumans.mpi-inf.mpg.de/neuralgif/">paper</a></h5><h5 id="neural-human-performer-learning-generalizable-radiance-fields-for-human-performance-rendering-paper-code">•Neural Human Performer: Learning Generalizable Radiance Fields for HumanPerformance Rendering <a href="https://arxiv.org/pdf/2109.07448v1.pdf">paper</a> <a href="https://youngjoongunc.github.io/nhp/">code</a></h5><h5 id="topologically-consistent-multi-view-face-inference-using-volumetric-sampling-paper">•Topologically Consistent Multi-View Face Inference Using VolumetricSampling <a href="https://arxiv.org/pdf/2110.02948.pdf">paper</a></h5><h5 id="creating-and-reenacting-controllable-3d-humans-with-differentiable-rendering-paper">•Creating and Reenacting Controllable 3D Humans with DifferentiableRendering <a href="https://arxiv.org/pdf/2110.11746v1.pdf">paper</a></h5><h5 id="h-nerf-neural-radiance-fields-for-rendering-and-temporal-reconstruction-of-humans-in-motion-paper">•H-NeRF: Neural Radiance Fields for Rendering and Temporal Reconstructionof Humans in Motion <a href="https://arxiv.org/pdf/2110.13746v1.pdf">paper</a></h5><h5 id="fenerf-face-editing-in-neural-radiance-fields-paper">• FENeRF:Face Editing in Neural Radiance Fields <a href="https://arxiv.org/pdf/2111.15490v1.pdf">paper</a></h5><h5 id="latenthuman-shape-and-pose-disentangled-latent-representation-for-human-bodies-papercode">•LatentHuman: Shape-and-Pose Disentangled Latent Representation for HumanBodies <a href="https://latenthuman.github.io/">paper&amp;code</a></h5><h5 id="neural-head-avatars-from-monocular-rgb-videos-paper">• NeuralHead Avatars from Monocular RGB Videos <a href="https://arxiv.org/pdf/2112.01554v1.pdf">paper</a></h5><h5 id="humannerf-generalizable-neural-human-radiance-field-from-sparse-inputs-paper">•HumanNeRF: Generalizable Neural Human Radiance Field from Sparse Inputs<a href="https://arxiv.org/pdf/2112.02789v1.pdf">paper</a></h5><h5 id="implicit-neural-deformation-for-multi-view-face-reconstruction-paper">•Implicit Neural Deformation for Multi-View Face Reconstruction <a href="https://arxiv.org/pdf/2112.02494v1.pdf">paper</a></h5><h5 id="mofanerf-morphable-facial-neural-radiance-field-paper-code">•MoFaNeRF: Morphable Facial Neural Radiance Field <a href="https://arxiv.org/pdf/2112.02308.pdf">paper</a> <a href="https://github.com/zhuhao-nju/mofanerf">code</a></h5><h5 id="geometry-guided-progressive-nerf-for-generalizable-and-efficient-neural-human-rendering-paper">•Geometry-Guided Progressive NeRF for Generalizable and Efficient NeuralHuman Rendering <a href="https://arxiv.org/pdf/2112.04312v1.pdf">paper</a></h5><h5 id="headnerf-a-real-time-nerf-based-parametric-head-model-paper-code">•HeadNeRF: A Real-time NeRF-based Parametric Head Model <a href="https://arxiv.org/pdf/2112.05637v1.pdf">paper</a> <a href="https://github.com/CrisHY1995/headnerf">code</a></h5><h5 id="i-m-avatar-implicit-morphable-head-avatars-from-videos-paper-code">•I M Avatar: Implicit Morphable Head Avatars from Videos <a href="https://arxiv.org/pdf/2112.07471v1.pdf">paper</a> <a href="https://github.com/zhengyuf/IMavatar">code</a></h5><h5 id="lookingoodπ-real-time-person-independent-neural-re-rendering-for-high-quality-human-performance-capture-paper">•LookinGoodπ: Real-time Person-independent Neural Re-rendering forHigh-quality Human Performance Capture <a href="https://arxiv.org/pdf/2112.08037v1.pdf">paper</a></h5><h5 id="icon-implicit-clothed-humans-obtained-from-normals-paper-code">•ICON: Implicit Clothed humans Obtained from Normals <a href="https://arxiv.org/pdf/2112.09127v1.pdf">paper</a> <a href="https://github.com/YuliangXiu/ICON">code</a></h5><h5 id="dd-nerf-double-diffusion-neural-radiance-field-as-a-generalizable-implicit-body-representation-paper">•DD-NeRF: Double-Diffusion Neural Radiance Field as a GeneralizableImplicit Body Representation <a href="https://arxiv.org/pdf/2112.12390v1.pdf">paper</a></h5><h5 id="human-view-synthesis-using-a-single-sparse-rgb-d-input-paper">•Human View Synthesis using a Single Sparse RGB-D Input <a href="https://arxiv.org/pdf/2112.13889v1.pdf">paper</a></h5><h5 id="surface-aligned-neural-radiance-fields-for-controllable-3d-human-synthesis-paper">•Surface-Aligned Neural Radiance Fields for Controllable 3D HumanSynthesis <a href="https://arxiv.org/pdf/2201.01683v1.pdf">paper</a></h5><h5 id="embodied-hands-modeling-and-capturing-hands-and-bodies-together-paper">•Embodied Hands: Modeling and Capturing Hands and Bodies Together <a href="https://arxiv.org/pdf/2201.02610v1.pdf">paper</a></h5><h5 id="humannerf-free-viewpoint-rendering-of-moving-people-from-monocular-video-paper-code">•HumanNeRF: Free-viewpoint Rendering of Moving People from MonocularVideo <a href="https://arxiv.org/pdf/2201.04127v1.pdf">paper</a> <a href="https://github.com/chungyiweng/humannerf">code</a></h5><h5 id="gdna-towards-generative-detailed-neural-avatars-paper-code">•gDNA: Towards Generative Detailed Neural Avatars <a href="https://arxiv.org/pdf/2201.04123v1.pdf">paper</a> <a href="https://github.com/xuchen-ethz/gdna">code</a></h5><h5 id="selfrecon-self-reconstruction-your-digital-avatar-from-monocular-video-paper">•SelfRecon: Self Reconstruction Your Digital Avatar from Monocular Video<a href="https://arxiv.org/pdf/2201.12792v1.pdf">paper</a></h5><h5 id="neuvv-neural-volumetric-videos-with-immersive-rendering-and-editing-paper">•NeuVV: Neural Volumetric Videos with Immersive Rendering and Editing <a href="https://arxiv.org/pdf/2202.06088v1.pdf">paper</a></h5><h5 id="animatable-neural-radiance-fields-for-modeling-dynamic-human-bodies-paper-code">•Animatable Neural Radiance Fields for Modeling Dynamic Human Bodies <a href="https://arxiv.org/abs/2105.02872">paper</a> <a href="https://github.com/zju3dv/animatable_nerf">code</a></h5><h5 id="deepmulticap-performance-capture-of-multiple-characters-using-sparse-multiview-cameras-paper-code">•DeepMultiCap: Performance Capture of Multiple Characters Using SparseMultiview Cameras <a href="https://arxiv.org/abs/2105.00261">paper</a><a href="https://github.com/DSaurus/DeepMultiCap">code</a></h5><h5 id="neuralfusion-neural-volumetric-rendering-under-human-object-interactions-paper">•NeuralFusion: Neural Volumetric Rendering under Human-objectInteractions <a href="https://arxiv.org/pdf/2202.12825v1.pdf">paper</a></h5><h5 id="pina-learning-a-personalized-implicit-neural-avatar-from-a-single-rgb-d-video-sequence-paper-code">•PINA: Learning a Personalized Implicit Neural Avatar from a Single RGB-DVideo Sequence <a href="https://arxiv.org/pdf/2203.01754v1.pdf">paper</a> <a href="https://github.com/zj-dong/pina/tree/page">code</a></h5><h5 id="neuman-neural-human-radiance-field-from-a-single-video-paper">•NeuMan: Neural Human Radiance Field from a Single Video <a href="https://arxiv.org/pdf/2203.12575v1.pdf">paper</a></h5><h5 id="m-avatar-implicit-morphable-head-avatars-from-videos-paper-code">• MAvatar: Implicit Morphable Head Avatars from Videos <a href="https://arxiv.org/pdf/2112.07471.pdf">paper</a> <a href="https://github.com/zhengyuf/IMavatar">code</a></h5><h5 id="imface-a-nonlinear-3d-morphable-face-model-with-implicit-neural-representations-paper">•ImFace: A Nonlinear 3D Morphable Face Model with Implicit NeuralRepresentations <a href="https://arxiv.org/pdf/2203.14510.pdf">paper</a></h5><h5 id="coap-compositional-articulated-occupancy-of-people-paper-code">•COAP: Compositional Articulated Occupancy of People <a href="https://arxiv.org/pdf/2204.06184v1.pdf">paper</a> <a href="https://github.com/markomih/COAP">code</a></h5><h5 id="sunstage-portrait-reconstruction-and-relighting-using-the-sun-as-a-light-stage-paper">•SunStage: Portrait Reconstruction and Relighting using the Sun as aLight Stage <a href="https://arxiv.org/pdf/2204.03648v1.pdf">paper</a></h5><h5 id="lisa-learning-implicit-shape-and-appearance-of-hands-paper">•LISA: Learning Implicit Shape and Appearance of Hands <a href="https://arxiv.org/pdf/2204.01695v1.pdf">paper</a></h5><h5 id="animatable-neural-radiance-fields-from-monocular-rgb-d-paper">•Animatable Neural Radiance Fields from Monocular RGB-D <a href="https://arxiv.org/pdf/2204.01218v1.pdf">paper</a></h5><h5 id="jiff-jointly-aligned-implicit-face-function-for-high-quality-single-view-clothed-human-reconstruction-paper">•JIFF: Jointly-aligned Implicit Face Function for High Quality SingleView Clothed Human Reconstruction <a href="https://arxiv.org/pdf/2204.10549v1.pdf">paper</a></h5><h5 id="generalizable-neural-performer-learning-robust-radiance-fields-for-human-novel-view-synthesis-paper">•Generalizable Neural Performer: Learning Robust Radiance Fields forHuman Novel View Synthesis <a href="https://arxiv.org/pdf/2204.11798v1.pdf">paper</a></h5><h5 id="danbo-disentangled-articulated-neural-body-representations-via-graph-neural-networks-paper">•DANBO: Disentangled Articulated Neural Body Representations via GraphNeural Networks <a href="https://arxiv.org/pdf/2205.01666.pdf">paper</a></h5><h5 id="single-view-3d-body-and-cloth-reconstruction-under-complex-poses-paper">•Single-view 3D Body and Cloth Reconstruction under Complex Poses <a href="https://arxiv.org/pdf/2205.04087v1.pdf">paper</a></h5><h5 id="keypointnerf-generalizing-image-based-volumetric-avatars-using-relative-spatial-encoding-of-keypoints-paper">•KeypointNeRF: Generalizing Image-based Volumetric Avatars using RelativeSpatial Encoding of Keypoints <a href="https://arxiv.org/pdf/2205.04992.pdf">paper</a></h5><h5 id="h3d-net-few-shot-high-fidelity-3d-head-reconstruction-paper-code">•H3D-Net: Few-Shot High-Fidelity 3D Head Reconstruction <a href="https://arxiv.org/pdf/2107.12512.pdf">paper</a> <a href="https://github.com/MaxPolak97/H3D-Net-reproduction">code</a></h5><h5 id="photorealistic-monocular-3d-reconstruction-of-humans-wearing-clothing-paper">•Photorealistic Monocular 3D Reconstruction of Humans Wearing Clothing <a href="https://arxiv.org/abs/2204.08906">paper</a></h5><h5 id="high-fidelity-human-avatars-from-a-single-rgb-camera-paper">•High-Fidelity Human Avatars from a Single RGB Camera <a href="https://github.com/hzhao1997/HF-Avatar">paper</a></h5><h5 id="uv-volumes-for-real-time-rendering-of-editable-free-view-human-performance-code">•UV Volumes for Real-time Rendering of Editable Free-view HumanPerformance <a href="https://github.com/fanegg/UV-Volumes">code</a></h5><h5 id="fof-learning-fourier-occupancy-field-for-monocular-real-time-human-reconstruction-paper">•FOF: Learning Fourier Occupancy Field for Monocular Real-time HumanReconstruction <a href="https://arxiv.org/pdf/2206.02194v1.pdf">paper</a></h5><h5 id="nemf-neural-motion-fields-for-kinematic-animation-paper">• NeMF:Neural Motion Fields for Kinematic Animation <a href="https://arxiv.org/pdf/2206.03287v1.pdf">paper</a></h5><h5 id="rignerf-fully-controllable-neural-3d-portraits-paper">• RigNeRF:Fully Controllable Neural 3D Portraits <a href="https://arxiv.org/pdf/2206.06481v1.pdf">paper</a></h5><h5 id="eyenerf-a-hybrid-representation-for-photorealistic-synthesis-animation-and-relighting-of-human-eyes-paper">•EyeNeRF: A Hybrid Representation for Photorealistic Synthesis, Animationand Relighting of Human Eyes <a href="https://arxiv.org/pdf/2206.08428v1.pdf">paper</a></h5><h5 id="tava-template-free-animatable-volumetric-actors-paper">• TAVA:Template-free Animatable Volumetric Actors <a href="https://arxiv.org/pdf/2206.08929v1.pdf">paper</a></h5><h5 id="neural-surface-reconstruction-of-dynamic-scenes-with-monocular-rgb-d-camera-homepage">•Neural Surface Reconstruction of Dynamic Scenes with Monocular RGB-DCamera <a href="https://ustc3dv.github.io/ndr/">homepage</a></h5><h5 id="generative-neural-articulated-radiance-fields-paper">•Generative Neural Articulated Radiance Fields <a href="https://arxiv.org/pdf/2206.14314v1.pdf">paper</a></h5><h5 id="neural-parameterization-for-dynamic-human-head-editing-paper">•Neural Parameterization for Dynamic Human Head Editing <a href="https://arxiv.org/pdf/2207.00210v1.pdf">paper</a></h5><h5 id="avatarcap-animatable-avatar-conditioned-monocular-human-volumetric-capture-paper-code">•AvatarCap: Animatable Avatar Conditioned Monocular Human VolumetricCapture <a href="https://arxiv.org/pdf/2207.02031v1.pdf">paper</a> <a href="https://github.com/lizhe00/AvatarCap">code</a></h5><h5 id="learning-implicit-templates-for-point-based-clothed-human-modeling-homepage">•Learning Implicit Templates for Point-Based Clothed Human Modeling <a href="https://jsnln.github.io/fite/">homepage</a></h5><h5 id="relighting4d-neural-relightable-human-from-videos-code">•Relighting4D:Neural Relightable Human from Videos <a href="https://github.com/FrozenBurning/Relighting4D">code</a></h5><h5 id="high-quality-human-reconstruction-via-diffusion-based-stereo-using-sparse-cameras-paper">•HighQuality Human Reconstruction via Diffusion-based Stereo Using SparseCameras <a href="https://arxiv.org/pdf/2207.08000v1.pdf">paper</a></h5><h5 id="crosshuman-learning-cross-guidance-from-multi-frame-images-for-human-reconstruction-paper">•CrossHuman:Learning Cross-Guidance from Multi-Frame Images for Human Reconstruction<a href="https://arxiv.org/pdf/2207.09735v1.pdf">paper</a></h5><h5 id="unif-united-neural-implicit-functions-for-clothed-human-reconstruction-and-animation-paper-code">•UNIF:United Neural Implicit Functions for Clothed Human Reconstruction andAnimation <a href="https://arxiv.org/pdf/2207.09835v1.pdf">paper</a> <a href="https://github.com/ShenhanQian/UNIF">code</a></h5><h5 id="drivable-volumetric-avatars-using-texel-aligned-features-paper">•DrivableVolumetric Avatars using Texel-Aligned Features <a href="https://arxiv.org/pdf/2207.09774v1.pdf">paper</a></h5><h5 id="the-one-where-they-reconstructed-3d-humans-and-environments-in-tv-shows-homepage">•TheOne Where They Reconstructed 3D Humans and Environments in TV Shows <a href="https://ethanweber.me/sitcoms3D/">homepage</a></h5><h5 id="avatargen-a-3d-generative-model-for-animatable-human-avatars-paper">•AvatarGen:a 3D Generative Model for Animatable Human Avatars <a href="https://arxiv.org/pdf/2208.00561.pdf">paper</a></h5><h5 id="voltemorph-realtime-controllable-and-generalisable-animation-of-volumetric-representations-paper">•VolTeMorph:Realtime, Controllable and Generalisable Animation of VolumetricRepresentations <a href="https://www.arxiv-vanity.com/papers/2208.00949/?continueFlag=acd9680585ca1db48ed3cbc277e4da97">paper</a></h5><h5 id="multi-neus-3d-head-portraits-from-single-image-with-neural-implicit-functions-paper">•Multi-NeuS:3D Head Portraits from Single Image with Neural Implicit Functions <a href="https://arxiv.org/pdf/2209.04436.pdf">paper</a></h5><h5 id="learning-to-relight-portrait-images-via-a-virtual-light-stage-and-synthetic-to-real-adaptation-paper">•Learningto Relight Portrait Images via a Virtual Light Stage andSynthetic-to-Real Adaptation <a href="https://arxiv.org/pdf/2209.10510.pdf">paper</a></h5><h3 id="geo-fusion">geo fusion</h3><h5 id="doublefusion-real-time-capture-of-human-performances-with-inner-body-shapes-from-a-single-depth-sensor-paper">•DoubleFusion: Real-time Capture of Human Performances with Inner BodyShapes from a Single Depth Sensor <a href="https://arxiv.org/abs/1804.06023">paper</a></h5><h5 id="robust-3d-self-portraits-in-seconds-paper">• Robust 3DSelf-portraits in Seconds <a href="https://arxiv.org/abs/2004.02460">paper</a></h5><h5 id="accurate-human-body-reconstruction-for-volumetric-video-paper">•ACCURATE HUMAN BODY RECONSTRUCTION FOR VOLUMETRIC VIDEO <a href="https://arxiv.org/pdf/2202.13118v1.pdf">paper</a></h5><h5 id="occlusionfusion-occlusion-aware-motion-estimation-for-real-time-dynamic-3d-reconstruction-paper-code">•OcclusionFusion: Occlusion-aware Motion Estimation for Real-time Dynamic3D Reconstruction <a href="https://arxiv.org/pdf/2203.07977v1.pdf">paper</a> <a href="https://github.com/wenbin-lin/OcclusionFusion/">code</a></h5><h3 id="photo">photo</h3><h5 id="portrait-reconstruction-and-relighting-using-the-sun-as-a-light-stage-homepage">•Portrait Reconstruction and Relighting using the Sun as a Light Stage <a href="https://grail.cs.washington.edu/projects/sunstage/">homepage</a></h5><h3 id="d-human-whole-body">3D human whole body</h3><h5 id="monocular-expressive-body-regression-through-body-driven-attention-paper-code">•Monocular Expressive Body Regression through Body-Driven Attention <a href="https://arxiv.org/abs/2008.09062">paper</a> <a href="https://github.com/vchoutas/expose">code</a></h5><h5 id="frankmocap-fast-monocular-3d-hand-and-body-motion-capture-by-regression-and-integration-paper-code">•FrankMocap: Fast Monocular 3D Hand and Body Motion Capture by Regressionand Integration <a href="https://arxiv.org/pdf/2008.08324.pdf">paper</a><a href="https://github.com/facebookresearch/frankmocap">code</a></h5><h5 id="collaborative-regression-of-expressive-bodies-using-moderation-papercode">•Collaborative Regression of Expressive Bodies using Moderation <a href="https://pixie.is.tue.mpg.de/">paper&amp;code</a></h5><h5 id="monocular-real-time-full-body-capture-with-inter-part-correlations-papercode">•Monocular Real-time Full Body Capture with Inter-part Correlations <a href="https://calciferzh.github.io/publications/zhou2021monocular">paper&amp;code</a></h5><h5 id="monocular-real-time-hand-shape-and-motion-capture-using-multi-modal-data-papercode">•Monocular Real-time Hand Shape and Motion Capture using Multi-modal Data<a href="https://calciferzh.github.io/publications/zhou2020monocular">paper&amp;code</a></h5><h5 id="real-time-rgbd-based-extended-body-pose-estimation-paper-code">•Real-time RGBD-based Extended Body Pose Estimation <a href="https://arxiv.org/pdf/2103.03663.pdf">paper</a> <a href="https://github.com/rmbashirov/rgbd-kinect-pose">code</a></h5><h5 id="detailed-avatar-recovery-from-single-image-paper">• DetailedAvatar Recovery from Single Image <a href="https://arxiv.org/pdf/2108.02931v1.pdf">paper</a></h5><h5 id="lightweight-multi-person-total-motion-capture-using-sparse-multi-view-cameras-paper">•Lightweight Multi-person Total Motion Capture Using Sparse Multi-viewCameras <a href="https://arxiv.org/pdf/2108.10378v1.pdf">paper</a></h5><h5 id="imposing-temporal-consistency-on-deep-monocular-body-shape-and-pose-estimation-paper">•Imposing Temporal Consistency on Deep Monocular Body Shape and PoseEstimation <a href="https://arxiv.org/pdf/2202.03074v1.pdf">paper</a></h5><h5 id="piano-a-parametric-hand-bone-model-from-magnetic-resonance-imaging-github">•PIANO: A Parametric Hand Bone Model from Magnetic Resonance Imaging <a href="https://github.com/reyuwei/PIANO_model">github</a></h5><h5 id="goal-generating-4d-whole-body-motion-for-hand-object-grasping-paper-code">•GOAL: Generating 4D Whole-Body Motion for Hand-Object Grasping <a href="https://arxiv.org/pdf/2112.11454.pdf">paper</a> <a href="https://github.com/otaheri/GOAL">code</a></h5><h5 id="pymaf-x-towards-well-aligned-full-body-model-regression-from-monocular-images-paper">•PyMAF-X: Towards Well-aligned Full-body Model Regression from MonocularImages <a href="https://arxiv.org/pdf/2207.06400v1.pdf">paper</a></h5><h3 id="d-human-body">3D human body</h3><h5 id="pose2mesh-graph-convolutional-network-for-3d-human-pose-and-mesh-recovery-from-a-2d-human-pose-paper-code">•Pose2Mesh: Graph Convolutional Network for 3D Human Pose and MeshRecovery from a 2D Human Pose <a href="https://arxiv.org/abs/2008.09047">paper</a> <a href="https://github.com/hongsukchoi/Pose2Mesh_RELEASE">code</a></h5><h5 id="monocular-real-time-volumetric-performance-capture-paper-code">•Monocular Real-Time Volumetric Performance Capture <a href="https://arxiv.org/abs/2007.13988">paper</a> <a href="https://github.com/Project-Splinter/MonoPort">code</a></h5><h5 id="full-body-awareness-from-partial-observations-paper-code">•Full-Body Awareness from Partial Observations <a href="https://arxiv.org/abs/2008.06046">paper</a> <a href="https://github.com/crockwell/partial_humans">code</a></h5><h5 id="centerhmr-a-bottom-up-single-shot-method-for-multi-person-3d-mesh-recovery-from-a-single-image-paper-code">•CenterHMR: a Bottom-up Single-shot Method for Multi-person 3D MeshRecovery from a Single Image <a href="https://arxiv.org/pdf/2008.12272.pdf">paper</a> <a href="https://github.com/Arthur151/CenterHMR">code</a></h5><h5 id="reconstructing-nba-players-paper-code">• Reconstructing NBAplayers <a href="https://arxiv.org/pdf/2007.13303.pdf">paper</a> <a href="https://github.com/luyangzhu/NBA-Players">code</a></h5><h5 id="going-beyond-free-viewpoint-creating-animatable-volumetric-video-of-human-performances-paper">•Going beyond Free Viewpoint: Creating Animatable Volumetric Video ofHuman Performances <a href="https://arxiv.org/abs/2009.00922">paper</a></h5><h5 id="synthetic-training-for-accurate-3d-human-pose-and-shape-estimation-in-the-wild-paper-code">•Synthetic Training for Accurate 3D Human Pose and Shape Estimation inthe Wild <a href="https://arxiv.org/pdf/2009.10013.pdf">paper</a> <a href="https://github.com/akashsengupta1997/STRAPS-3DHumanShapePose">code</a></h5><h5 id="monoclothcap-towards-temporally-coherent-clothing-capture-from-monocular-rgb-video-paper">•MonoClothCap: Towards Temporally Coherent Clothing Capture fromMonocular RGB Video <a href="https://arxiv.org/pdf/2009.10711.pdf">paper</a></h5><h5 id="multi-view-consistency-loss-for-improved-single-image-3d-reconstruction-of-clothed-people-paper-code">•Multi-View Consistency Loss for Improved Single-Image 3D Reconstructionof Clothed People <a href="https://arxiv.org/abs/2009.14162">paper</a><a href="https://akincaliskan3d.github.io/MV3DH/">code</a></h5><h5 id="synthetic-training-for-monocular-human-mesh-recovery-paper">•Synthetic Training for Monocular Human Mesh Recovery <a href="https://arxiv.org/abs/2010.14036">paper</a></h5><h5 id="pose2pose-3d-positional-pose-guided-3d-rotational-pose-prediction-for-expressive-3d-human-pose-and-mesh-estimation-paper">•Pose2Pose: 3D Positional Pose-Guided 3D Rotational Pose Prediction forExpressive 3D Human Pose and Mesh Estimation <a href="https://arxiv.org/pdf/2011.11534.pdf">paper</a></h5><h5 id="deep-physics-aware-inference-of-cloth-deformation-for-monocular-human-performance-capture">•Deep Physics-aware Inference of Cloth Deformation for Monocular HumanPerformance Capture</h5><h5 id="d-human-body-capture-from-egocentric-video-via-3d-scene-grounding-paper">•4D Human Body Capture from Egocentric Video via 3D Scene Grounding <a href="https://arxiv.org/abs/2011.13341">paper</a></h5><h5 id="hybrik-a-hybrid-analytical-neural-inverse-kinematics-solution-for-3d-human-pose-and-shape-estimation-paper-code">•HybrIK: A Hybrid Analytical-Neural Inverse Kinematics Solution for 3DHuman Pose and Shape Estimation <a href="https://arxiv.org/pdf/2011.14672.pdf">paper</a> <a href="https://github.com/Jeff-sjtu/HybrIK">code</a></h5><h5 id="we-are-more-than-our-joints-predicting-how-3d-bodies-move-paper">•We are More than Our Joints: Predicting how 3D Bodies Move <a href="https://arxiv.org/pdf/2012.00619.pdf">paper</a></h5><h5 id="smply-benchmarking-3d-human-pose-estimation-in-the-wild-paper">•SMPLy Benchmarking 3D Human Pose Estimation in the Wild <a href="https://arxiv.org/pdf/2012.02743v1.pdf">paper</a></h5><h5 id="synthesizing-long-term-3d-human-motion-and-interaction-in-3d-paper">•Synthesizing Long-Term 3D Human Motion and Interaction in 3D <a href="https://jiashunwang.github.io/Long-term-Motion-in-3D-Scenes/">paper</a></h5><h5 id="detailed-3d-human-body-reconstruction-from-multi-view-images-combining-voxel-super-resolution-and-learned-implicit-representation">•Detailed 3D Human Body Reconstruction from Multi-view Images CombiningVoxel Super-Resolution and Learned Implicit Representation</h5><h5 id="a-novel-joint-points-and-silhouette-based-method-to-estimate-3d-human-pose-and-shape">•A novel joint points and silhouette-based method to estimate 3D humanpose and shape</h5><h5 id="facedet3d-facial-expressions-with-3d-geometric-detail-prediction-paper">•FaceDet3D: Facial Expressions with 3D Geometric Detail Prediction <a href="https://arxiv.org/pdf/2012.07999.pdf">paper</a></h5><h5 id="nerface-dynamic-neural-radiance-fields-for-monocular-4d-facial-avatar-reconstruction-code">•NerFACE: Dynamic Neural Radiance Fields for Monocular 4D Facial AvatarReconstruction <a href="https://github.com/gafniguy/4D-Facial-Avatars">code</a></h5><h5 id="learning-complex-3d-human-self-contact-paper">• Learning Complex3D Human Self-Contact <a href="https://arxiv.org/pdf/2012.10366.pdf">paper</a></h5><h5 id="populating-3d-scenes-by-learning-human-scene-interaction-paper">•Populating 3D Scenes by Learning Human-Scene Interaction <a href="https://arxiv.org/pdf/2012.11581.pdf">paper</a></h5><h5 id="anr-articulated-neural-rendering-for-virtual-avatars-paper">•ANR: Articulated Neural Rendering for Virtual Avatars <a href="https://anr-avatars.github.io/">paper</a></h5><h5 id="human-mesh-recovery-from-multiple-shots-paper">• Human MeshRecovery from Multiple Shots <a href="https://geopavlakos.github.io/multishot/">paper</a></h5><h5 id="lifting-2d-stylegan-for-3d-aware-face-generation-paper">•Lifting 2D StyleGAN for 3D-Aware Face Generation <a href="S3:%20Neural%20Shape,%20Skeleton,%20and%20Skinning%20Fields%20for%203D%20Human%20Modeling">paper</a></h5><h5 id="capturing-detailed-deformations-of-moving-human-bodies-paper">•Capturing Detailed Deformations of Moving Human Bodies <a href="https://arxiv.org/pdf/2102.07343.pdf">paper</a></h5><h5 id="d-human-pose-shape-and-texture-from-low-resolution-images-and-videos-paper">•3D Human Pose, Shape and Texture from Low-Resolution Images and Videos<a href="https://arxiv.org/pdf/2103.06498v1.pdf">paper</a></h5><h5 id="challencap-monocular-3d-capture-of-challenging-human-performances-using-multi-modal-references-paper">•ChallenCap: Monocular 3D Capture of Challenging Human Performances usingMulti-Modal References <a href="https://arxiv.org/pdf/2103.06747v1.pdf">paper</a></h5><h5 id="smplicit-topology-aware-generative-model-for-clothed-people-papercode">•SMPLicit: Topology-aware Generative Model for Clothed People <a href="http://www.iri.upc.edu/people/ecorona/smplicit/">paper&amp;code</a></h5><h5 id="learning-high-fidelity-depths-of-dressed-humans-by-watching-social-media-dance-videos-paper">•Learning High Fidelity Depths of Dressed Humans by Watching Social MediaDance Videos <a href="https://arxiv.org/pdf/2103.03319.pdf">paper</a></h5><h5 id="neuralhumanfvv-real-time-neural-volumetric-human-performance-rendering-using-rgb-cameras-paper">•NeuralHumanFVV: Real-Time Neural Volumetric Human Performance Renderingusing RGB Cameras <a href="https://arxiv.org/pdf/2103.07700v1.pdf">paper</a></h5><h5 id="probabilistic-3d-human-shape-and-pose-estimation-from-multiple-unconstrained-images-in-the-wild-paper">•Probabilistic 3D Human Shape and Pose Estimation from MultipleUnconstrained Images in the Wild <a href="https://arxiv.org/pdf/2103.10978v1.pdf">paper</a></h5><h5 id="dcrowdnet-2d-human-pose-guided-3d-crowd-human-pose-and-shape-estimation-in-the-wild-paper">•3DCrowdNet: 2D Human Pose-Guided 3D Crowd Human Pose and ShapeEstimation in the Wild <a href="https://arxiv.org/pdf/2104.07300v1.pdf">paper</a></h5><h5 id="scale-modeling-clothed-humans-with-a-surface-codec-of-articulated-local-elements-paper">•SCALE: Modeling Clothed Humans with a Surface Codec of Articulated LocalElements <a href="https://arxiv.org/pdf/2104.07660.pdf">paper</a></h5><h5 id="multi-person-implicit-reconstruction-from-a-single-image-paper">•Multi-person Implicit Reconstruction from a Single Image <a href="https://arxiv.org/pdf/2104.09283v1.pdf">paper</a></h5><h5 id="pare-part-attention-regressor-for-3d-human-body-estimation-paper-code">•PARE: Part Attention Regressor for 3D Human Body Estimation <a href="https://arxiv.org/pdf/2104.08527v1.pdf">paper</a> <a href="https://pare.is.tue.mpg.de/">code</a></h5><h5 id="temporal-consistency-loss-for-high-resolution-textured-and-clothed-3d-human-reconstruction-from-monocular-video-paper">•Temporal Consistency Loss for High Resolution Textured and Clothed 3DHuman Reconstruction from Monocular Video <a href="https://arxiv.org/pdf/2104.09259.pdf">paper</a></h5><h5 id="function4d-real-time-human-volumetric-capture-from-very-sparse-consumer-rgbd-sensors-paper">•Function4D: Real-time Human Volumetric Capture from Very Sparse ConsumerRGBD Sensors <a href="http://www.liuyebin.com/Function4D/Function4D.html">paper</a></h5><h5 id="end-to-end-human-pose-and-mesh-reconstruction-with-transformers-paper">•End-to-End Human Pose and Mesh Reconstruction with Transformers <a href="https://github.com/microsoft/MeshTransformer">paper</a></h5><h5 id="revitalizing-optimization-for-3d-human-pose-and-shape-estimation-a-sparse-constrained-formulation-paper">•Revitalizing Optimization for 3D Human Pose and Shape Estimation: ASparse Constrained Formulation <a href="https://arxiv.org/pdf/2105.13965v1.pdf">paper</a></h5><h5 id="sharp-shape-aware-reconstruction-of-people-in-loose-clothing-paper">•SHARP: Shape-Aware Reconstruction of People In Loose Clothing <a href="https://arxiv.org/pdf/2106.04778v1.pdf">paper</a></h5><h5 id="thundr-transformer-based-3d-human-reconstruction-with-markers-paper">•THUNDR: Transformer-based 3D HUmaN Reconstruction with Markers <a href="https://arxiv.org/pdf/2106.09336v1.pdf">paper</a></h5><h5 id="deep3dpose-realtime-reconstruction-of-arbitrarily-posed-human-bodies-from-single-rgb-images-paper">•Deep3DPose: Realtime Reconstruction of Arbitrarily Posed Human Bodiesfrom Single RGB Images <a href="https://arxiv.org/pdf/2106.11536v1.pdf">paper</a></h5><h5 id="learning-local-recurrent-models-for-human-mesh-recovery-paper">•Learning Local Recurrent Models for Human Mesh Recovery <a href="https://arxiv.org/pdf/2107.12847v1.pdf">paper</a></h5><h5 id="posefusion2-simultaneous-background-reconstruction-and-human-shape-recovery-in-real-time-paper">•PoseFusion2: Simultaneous Background Reconstruction and Human ShapeRecovery in Real-time <a href="https://arxiv.org/pdf/2108.00695v1.pdf">paper</a></h5><h5 id="lasor-learning-accurate-3d-human-pose-and-shape-via-synthetic-occlusion-aware-data-and-neural-mesh-rendering-paper">•LASOR: Learning Accurate 3D Human Pose and Shape Via SyntheticOcclusion-Aware Data and Neural Mesh Rendering <a href="https://arxiv.org/pdf/2108.00351v1.pdf">paper</a></h5><h5 id="learning-motion-priors-for-4d-human-body-capture-in-3d-scenes-paper-code">•Learning Motion Priors for 4D Human Body Capture in 3D Scenes <a href="https://arxiv.org/pdf/2108.10399v1.pdf">paper</a> <a href="https://github.com/sanweiliti/LEMO">code</a></h5><h5 id="probabilistic-modeling-for-human-mesh-recovery-papercode">•Probabilistic Modeling for Human Mesh Recovery <a href="https://www.seas.upenn.edu/~nkolot/projects/prohmr/">paper&amp;code</a></h5><h5 id="dc-gnet-deep-mesh-relation-capturing-graph-convolution-network-for-3d-human-shape-reconstruction-paper">•DC-GNet: Deep Mesh Relation Capturing Graph Convolution Network for 3DHuman Shape Reconstruction <a href="https://arxiv.org/pdf/2108.12384v1.pdf">paper</a></h5><h5 id="encoder-decoder-with-multi-level-attention-for-3d-human-shape-and-pose-estimation-paper-code">•Encoder-decoder with Multi-level Attention for 3D Human Shape and PoseEstimation <a href="https://arxiv.org/pdf/2109.02303v1.pdf">paper</a> <a href="https://github.com/ziniuwan/maed">code</a></h5><h5 id="action-conditioned-3d-human-motion-synthesis-with-transformer-vae-code">•Action-Conditioned 3D Human Motion Synthesis with Transformer VAE <a href="https://github.com/Mathux/ACTOR">code</a></h5><h5 id="learning-to-regress-bodies-from-images-using-differentiable-semantic-rendering-paper">•Learning to Regress Bodies from Images using Differentiable SemanticRendering <a href="https://arxiv.org/pdf/2110.03480v1.pdf">paper</a></h5><h5 id="deep-two-stream-video-inference-for-human-body-pose-and-shape-estimation-paper">•Deep Two-Stream Video Inference for Human Body Pose and Shape Estimation<a href="https://arxiv.org/pdf/2110.11680v1.pdf">paper</a></h5><h5 id="ultrapose-synthesizing-dense-pose-with-1-billion-points-by-human-body-decoupling-3d-model-paper">•UltraPose: Synthesizing Dense Pose with 1 Billion Points by Human-bodyDecoupling 3D Model <a href="https://arxiv.org/pdf/2110.15267v1.pdf">paper</a></h5><h5 id="body-size-and-depth-disambiguation-in-multi-person-reconstruction-from-single-images-paper-code">•Body Size and Depth Disambiguation in Multi-Person Reconstruction fromSingle Images <a href="https://arxiv.org/pdf/2111.01884v1.pdf">paper</a><a href="https://github.com/nicolasugrinovic/size_depth_disambiguation">code</a></h5><h5 id="unified-3d-mesh-recovery-of-humans-and-animals-by-learning-animal-exercise-paper">•Unified 3D Mesh Recovery of Humans and Animals by Learning AnimalExercise <a href="https://arxiv.org/pdf/2111.02450v1.pdf">paper</a></h5><h5 id="d-human-shape-and-pose-from-a-single-low-resolution-image-with-self-supervised-learning-paper-code">•3D Human Shape and Pose from a Single Low-Resolution Image withSelf-Supervised Learning <a href="https://arxiv.org/pdf/2007.13666.pdf">paper</a> <a href="https://github.com/xuxy09/RSC-Net">code</a></h5><h5 id="out-of-domain-human-mesh-reconstruction-via-bilevel-online-adaptation-paper-code">•Out-of-Domain Human Mesh Reconstruction via Bilevel Online Adaptation <a href="https://drive.google.com/file/d/1b6e3rMrVn_xNhM-MitqpLtulARdl4M9F/view?usp=sharing">paper</a><a href="https://www.google.com/url?q=https%3A%2F%2Fgithub.com%2Fsyguan96%2FDynaBOA&amp;sa=D&amp;sntz=1&amp;usg=AFQjCNHYmgSyYqdKGYNp7W-bAO2MrHfp1w">code</a></h5><h4 id="meshletemp-leveraging-the-learnable-vertex-vertex-relationship-to-generalize-human-pose-and-mesh-reconstruction-for-in-the-wild-scenes-paper">•MeshLeTemp: Leveraging the Learnable Vertex-Vertex Relationship toGeneralize Human Pose and Mesh Reconstruction for In-the-Wild Scenes <a href="https://arxiv.org/pdf/2202.07228v1.pdf">paper</a></h4><h5 id="monocular-human-shape-and-pose-with-dense-mesh-borne-local-image-features-paper">•Monocular Human Shape and Pose with Dense Mesh-borne Local ImageFeatures <a href="https://arxiv.org/pdf/2111.05319v1.pdf">paper</a></h5><h5 id="human-performance-capture-from-monocular-video-in-the-wild-paper">•Human Performance Capture from Monocular Video in the Wild <a href="https://arxiv.org/pdf/2111.14672.pdf">paper</a></h5><h5 id="probabilistic-estimation-of-3d-human-shape-and-pose-with-a-semantic-local-parametric-model-paper">•Probabilistic Estimation of 3D Human Shape and Pose with a SemanticLocal Parametric Model <a href="https://arxiv.org/pdf/2111.15404v1.pdf">paper</a></h5><h5 id="glamr-global-occlusion-aware-human-mesh-recovery-with-dynamic-cameras-paper-code">•GLAMR: Global Occlusion-Aware Human Mesh Recovery with Dynamic Cameras<a href="https://arxiv.org/pdf/2112.01524v1.pdf">paper</a> <a href="https://github.com/NVlabs/GLAMR">code</a></h5><h5 id="metaavatar-learning-animatable-clothed-human-models-from-few-depth-images-paper-code">•MetaAvatar: Learning Animatable Clothed Human Models from Few DepthImages} <a href="https://openreview.net/forum?id=Q-PA3D1OsDz">paper</a><a href="https://github.com/taconite/MetaAvatar-release">code</a></h5><h5 id="egobody-human-body-shape-motion-and-social-interactions-from-head-mounted-devices-paper-code">•EgoBody: Human Body Shape, Motion and Social Interactions fromHead-Mounted Devices <a href="https://arxiv.org/pdf/2112.07642v1.pdf">paper</a> <a href="https://sanweiliti.github.io/egobody/egobody.html">code</a></h5><h5 id="putting-people-in-their-place-monocular-regression-of-3d-people-in-depth-paper">•Putting People in their Place: Monocular Regression of 3D People inDepth <a href="https://arxiv.org/pdf/2112.08274v1.pdf">paper</a></h5><h5 id="multi-initialization-optimization-network-for-accurate-3d-human-pose-and-shape-estimation-paper">•Multi-initialization Optimization Network for Accurate 3D Human Pose andShape Estimation <a href="https://arxiv.org/pdf/2112.12917v1.pdf">paper</a></h5><h5 id="moothnet-a-plug-and-play-network-for-refining-human-poses-in-videos-paper">•moothNet: A Plug-and-Play Network for Refining Human Poses in Videos <a href="https://arxiv.org/pdf/2112.13715v1.pdf">paper</a></h5><h5 id="hspace-synthetic-parametric-humans-animated-in-complex-environments-paper">•HSPACE: Synthetic Parametric Humans Animated in Complex Environments <a href="https://arxiv.org/pdf/2112.12867v1.pdf">paper</a></h5><h5 id="votehmr-occlusion-aware-voting-network-for-robust-3d-human-mesh-recovery-from-partial-point-clouds-paper-code">•VoteHMR: Occlusion-Aware Voting Network for Robust 3D Human MeshRecovery from Partial Point Clouds <a href="https://arxiv.org/pdf/2110.08729.pdf">paper</a> <a href="https://github.com/hanabi7/VoteHMR">code</a></h5><h5 id="h4d-human-4d-modeling-by-learning-neural-compositional-representation-paper">•H4D: Human 4D Modeling by Learning Neural Compositional Representation<a href="https://arxiv.org/pdf/2203.01247v1.pdf">paper</a></h5><h5 id="capturing-humans-in-motion-temporal-attentive-3d-human-pose-and-shape-estimation-from-monocular-video-paper-code">•Capturing Humans in Motion: Temporal-Attentive 3D Human Pose and ShapeEstimation from Monocular Video <a href="https://arxiv.org/pdf/2203.08534v1.pdf">paper</a> <a href="https://mps-net.github.io/MPS-Net/">code</a></h5><h5 id="hybridcap-inertia-aid-monocular-capture-of-challenging-human-motions-paper">•HybridCap: Inertia-aid Monocular Capture of Challenging Human Motions <a href="https://arxiv.org/pdf/2203.09287v1.pdf">paper</a></h5><h5 id="learning-to-estimate-robust-3d-human-mesh-from-in-the-wild-crowded-scenes-paper-code">•Learning to Estimate Robust 3D Human Mesh from In-the-Wild CrowdedScenes <a href="https://arxiv.org/abs/2104.07300">paper</a> <a href="https://github.com/hongsukchoi/3DCrowdNet_RELEASE">code</a></h5><h5 id="bodyslam-joint-camera-localisation-mapping-and-human-motion-tracking-paper">•BodySLAM: Joint Camera Localisation, Mapping, and Human Motion Tracking<a href="https://arxiv.org/pdf/2205.02301v1.pdf">paper</a></h5><h5 id="hulc-3d-human-motion-capture-with-pose-manifold-sampling-and-dense-contact-guidance-paper">•HULC: 3D Human Motion Capture with Pose Manifold Sampling and DenseContact Guidance <a href="https://arxiv.org/pdf/2205.05677v1.pdf">paper</a></h5><h5 id="learned-vertex-descent-a-new-direction-for-3d-human-model-fitting-paper-code">•Learned Vertex Descent: A New Direction for 3D Human Model Fitting <a href="https://arxiv.org/pdf/2205.06254v1.pdf">paper</a> <a href="https://github.com/enriccorona/LVD">code</a></h5><h5 id="mug-multi-human-graph-network-for-3d-mesh-reconstruction-from-2d-pose-paper">•MUG: Multi-human Graph Network for 3D Mesh Reconstruction from 2D Pose<a href="https://www.arxivdaily.com/thread/26950">paper</a></h5><h5 id="accurate-3d-body-shape-regression-using-metric-and-semantic-attributes-paper">•Accurate 3D Body Shape Regression using Metric and Semantic Attributes<a href="https://arxiv.org/pdf/2206.07036v1.pdf">paper</a></h5><h5 id="capturing-and-inferring-dense-full-body-human-scene-contact-homepage">•Capturing and Inferring Dense Full-Body Human-Scene Contact <a href="https://rich.is.tue.mpg.de/index.html">homepage</a></h5><h5 id="occluded-human-body-capture-with-self-supervised-spatial-temporal-motion-prior-paper">•Occluded Human Body Capture with Self-Supervised Spatial-Temporal MotionPrior <a href="https://arxiv.org/pdf/2207.05375v1.pdf">paper</a></h5><h5 id="live-stream-temporally-embedded-3d-human-body-pose-and-shape-estimation-paper-code">•Live Stream Temporally Embedded 3D Human Body Pose and Shape Estimation<a href="https://arxiv.org/pdf/2207.12537v1.pdf">paper</a> <a href="https://github.com/ostadabbas/TePose">code</a></h5><h5 id="cliff-carrying-location-information-in-full-frames-into-human-pose-and-shape-estimation-paper">•CLIFF: Carrying Location Information in Full Frames into Human Pose andShape Estimation <a href="https://arxiv.org/pdf/2208.00571.pdf">paper</a></h5><h3 id="d-human-face">3d human face</h3><h5 id="high-fidelity-3d-digital-human-creation-from-rgb-d-selfies-paper-code">•High-Fidelity 3D Digital Human Creation from RGB-D Selfies <a href="https://arxiv.org/pdf/2010.05562.pdf">paper</a> <a href="https://github.com/tencent-ailab/hifi3dface">code</a></h5><h5 id="styleuv-diverse-and-high-quality-uv-map-generative-model-paper">•StyleUV: Diverse and High-quality UV Map Generative Model <a href="https://arxiv.org/pdf/2011.12893.pdf">paper</a></h5><h5 id="i3dmm-deep-implicit-3d-morphable-model-of-human-heads-paper">•i3DMM: Deep Implicit 3D Morphable Model of Human Heads <a href="https://arxiv.org/pdf/2011.14143v1.pdf">paper</a></h5><h5 id="relightable-3d-head-portraits-from-a-smartphone-video-paper">•Relightable 3D Head Portraits from a Smartphone Video <a href="https://arxiv.org/pdf/2012.09963.pdf">paper</a></h5><h5 id="learning-compositional-radiance-fields-of-dynamic-human-heads-paper">•Learning Compositional Radiance Fields of Dynamic Human Heads <a href="https://arxiv.org/pdf/2012.09955.pdf">paper</a></h5><h5 id="sider-single-image-neural-optimization-for-facial-geometric-detail-recovery-paper">•SIDER : Single-Image Neural Optimization for Facial Geometric DetailRecovery <a href="https://arxiv.org/pdf/2108.05465v1.pdf">paper</a></h5><h5 id="synergy-between-3dmm-and-3d-landmarks-for-accurate-3d-facial-geometry-paper-code">•Synergy between 3DMM and 3D Landmarks for Accurate 3D Facial Geometry <a href="https://arxiv.org/pdf/2110.09772.pdf">paper</a> <a href="https://github.com/choyingw/SynergyNet">code</a></h5><h5 id="high-quality-real-time-facial-capture-based-on-single-camera-paper">•HIGH-QUALITY REAL TIME FACIAL CAPTURE BASED ON SINGLE CAMERA <a href="https://arxiv.org/pdf/2111.07556v1.pdf">paper</a></h5><h5 id="self-supervised-high-fidelity-and-re-renderable-3d-facial-reconstruction-from-a-single-imag-paper">•Self-supervised High-fidelity and Re-renderable 3D Facial Reconstructionfrom a Single Imag <a href="https://arxiv.org/pdf/2111.08282.pdf">paper</a></h5><h5 id="generating-diverse-3d-reconstructions-from-a-single-occluded-face-image-paper">•Generating Diverse 3D Reconstructions from a Single Occluded Face Image<a href="https://arxiv.org/pdf/2112.00879v1.pdf">paper</a></h5><h5 id="self-supervised-robustifying-guidance-for-monocular-3d-face-reconstruction-paper">•Self-Supervised Robustifying Guidance for Monocular 3D FaceReconstruction <a href="https://arxiv.org/pdf/2112.14382v1.pdf">paper</a></h5><h5 id="babynet-reconstructing-3d-faces-of-babies-from-uncalibrated-photographs-paper">•BabyNet: Reconstructing 3D faces of babies from uncalibrated photographs<a href="https://arxiv.org/pdf/2203.05908v1.pdf">paper</a></h5><h5 id="s2f2-self-supervised-high-fidelity-face-reconstruction-from-monocular-image-paper">•S2F2: Self-Supervised High Fidelity Face Reconstruction from MonocularImage <a href="https://arxiv.org/pdf/2203.07732v1.pdf">paper</a></h5><h5 id="facial-geometric-detail-recovery-via-implicit-representation-paper-code">•Facial Geometric Detail Recovery via Implicit Representation <a href="https://arxiv.org/pdf/2203.09692v1.pdf">paper</a> <a href="https://github.com/deepinsight/insightface/tree/master/reconstruction/PBIDR">code</a></h5><h5 id="beyond-3dmm-learning-to-capture-high-fidelity-3d-face-shape-paper">•Beyond 3DMM: Learning to Capture High-fidelity 3D Face Shape <a href="https://arxiv.org/pdf/2204.04379v1.pdf">paper</a></h5><h5 id="from-2d-images-to-3d-model-weakly-supervised-multi-view-face-reconstruction-with-deep-fusion-paper">•From 2D Images to 3D Model: Weakly Supervised Multi-View FaceReconstruction with Deep Fusion <a href="https://arxiv.org/pdf/2204.03842v1.pdf">paper</a></h5><h5 id="f3d-face-reconstruction-with-dense-landmarks-paper">• F3D facereconstruction with dense landmarks <a href="https://arxiv.org/pdf/2204.02776v1.pdf">paper</a></h5><h5 id="emoca-emotion-driven-monocular-face-capture-and-animation-paper">•EMOCA: Emotion Driven Monocular Face Capture and Animation <a href="https://arxiv.org/pdf/2204.11312v1.pdf">paper</a></h5><h5 id="single-image-3d-face-reconstruction-under-perspective-projection-paper">•Single-Image 3D Face Reconstruction under Perspective Projection <a href="https://arxiv.org/pdf/2205.04126v1.pdf">paper</a></h5><h3 id="d-human-head">3d human head</h3><h5 id="deca-detailed-expression-capture-and-animation-paper-code">•DECA: Detailed Expression Capture and Animation <a href="https://arxiv.org/pdf/2012.04012.pdf">paper</a> <a href="https://github.com/YadiraF/DECA?utm_source=catalyzex.com">code</a></h5><h5 id="pixel-codec-avatars-paper">• Pixel Codec Avatars <a href="https://arxiv.org/pdf/2104.04638.pdf">paper</a></h5><h5 id="h3d-net-few-shot-high-fidelity-3d-head-reconstruction-paper">•H3D-Net: Few-Shot High-Fidelity 3D Head Reconstruction <a href="https://arxiv.org/pdf/2107.12512v1.pdf">paper</a></h5><h5 id="towards-metrical-reconstruction-of-human-faces-paper-code">•TowardsMetrical Reconstruction of Human Faces <a href="https://arxiv.org/pdf/2204.06607v1.pdf">paper</a> <a href="https://zielon.github.io/mica/">code</a></h5><h5 id="data-driven-3d-human-head-reconstruction-paper">•Data-driven 3Dhuman head reconstruction <a href="https://www.sciencedirect.com/science/article/abs/pii/S0097849319300317">paper</a></h5><h5 id="dynamic-3d-avatar-creation-from-hand-held-video-input-acm-paper">•Dynamic3D avatar creation from hand-held video input, ACM <a href="http://sofienbouaziz.com/pdf/Avatars_SIGG15.pdf">paper</a></h5><h5 id="realistic-one-shot-mesh-based-head-avatars-paper">•RealisticOne-shot Mesh-based Head Avatars <a href="https://arxiv.org/pdf/2206.08343.pdf">paper</a></h5><h5 id="authentic-volumetric-avatars-from-a-phone-scan-paper">•AuthenticVolumetric Avatars from a Phone Scan <a href="https://drive.google.com/file/d/1i4NJKAggS82wqMamCJ1OHRGgViuyoY6R/view">paper</a></h5><h5 id="neural-head-avatars-from-monocular-rgb-videos-homepage">•NeuralHead Avatars from Monocular RGB Videos <a href="https://philgras.github.io/neural_head_avatars/neural_head_avatars.html">homepage</a></h5><h5 id="towards-metrical-reconstruction-of-human-faces-homepage">•TowardsMetrical Reconstruction of Human Faces <a href="https://zielon.github.io/mica/">homepage</a></h5><h3 id="d-human-hand">3D human hand</h3><h5 id="active-learning-for-bayesian-3d-hand-pose-estimation-paper-code">•Active Learning for Bayesian 3D Hand Pose Estimation <a href="https://arxiv.org/pdf/2010.00694.pdf">paper</a> <a href="https://github.com/razvancaramalau/al_bhpe">code</a></h5><h5 id="multi-view-consistency-loss-for-improved-single-image-3d-reconstruction-of-clothed-people-paper-code-1">•Multi-View Consistency Loss for Improved Single-Image 3D Reconstructionof Clothed People <a href="https://akincaliskan3d.github.io/MV3DH//resources/ACCV_Cam_Ready_Multi_View_3D_Human.pdf">paper</a><a href="https://github.com/akcalakcal/Multi_View_Consistent_Single_Image_3D_Human_Reconstruction">code</a></h5><h5 id="eventhands-real-time-neural-3d-hand-reconstruction-from-an-event-stream">•EventHands: Real-Time Neural 3D Hand Reconstruction from an EventStream</h5><h5 id="monocular-real-time-full-body-capture-with-inter-part-correlations">•Monocular Real-time Full Body Capture with Inter-part Correlations</h5><h5 id="im2mesh-gan-accurate-3d-hand-mesh-recovery-from-a-single-rgb-image">•Im2Mesh GAN: Accurate 3D Hand Mesh Recovery from a Single RGB Image</h5><h5 id="handtailor-towards-high-precision-monocular-3d-hand-recovery-paper-code">•HandTailor: Towards High-Precision Monocular 3D Hand Recovery <a href="https://arxiv.org/pdf/2102.09244v1.pdf">paper</a> <a href="https://github.com/LyuJ1998/HandTailor">code</a></h5><h5 id="camera-space-hand-mesh-recovery-via-semantic-aggregation-and-adaptive-2d-1d-registration-paper-code">•Camera-Space Hand Mesh Recovery via Semantic Aggregation and Adaptive2D-1D Registration <a href="https://arxiv.org/pdf/2103.02845v1.pdf">paper</a> <a href="https://github.com/SeanChenxy/HandMesh">code</a></h5><h5 id="model-based-3d-hand-reconstruction-via-self-supervised-learning-paper-code">•Model-based 3D Hand Reconstruction via Self-Supervised Learning <a href="https://arxiv.org/pdf/2103.11703v1.pdf">paper</a> <a href="https://github.com/TerenceCYJ/S2HAND">code</a></h5><h5 id="action-conditioned-3d-human-motion-synthesis-with-transformer-vae-paper">•Action-Conditioned 3D Human Motion Synthesis with Transformer VAE <a href="https://arxiv.org/pdf/2104.05670.pdf">paper</a></h5><h5 id="semi-supervised-3d-hand-object-poses-estimation-with-interactions-in-time-paper">•Semi-Supervised 3D Hand-Object Poses Estimation with Interactions inTime <a href="https://arxiv.org/pdf/2106.05266v1.pdf">paper</a></h5><h5 id="rgb2hands-real-time-tracking-of-3d-hand-interactions-from-monocular-rgb-video-paper">•RGB2Hands: Real-Time Tracking of 3D Hand Interactions from Monocular RGBVideo <a href="https://arxiv.org/pdf/2106.11589v1.pdf">paper</a></h5><h5 id="artiboost-boosting-articulated-3d-hand-object-pose-estimation-via-online-exploration-and-synthesis-paper-code">•ArtiBoost: Boosting Articulated 3D Hand-Object Pose Estimation viaOnline Exploration and Synthesis <a href="https://arxiv.org/pdf/2109.05488v1.pdf">paper</a> <a href="https://github.com/MVIG-SJTU/ArtiBoost">code</a></h5><h5 id="monocular-3d-reconstruction-of-interacting-hands-via-collision-aware-factorized-refinements-paper-code">•Monocular 3D Reconstruction of Interacting Hands via Collision-AwareFactorized Refinements <a href="https://arxiv.org/pdf/2111.00763v1.pdf">paper</a> <a href="https://penincillin.github.io/ihmr_3dv2021">code</a></h5><h5 id="dynamic-iterative-refinement-for-efficient-3d-hand-pose-estimation-paper">•Dynamic Iterative Refinement for Efficient 3D Hand Pose Estimation <a href="https://arxiv.org/pdf/2111.06500v1.pdf">paper</a></h5><h5 id="semi-supervised-3d-hand-shape-and-pose-estimation-with-label-propagation-paper">•Semi-Supervised 3D Hand Shape and Pose Estimation with Label Propagation<a href="https://arxiv.org/pdf/2111.15199v1.pdf">paper</a></h5><h5 id="mobrecon-mobile-friendly-hand-mesh-reconstruction-from-monocular-image-paper-code">•MobRecon: Mobile-Friendly Hand Mesh Reconstruction from Monocular Image<a href="https://arxiv.org/pdf/2112.02753v1.pdf">paper</a> <a href="https://github.com/SeanChenxy/HandMesh">code</a></h5><h5 id="consistent-3d-hand-reconstruction-in-video-via-self-supervised-learning-paper">•Consistent 3D Hand Reconstruction in Video via Self-Supervised Learning<a href="https://arxiv.org/pdf/2201.09548v1.pdf">paper</a></h5><h5 id="interacting-attention-graph-for-single-image-two-hand-reconstruction-paper">•Interacting Attention Graph for Single Image Two-Hand Reconstruction <a href="https://arxiv.org/pdf/2203.09364v1.pdf">paper</a></h5><h5 id="handoccnet-occlusion-robust-3d-hand-mesh-estimation-network-paper-code">•HandOccNet: Occlusion-Robust 3D Hand Mesh Estimation Network <a href="https://arxiv.org/abs/2203.14564">paper</a> <a href="https://github.com/namepllet/HandOccNet">code</a></h5><h5 id="toch-spatio-temporal-object-correspondence-to-hand-for-motion-refinement-paper">•TOCH: Spatio-Temporal Object Correspondence to Hand for MotionRefinement <a href="https://arxiv.org/pdf/2205.07982v1.pdf">paper</a></h5><h5 id="end-to-end-3d-hand-pose-estimation-from-stereo-cameras-paper">•End-to-End 3D Hand Pose Estimation from Stereo Cameras <a href="https://arxiv.org/pdf/2206.01384v1.pdf">paper</a></h5><h5 id="efficient-annotation-and-learning-for-3dhand-pose-estimation-a-survey-paper">•Efficient Annotation and Learning for 3DHand Pose Estimation: A Survey<a href="https://arxiv.org/pdf/2206.02257v1.pdf">paper</a></h5><h5 id="d-interacting-hand-pose-estimation-by-hand-de-occlusion-and-removal-code">•3D Interacting Hand Pose Estimation by Hand De-occlusion and Removal <a href="https://github.com/MengHao666/HDR">code</a></h5><h3 id="d-hair">3d hair</h3><h5 id="neuralhdhair-automatic-high-fidelity-hair-modeling-from-a-single-image-using-implicit-neural-representations-paper">•NeuralHDHair: Automatic High-fidelity Hair Modeling from a Single ImageUsing Implicit Neural Representations <a href="https://arxiv.org/pdf/2205.04175v1.pdf">paper</a></h5><h5 id="d-hair-synthesis-using-volumetric-variational-autoencoders">•3Dhair synthesis using volumetric variational autoencoders</h5><h5 id="ao-cnn-filament-aware-hair-reconstruction-based-on-volumetric-vector-fields">•AO-CNN:filament-aware hair reconstruction based on volumetric vectorfields</h5><h5 id="neural-strands-learning-hair-geometry-and-appearance-from-multi-view-images-paper">•NeuralStrands: Learning Hair Geometry and Appearance from Multi-View Images <a href="https://arxiv.org/abs/2207.14067">paper</a></h5><h3 id="d-teeth">3d teeth</h3><h5 id="model-based-teeth-reconstruction-paper">• Model-based teethreconstruction <a href="https://vcai.mpi-inf.mpg.de/projects/MZ/Papers/SGASIA2016_TR/page.html">paper</a></h5><h3 id="d-eyelids">3d eyelids</h3><h5 id="real-time-3d-eyelids-tracking-from-semantic-edges-paper">•Real-time 3D Eyelids Tracking from Semantic Edges <a href="http://xufeng.site/publications/2017/2017_Real-time%203D%20Eyelids%20Tracking%20from%20Semantic%20Edges-min.pdf">paper</a></h5><h2 id="related">related</h2><h3 id="human-mattting">human mattting</h3><h5 id="real-time-high-resolution-background-matting-code">• Real-TimeHigh-Resolution Background Matting <a href="https://github.com/PeterL1n/BackgroundMattingV2">code</a></h5><h5 id="real-time-monocular-human-depth-estimation-and-segmentation-on-embedded-systems-paper">•Real-Time Monocular Human Depth Estimation and Segmentation on EmbeddedSystems <a href="https://arxiv.org/pdf/2108.10506v1.pdf">paper</a></h5><h5 id="deepsportlab-a-unified-framework-for-ball-detection-player-instance-segmentation-and-pose-estimation-in-team-sports-scenes-paper">•DeepSportLab: a Unified Framework for Ball Detection, Player InstanceSegmentation and Pose Estimation in Team Sports Scenes <a href="https://arxiv.org/pdf/2112.00627v1.pdf">paper</a></h5><h5 id="pp-humanseg-connectivity-aware-portrait-segmentation-with-a-large-scale-teleconferencing-video-dataset-paper-code">•PP-HumanSeg: Connectivity-Aware Portrait Segmentation with a Large-ScaleTeleconferencing Video Dataset <a href="https://arxiv.org/pdf/2112.07146v1.pdf">paper</a> <a href="https://github.com/PaddlePaddle/PaddleSeg">code</a></h5><h5 id="portrait-segmentation-using-deep-learning-paper">• PortraitSegmentation Using Deep Learning <a href="https://arxiv.org/pdf/2202.02705v1.pdf">paper</a></h5><h5 id="human-instance-matting-via-mutual-guidance-and-multi-instance-refinement-paper-code">•Human Instance Matting via Mutual Guidance and Multi-Instance Refinement<a href="https://arxiv.org/pdf/2205.10767v1.pdf">paper</a> <a href="https://github.com/nowsyn/InstMatt">code</a></h5><h3 id="pose-estimation">pose estimation</h3><h5 id="canonpose-self-supervised-monocular-3d-human-pose-estimation-in-the-wild-paper">•CanonPose: Self-Supervised Monocular 3D Human Pose Estimation in theWild <a href="https://arxiv.org/pdf/2011.14679.pdf">paper</a></h5><h5 id="active-learning-for-bayesian-3d-hand-pose-estimation-paper-code-1">•Active Learning for Bayesian 3D Hand Pose Estimation <a href="https://arxiv.org/pdf/2010.00694v2.pdf">paper</a> <a href="https://github.com/razvancaramalau/al_bhpe">code</a></h5><h5 id="a-nerf-surface-free-human-3d-pose-refinement-via-neural-rendering">•A-NeRF: Surface-free Human 3D Pose Refinement via Neural Rendering</h5><h5 id="handsformer-keypoint-transformer-for-monocular-3d-pose-estimation-of-hands-and-object-in-interaction-paper">•HandsFormer: Keypoint Transformer for Monocular 3D Pose Estimation ofHands and Object in Interaction <a href="https://arxiv.org/pdf/2104.14639v1.pdf">paper</a></h5><h5 id="humor-3d-human-motion-model-for-robust-pose-estimation-paper">•HuMoR: 3D Human Motion Model for Robust Pose Estimation <a href="https://geometry.stanford.edu/projects/humor/">paper</a></h5><h5 id="multi-person-extreme-motion-prediction-with-cross-interaction-attention-paper">•Multi-Person Extreme Motion Prediction with Cross-Interaction Attention<a href="https://arxiv.org/abs/2105.08825">paper</a></h5><h5 id="voxeltrack-multi-person-3d-human-pose-estimation-and-tracking-in-the-wild-paper">•VoxelTrack: Multi-Person 3D Human Pose Estimation and Tracking in theWild <a href="https://arxiv.org/pdf/2108.02452v1.pdf">paper</a></h5><h5 id="gravity-aware-monocular-3d-human-object-reconstruction-papercode">•Gravity-Aware Monocular 3D Human-Object Reconstruction <a href="http://4dqv.mpi-inf.mpg.de/GraviCap/">paper&amp;code</a></h5><h5 id="densepose-3d-lifting-canonical-surface-maps-of-articulated-objects-to-the-third-dimension-paper">•DensePose 3D: Lifting Canonical Surface Maps of Articulated Objects tothe Third Dimension <a href="https://arxiv.org/pdf/2109.00033v1.pdf">paper</a></h5><h5 id="graph-based-3d-multi-person-pose-estimation-using-multi-view-images-paper">•Graph-Based 3D Multi-Person Pose Estimation Using Multi-View Images <a href="https://arxiv.org/pdf/2109.05885v1.pdf">paper</a></h5><h5 id="learning-dynamical-human-joint-affinity-for-3d-pose-estimation-in-videos-paper">•Learning Dynamical Human-Joint Affinity for 3D Pose Estimation in Videos<a href="https://arxiv.org/pdf/2109.07353v1.pdf">paper</a></h5><h5 id="physics-based-human-motion-estimation-and-synthesis-from-videos-paper">•Physics-based Human Motion Estimation and Synthesis from Videos <a href="https://arxiv.org/pdf/2109.09913.pdf">paper</a></h5><h5 id="real-time-low-cost-multi-person-3d-pose-estimation-paper">•Real-time, low-cost multi-person 3D pose estimation <a href="https://arxiv.org/pdf/2110.11414v1.pdf">paper</a></h5><h5 id="direct-multi-view-multi-person-3d-human-pose-estimation-paper-code">•Direct Multi-view Multi-person 3D Human Pose Estimation <a href="https://arxiv.org/pdf/2111.04076.pdf">paper</a> <a href="https://github.com/sail-sg/mvp">code</a></h5><h5 id="rethinking-keypoint-representations-modeling-keypoints-and-poses-as-objects-for-multi-person-human-pose-estimation-code">•Rethinking Keypoint Representations: Modeling Keypoints and Poses asObjects for Multi-Person Human Pose Estimation <a href="https://github.com/wmcnally/kapao">code</a></h5><h5 id="camera-distortion-aware-3d-human-pose-estimation-in-video-with-optimization-based-meta-learning-paper">•Camera Distortion-aware 3D Human Pose Estimation in Video withOptimization-based Meta-Learning <a href="https://arxiv.org/pdf/2111.15056v1.pdf">paper</a></h5><h5 id="in-bed-human-pose-estimation-from-unseen-and-privacy-preserving-image-domains-paper">•In-Bed Human Pose Estimation from Unseen and Privacy-Preserving ImageDomains <a href="https://arxiv.org/pdf/2111.15124v1.pdf">paper</a></h5><h5 id="camera-motion-agnostic-3d-human-pose-estimation-paper-code">•Camera Motion Agnostic 3D Human Pose Estimation <a href="https://github.com/seonghyunkim1212/GMR">paper</a> <a href="https://arxiv.org/pdf/2112.00343v1.pdf">code</a></h5><h5 id="elepose-unsupervised-3d-human-pose-estimation-by-predicting-camera-elevation-and-learning-normalizing-flows-on-2d-poses-paper">•ElePose: Unsupervised 3D Human Pose Estimation by Predicting CameraElevation and Learning Normalizing Flows on 2D Poses <a href="https://arxiv.org/pdf/2112.07088v1.pdf">paper</a></h5><h5 id="multi-modal-3d-human-pose-estimation-with-2d-weak-supervision-in-autonomous-driving-paper">•Multi-modal 3D Human Pose Estimation with 2D Weak Supervision inAutonomous Driving <a href="https://arxiv.org/pdf/2112.12141v1.pdf">paper</a></h5><h5 id="adaptpose-cross-dataset-adaptation-for-3d-human-pose-estimation-by-learnable-motion-generation-paper">•AdaptPose: Cross-Dataset Adaptation for 3D Human Pose Estimation byLearnable Motion Generation <a href="https://arxiv.org/pdf/2112.11593v1.pdf">paper</a></h5><h5 id="flag-flow-based-3d-avatar-generation-from-sparse-observations-paper">•FLAG: Flow-based 3D Avatar Generation from Sparse Observations <a href="https://arxiv.org/pdf/2203.05789v1.pdf">paper</a></h5><h5 id="pose-mum-reinforcing-key-points-relationship-for-semi-supervised-human-pose-estimation-paper">•Pose-MUM : Reinforcing Key Points Relationship for Semi-Supervised HumanPose Estimation <a href>paper</a></h5><h5 id="distribution-aware-single-stage-models-for-multi-person-3d-pose-estimation-paper">•Distribution-Aware Single-Stage Models for Multi-Person 3D PoseEstimation <a href="https://arxiv.org/pdf/2203.07697v1.pdf">paper</a></h5><h5 id="p-stmo-pre-trained-spatial-temporal-many-to-one-model-for-3d-human-pose-estimation-paper-code">•P-STMO: Pre-Trained Spatial Temporal Many-to-One Model for 3D Human PoseEstimation <a href="https://arxiv.org/pdf/2203.07628v1.pdf">paper</a> <a href="https://github.com/paTRICK-swk/P-STMO">code</a></h5><h5 id="posepipe-open-source-human-pose-estimation-pipeline-for-clinical-research-paper-code">•PosePipe: Open-Source Human Pose Estimation Pipeline for ClinicalResearch <a href="https://arxiv.org/pdf/2203.08792v1.pdf">paper</a> <a href="https://github.com/peabody124/PosePipeline/">code</a></h5><h5 id="d-human-pose-estimation-using-möbius-graph-convolutional-networks-paper">•3D Human Pose Estimation Using Möbius Graph Convolutional Networks <a href="https://arxiv.org/pdf/2203.10554v1.pdf">paper</a></h5><h5 id="ray3d-ray-based-3d-human-pose-estimation-for-monocular-absolute-3d-localization-paper-code">•Ray3D: ray-based 3D human pose estimation for monocular absolute 3Dlocalization <a href="https://arxiv.org/pdf/2203.11471v1.pdf">paper</a><a href="https://github.com/YxZhxn/Ray3D">code</a></h5><h5 id="yolo-pose-enhancing-yolo-for-multi-person-pose-estimation-using-object-keypoint-similarity-loss-paper-code">•YOLO-Pose: Enhancing YOLO for Multi Person Pose Estimation Using ObjectKeypoint Similarity Loss <a href="https://arxiv.org/pdf/2204.06806v1.pdf">paper</a> <a href="https://github.com/TexasInstruments/edgeai-yolov5">code</a></h5><h5 id="permutation-invariant-relational-network-for-multi-person-3d-pose-estimation-paper">•Permutation-Invariant Relational Network for Multi-person 3D PoseEstimation <a href="https://arxiv.org/pdf/2204.04913v1.pdf">paper</a></h5><h5 id="non-local-latent-relation-distillation-for-self-adaptive-3d-human-pose-estimation-paper">•Non-Local Latent Relation Distillation for Self-Adaptive 3D Human PoseEstimation <a href="https://arxiv.org/pdf/2204.01971v1.pdf">paper</a></h5><h5 id="aligning-silhouette-topology-for-self-adaptive-3d-human-pose-recovery-paper">•Aligning Silhouette Topology for Self-Adaptive 3D Human Pose Recovery <a href="https://arxiv.org/pdf/2204.01276v1.pdf">paper</a></h5><h5 id="dite-hrnet-dynamic-lightweight-high-resolution-network-for-human-pose-estimation-paper">•Dite-HRNet: Dynamic Lightweight High-Resolution Network for Human PoseEstimation <a href="https://arxiv.org/pdf/2204.10762v1.pdf">paper</a></h5><h5 id="pedrecnet-multi-task-deep-neural-network-for-full-3d-human-pose-and-orientation-estimation-paper">•PedRecNet: Multi-task deep neural network for full 3D human pose andorientation estimation <a href="https://arxiv.org/pdf/2204.11548v1.pdf">paper</a></h5><h5 id="teaching-independent-parts-separately-tips-gan-improving-accuracy-and-stability-in-unsupervised-adversarial-2d-to-3d-human-pose-estimation-paper">•"Teaching Independent Parts Separately" (TIPS-GAN) : Improving Accuracyand Stability in Unsupervised Adversarial 2D to 3D Human Pose Estimation<a href="https://arxiv.org/pdf/2205.05980v1.pdf">paper</a></h5><h5 id="lightweight-human-pose-estimation-using-heatmap-weighting-loss-paper">•LightweightHuman Pose Estimation Using Heatmap-Weighting Loss <a href="https://arxiv.org/pdf/2205.10611v1.pdf">paper</a></h5><h5 id="vtp-volumetric-transformer-for-multi-view-multi-person-3d-pose-estimation-paper">•VTP:Volumetric Transformer for Multi-view Multi-person 3D Pose Estimation <a href="https://arxiv.org/pdf/2205.12602v1.pdf">paper</a></h5><h5 id="location-free-human-pose-estimation-paper">•Location-free HumanPose Estimation <a href="https://arxiv.org/pdf/2205.12619v1.pdf">paper</a></h5><h5 id="trajectory-optimization-for-physics-based-reconstruction-of-3d-human-pose-from-monocular-video-paper">•TrajectoryOptimization for Physics-Based Reconstruction of 3d Human Pose fromMonocular Video <a href="https://arxiv.org/pdf/2205.12292v1.pdf">paper</a></h5><h5 id="spgnet-spatial-projection-guided-3d-human-pose-estimation-in-low-dimensional-space-paper">•SPGNet:Spatial Projection Guided 3D Human Pose Estimation in Low DimensionalSpace <a href="https://arxiv.org/pdf/2206.01867v1.pdf">paper</a></h5><h5 id="graphmlp-a-graph-mlp-like-architecture-for-3d-human-pose-estimation-paper">•GraphMLP:A Graph MLP-Like Architecture for 3D Human Pose Estimation <a href="https://arxiv.org/pdf/2206.06420v1.pdf">paper</a></h5><h5 id="blazepose-ghum-holistic-real-time-3d-human-landmarks-and-pose-estimation-paper">•BlazePoseGHUM Holistic: Real-time 3D Human Landmarks and Pose Estimation <a href="https://arxiv.org/pdf/2206.11678v1.pdf">paper</a></h5><h5 id="mutual-adaptive-reasoning-for-monocular-3d-multi-person-pose-estimation-paper">•MutualAdaptive Reasoning for Monocular 3D Multi-Person Pose Estimation <a href="https://arxiv.org/pdf/2207.07900v1.pdf">paper</a></h5><h5 id="human-keypoint-detection-for-close-proximity-human-robot-interaction-paper">•Humankeypoint detection for close proximity human-robot interaction <a href="https://arxiv.org/pdf/2207.07742v1.pdf">paper</a></h5><h5 id="virtualpose-learning-generalizable-3d-human-pose-models-from-virtual-data-paper">•VirtualPose:Learning Generalizable 3D Human Pose Models from Virtual Data <a href="https://arxiv.org/pdf/2207.09949v1.pdf">paper</a></h5><h5 id="d-clothed-human-reconstruction-in-the-wild-paper-code">•3DClothed Human Reconstruction in the Wild <a href="https://arxiv.org/pdf/2207.10053v1.pdf">paper</a> <a href="https://github.com/hygenie1228/ClothWild_RELEASE">code</a></h5><h5 id="efficient-and-accurate-skeleton-based-two-person-interaction-recognition-using-inter--and-intra-body-graphs-paper">•EFFICIENTAND ACCURATE SKELETON-BASED TWO-PERSON INTERACTION RECOGNITION USINGINTER- AND INTRA-BODY GRAPHS <a href="https://arxiv.org/pdf/2207.12648v1.pdf">paper</a></h5><h3 id="registration">registration</h3><h5 id="loopreg-self-supervised-learning-of-implicit-surface-correspondences-pose-and-shape-for-3d-human-mesh-registration-paper-code">•LoopReg: Self-supervised Learning of Implicit Surface Correspondences,Pose and Shape for 3D Human Mesh Registration <a href="https://virtualhumans.mpi-inf.mpg.de/papers/bhatnagar2020loopreg/bhatnagar2020loopreg.pdf">paper</a><a href="https://github.com/bharat-b7/LoopReg">code</a></h5><h5 id="neural-deformation-graphs-for-globally-consistent-non-rigid-reconstruction-paper-code">•Neural Deformation Graphs for Globally-consistent Non-rigidReconstruction <a href="https://arxiv.org/pdf/2012.01451.pdf">paper</a><a href="https://github.com/AljazBozic/NeuralGraph">code</a></h5><h5 id="farm-functional-automatic-registration-method-for-3d-human-bodies-paper-code">•FARM: Functional Automatic Registration Method for 3D Human Bodies <a href="https://arxiv.org/abs/1807.10517">paper</a> <a href="https://github.com/riccardomarin/FARM">code</a></h5><h5 id="locally-aware-piecewise-transformation-fields-for-3d-human-mesh-registration-paper">•Locally Aware Piecewise Transformation Fields for 3D Human MeshRegistration <a href="https://arxiv.org/pdf/2104.08160v1.pdf">paper</a></h5><h5 id="unsupervised-3d-human-mesh-recovery-from-noisy-point-clouds-paper">•Unsupervised 3D Human Mesh Recovery from Noisy Point Clouds <a href="https://arxiv.org/pdf/2107.07539v1.pdf">paper</a></h5><h3 id="correspondence">correspondence</h3><h5 id="humangps-geodesic-preserving-feature-for-dense-human-correspondences-paper">•HumanGPS: Geodesic PreServing Feature for Dense Human Correspondences <a href="https://feitongt.github.io/HumanGPS/paper.pdf">paper</a></h5><h5 id="bodymap-learning-full-body-dense-correspondence-map-paper">•BodyMap:Learning Full-Body Dense Correspondence Map <a href="https://arxiv.org/pdf/2205.09111v1.pdf">paper</a></h5><h5 id="corri2p-deep-image-to-point-cloud-registration-via-dense-correspondence-paper">•CorrI2P:Deep Image-to-Point Cloud Registration via Dense Correspondence <a href="https://arxiv.org/pdf/2207.05483v1.pdf">paper</a></h5><h3 id="application">application</h3><h5 id="one-shot-free-view-neural-talking-head-synthesis-for-video-conferencing-paper">•One-Shot Free-View Neural Talking-Head Synthesis for Video Conferencing<a href="https://arxiv.org/pdf/2011.15126.pdf">paper</a></h5><h5 id="lipsync3d-data-efficient-learning-of-personalized-3d-talking-faces-from-video-using-pose-and-lighting-normalization-paper">•LipSync3D: Data-Efficient Learning of Personalized 3D Talking Faces fromVideo using Pose and Lighting Normalization <a href="https://arxiv.org/pdf/2106.04185v1.pdf">paper</a></h5><h5 id="arshoe-real-time-augmented-reality-shoe-try-on-system-on-smartphones-paper">•ARShoe: Real-Time Augmented Reality Shoe Try-on System on Smartphones <a href="https://arxiv.org/pdf/2108.10515v1.pdf">paper</a></h5><h5 id="a-neural-anthropometer-learning-from-body-dimensions-computed-on-human-3d-meshes-paper">•A Neural Anthropometer Learning from Body Dimensions Computed on Human3D Meshes <a href="https://arxiv.org/pdf/2110.04064v1.pdf">paper</a></h5><h5 id="robust-3d-garment-digitization-from-monocular-2d-images-for-3d-virtual-try-on-systems-paper">•Robust 3D Garment Digitization from Monocular 2D Images for 3D VirtualTry-On Systems <a href="https://arxiv.org/pdf/2111.15140v1.pdf">paper</a></h5><h5 id="single-image-human-body-reshaping-with-deep-neural-networks-paper">•Single-image Human-body Reshaping with Deep Neural Networks <a href="https://arxiv.org/pdf/2203.10496v1.pdf">paper</a></h5><h5 id="style-based-global-appearance-flow-for-virtual-try-on-paper">•Style-Based Global Appearance Flow for Virtual Try-On <a href="https://arxiv.org/pdf/2204.01046v1.pdf">paper</a></h5><h5 id="monitoring-of-pigmented-skin-lesions-using-3d-whole-body-imaging-paper">•Monitoring of Pigmented Skin Lesions Using 3D Whole Body Imaging <a href="https://arxiv.org/pdf/2205.07085v1.pdf">paper</a></h5><h5 id="estimation-of-3d-body-shape-and-clothing-measurements-from-frontaland-side-view-images-paper">•ESTIMATION OF 3D BODY SHAPE AND CLOTHING MEASUREMENTS FROM FRONTALANDSIDE-VIEW IMAGES <a href="https://arxiv.org/pdf/2205.14347v1.pdf">paper</a></h5><h5 id="dressing-avatars-deep-photorealistic-appearance-for-physically-simulated-clothing-paper">•Dressing Avatars: Deep Photorealistic Appearance for PhysicallySimulated Clothing <a href="https://arxiv.org/pdf/2206.15470.pdf">paper</a></h5><h5 id="aifit-automatic-3d-human-interpretable-feedback-models-for-fitness-training-paper">•AIFit: Automatic 3D Human-Interpretable Feedback Models for FitnessTraining <a href="http://vision.imar.ro/fit3d/">paper</a></h5><h3 id="texture">texture</h3><h5 id="spatiotemporal-texture-reconstruction-for-dynamic-objects-using-a-single-rgb-d-camera-paper">•Spatiotemporal Texture Reconstruction for Dynamic Objects Using a SingleRGB-D Camera <a href="https://arxiv.org/pdf/2108.09007v1.pdf">paper</a></h5><h5 id="semi-supervised-synthesis-of-high-resolution-editable-textures-for-3d-humans-paper">•Semi-supervised Synthesis of High-Resolution Editable Textures for 3DHumans <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Chaudhuri_Semi-Supervised_Synthesis_of_High-Resolution_Editable_Textures_for_3D_Humans_CVPR_2021_paper.pdf">paper</a></h5><h5 id="stylepeople-a-generative-model-of-fullbody-human-avatars-paper-code">•StylePeople: A Generative Model of Fullbody Human Avatars <a href="https://arxiv.org/pdf/2104.08363.pdf">paper</a> <a href="https://github.com/saic-vul/style-people">code</a></h5><h3 id="skin">skin</h3><h5 id="heterskinnet-a-heterogeneous-network-for-skin-weights-prediction-paper">•HeterSkinNet: A Heterogeneous Network for Skin Weights Prediction <a href="https://arxiv.org/pdf/2103.10602.pdf">paper</a></h5><h5 id="snarf-differentiable-forward-skinning-for-animating-non-rigid-neural-implicit-shapes-paper">•SNARF: Differentiable Forward Skinning for Animating Non-Rigid NeuralImplicit Shapes <a href="https://arxiv.org/pdf/2104.03953.pdf">paper</a></h5><h3 id="uncategorized">uncategorized</h3><h5 id="fully-convolutional-graph-neural-networks-for-parametric-virtual-try-on-paper">•Fully Convolutional Graph Neural Networks for Parametric Virtual Try-On<a href="https://arxiv.org/pdf/2009.04592.pdf">paper</a></h5><h5 id="tailornet-predicting-clothing-in-3d-as-a-function-of-human-pose-shape-and-garment-style-paper-code">•TailorNet: Predicting Clothing in 3D as a Function of Human Pose, Shapeand Garment Style <a href="https://arxiv.org/abs/2003.04583">paper</a><a href="https://github.com/chaitanya100100/TailorNet">code</a></h5><h5 id="dbooster-3d-body-shape-and-texture-recovery-paper">• 3DBooSTeR:3D Body Shape and Texture Recovery <a href="https://arxiv.org/pdf/2010.12670.pdf">paper</a></h5><h5 id="neural-3d-clothes-retargeting-from-a-single-image-paper">•Neural 3D Clothes Retargeting from a Single Image <a href="https://arxiv.org/pdf/2102.00062v1.pdf">paper</a></h5><h5 id="single-image-full-body-human-relighting-paper">• Single-imageFull-body Human Relighting <a href="https://arxiv.org/pdf/2107.07259.pdf">paper</a></h5><h5 id="ibutter-neural-interactive-bullet-time-generator-for-human-free-viewpoint-rendering-paper">•iButter: Neural Interactive Bullet Time Generator for HumanFree-viewpoint Rendering <a href="https://arxiv.org/pdf/2108.05577.pdf">paper</a></h5><h5 id="a-riemannian-framework-for-analysis-of-human-body-surface-paper">• ARiemannian Framework for Analysis of Human Body Surface <a href="https://arxiv.org/pdf/2108.11449v1.pdf">paper</a></h5><h5 id="the-power-of-points-for-modeling-humans-in-clothing-papercode">•The Power of Points for Modeling Humans in Clothing <a href="https://qianlim.github.io/POP.html">paper&amp;code</a></h5><h5 id="neural-human-deformation-transfer-paper">• Neural HumanDeformation Transfer <a href="https://arxiv.org/pdf/2109.01588v1.pdf">paper</a></h5><h5 id="d-human-texture-estimation-from-a-single-image-with-transformers-paper">•3D Human Texture Estimation from a Single Image with Transformers <a href="https://arxiv.org/pdf/2109.02563v1.pdf">paper</a></h5><h5 id="learning-to-predict-diverse-human-motions-from-a-single-image-via-mixture-density-networks-paper">•Learning to Predict Diverse Human Motions from a Single Image viaMixture Density Networks <a href="https://arxiv.org/pdf/2109.05776v1.pdf">paper</a></h5><h5 id="zflow-gated-appearance-flow-based-virtual-try-on-with-3d-priors-paper">•ZFlow: Gated Appearance Flow-based Virtual Try-on with 3D Priors <a href="https://arxiv.org/pdf/2109.07001v1.pdf">paper</a></h5><h5 id="a-shading-guided-generative-implicit-model-for-shape-accurate-3d-aware-image-synthesis-paper-code">•A Shading-Guided Generative Implicit Model for Shape-Accurate 3D-AwareImage Synthesis <a href="https://arxiv.org/abs/2110.15678">paper</a> <a href="https://github.com/XingangPan/ShadeGAN">code</a></h5><h5 id="action2video-generating-videos-of-human-3d-actions-paper">•Action2video: Generating Videos of Human 3D Actions <a href="https://arxiv.org/pdf/2111.06925v1.pdf">paper</a></h5><h5 id="garment4d-garment-reconstruction-from-point-cloud-sequences-paper-code">•Garment4D: Garment Reconstruction from Point Cloud Sequences <a href="https://papers.nips.cc/paper/2021/file/eb160de1de89d9058fcb0b968dbbbd68-Paper.pdf">paper</a><a href="https://github.com/hongfz16/Garment4D">code</a></h5><h5 id="adg-pose-automated-dataset-generation-for-real-world-human-pose-estimation-paper-code">•ADG-Pose: Automated Dataset Generation for Real-World Human PoseEstimation <a href="https://arxiv.org/pdf/2202.00753v1.pdf">paper</a> <a href="https://github.com/TeCSAR-UNCC/ADG-Pose">code</a></h5><h5 id="diffusionnet-discretization-agnostic-learning-on-surfaces-paper">•DiffusionNet: Discretization Agnostic Learning on Surfaces <a href="https://arxiv.org/pdf/2012.00888.pdf">paper</a></h5><h5 id="text-and-image-guided-3d-avatar-generation-and-manipulation-paper-code">•Text and Image Guided 3D Avatar Generation and Manipulation <a href="https://arxiv.org/pdf/2202.06079v1.pdf">paper</a> <a href="https://catlab-team.github.io/">code</a></h5><h5 id="quantification-of-occlusion-handling-capability-of-a-3d-human-pose-estimation-framework-paper">•Quantification of Occlusion Handling Capability of a 3D Human PoseEstimation Framework <a href="https://arxiv.org/pdf/2203.04113v1.pdf">paper</a></h5><h5 id="motron-multimodal-probabilistic-human-motion-forecasting-paper">•Motron: Multimodal Probabilistic Human Motion Forecasting <a href="https://arxiv.org/pdf/2203.04132v1.pdf">paper</a></h5><h5 id="fexgan-meta-facial-expression-generation-with-meta-humans-paper">•FEXGAN-META: FACIAL EXPRESSION GENERATION WITH META HUMANS <a href="https://arxiv.org/pdf/2203.05975v1.pdf">paper</a></h5><h5 id="actformer-a-gan-transformer-framework-towards-general-action-conditioned-3d-human-motion-generation">•ActFormer: A GAN Transformer Framework towards GeneralAction-Conditioned 3D Human Motion Generation</h5><h5 id="domain-adaptive-hand-keypoint-and-pixel-localization-in-the-wild-paper">•Domain Adaptive Hand Keypoint and Pixel Localization in the Wild <a href="https://arxiv.org/pdf/2203.08344v1.pdf">paper</a></h5><h5 id="portrait-eyeglasses-and-shadow-removal-by-leveraging-3d-synthetic-data-paper">•Portrait Eyeglasses and Shadow Removal by Leveraging 3D Synthetic Data<a href="https://arxiv.org/pdf/2203.10474v1.pdf">paper</a></h5><h5 id="recognition-of-freely-selected-keypoints-on-human-limbs-paper">•Recognition of Freely Selected Keypoints on Human Limbs <a href="https://arxiv.org/pdf/2204.06326v1.pdf">paper</a></h5><h5 id="whats-in-your-hands-3d-reconstruction-of-generic-objects-in-hands-paper-code">•What’s in your hands? 3D Reconstruction of Generic Objects in Hands <a href="https://arxiv.org/pdf/2204.07153v1.pdf">paper</a> <a href="https://github.com/JudyYe/ihoi">code</a></h5><h5 id="chore-contact-human-and-object-reconstruction-from-a-single-rgb-image-paper">•CHORE: Contact, Human and Object REconstruction from a single RGB image<a href="https://arxiv.org/pdf/2204.02445v1.pdf">paper</a></h5><h5 id="snug-self-supervised-neural-dynamic-garments-paper">• SNUG:Self-Supervised Neural Dynamic Garments <a href="https://arxiv.org/pdf/2204.02219v1.pdf">paper</a></h5><h5 id="d-magic-mirror-clothing-reconstruction-from-a-single-image-via-a-causal-perspective-paper">•3D Magic Mirror: Clothing Reconstruction from a Single Image via aCausal Perspective <a href="https://arxiv.org/pdf/2204.13096v1.pdf">paper</a></h5><h5 id="fake-it-till-you-make-it-face-analysis-in-the-wild-using-synthetic-data-alone-homepage">•Fake it till you make it: face analysis in the wild using synthetic dataalone <a href="https://microsoft.github.io/FaceSynthetics/">homepage</a></h5><h5 id="avatarclip-zero-shot-text-driven-generation-and-animation-of-3d-avatars-paper-code">•AvatarCLIP: Zero-Shot Text-Driven Generation and Animation of 3D Avatars<a href="https://arxiv.org/pdf/2205.08535v1.pdf">paper</a> <a href="https://github.com/hongfz16/AvatarCLIP">code</a></h5><h5 id="scene-aware-person-image-generation-through-global-contextual-conditioning-paper">•Scene Aware Person Image Generation through Global ContextualConditioning <a href="https://arxiv.org/pdf/2206.02717v1.pdf">paper</a></h5><h5 id="hairfit-pose-invariant-hairstyle-transfer-via-flow-based-hair-alignment-and-semantic-region-aware-inpainting-paper">•HairFIT: Pose-Invariant Hairstyle Transfer via Flow-based Hair Alignmentand Semantic-Region-Aware Inpainting <a href="https://arxiv.org/pdf/2206.08585v1.pdf">paper</a></h5><h5 id="from-a-few-accurate-2d-correspondences-to-3d-point-clouds-paper">•From a few Accurate 2D Correspondences to 3D Point Clouds <a href="https://arxiv.org/pdf/2206.08749v1.pdf">paper</a></h5><h5 id="convolutional-neural-network-based-partial-face-detection-paper">•Convolutional Neural Network Based Partial Face Detection <a href="https://arxiv.org/pdf/2206.14350v1.pdf">paper</a></h5><h5 id="spsn-superpixel-prototype-sampling-network-for-rgb-d-salient-object-detection-paper-code">•SPSN: Superpixel Prototype Sampling Network for RGB-D Salient ObjectDetection <a href="https://arxiv.org/pdf/2207.07898v1.pdf">paper</a> <a href="https://github.com/Hydragon516/SPSN">code</a></h5><h5 id="detecting-humans-in-rgb-d-data-with-cnns-paper">• DetectingHumans in RGB-D Data with CNNs <a href="https://arxiv.org/pdf/2207.08064v1.pdf">paper</a></h5><h5 id="animation-from-blur-multi-modal-blur-decomposition-with-motion-guidance-paper">•Animation from Blur: Multi-modal Blur Decomposition with Motion Guidance<a href="https://arxiv.org/pdf/2207.10123.pdf">paper</a></h5><h5 id="d-shape-sequence-of-human-comparison-and-classification-using-current-and-varifolds-paper-code">•3D Shape Sequence of Human Comparison and Classification using Currentand Varifolds <a href="https://arxiv.org/pdf/2207.12485v1.pdf">paper</a><a href="https://github.com/CRISTAL-3DSAM/HumanComparisonVarifolds">code</a></h5><h5 id="kinepose-a-temporally-optimized-inverse-kinematics-technique-for-6dof-human-pose-estimation-with-biomechanical-constraints-paper-code">•KinePose: A temporally optimized inverse kinematics technique for 6DOFhuman pose estimation with biomechanical constraints <a href="https://arxiv.org/pdf/2207.12841v1.pdf">paper</a> <a href="https://github.com/KevGildea/KinePose">code</a></h5><h5 id="skeleton-free-pose-transfer-for-stylized-3d-character-paper">•Skeleton-free Pose Transfer for Stylized 3D Character <a href="https://www.arxiv-vanity.com/papers/2208.00790/?continueFlag=acd9680585ca1db48ed3cbc277e4da97">paper</a></h5><h2 id="parametric-model">parametric model</h2><h3 id="body">body</h3><h5 id="smpl-a-skinned-multi-person-linear-model-paper-code">• SMPL: ASkinned Multi-Person Linear Model <a href="https://files.is.tue.mpg.de/black/papers/SMPL2015.pdf">paper</a><a href="https://github.com/CalciferZh/SMPL">code</a></h5><h5 id="expressive-body-capture-3d-hands-face-and-body-from-a-single-image-paper-code">•Expressive Body Capture: 3D Hands, Face, and Body from a Single Image <a href="https://arxiv.org/abs/1904.05866">paper</a> <a href="https://github.com/vchoutas/smplx">code</a></h5><h5 id="star-sparse-trained-articulated-human-body-regressor-paper-code">•STAR: Sparse Trained Articulated Human Body Regressor <a href="https://arxiv.org/abs/2008.08535">paper</a> <a href="https://github.com/ahmedosman/STAR">code</a></h5><h3 id="face">face</h3><h5 id="basel-face-model-2009-website">• Basel Face Model 2009 <a href="http://faces.cs.unibas.ch/bfm/?nav=1-0&amp;id=basel_face_model">website</a></h5><h5 id="basel-face-model-2017-website">• Basel Face Model 2017 <a href="http://faces.cs.unibas.ch/bfm/bfm2017.html">website</a></h5><h5 id="large-scale-3d-morphable-model-website">• Large Scale 3DMorphable Model <a href="https://xip.uclb.com/i/healthcare_tools/LSFM.html">website</a></h5><h3 id="head">head</h3><h5 id="flame-articulated-expressive-head-model-website">• FLAME:Articulated Expressive Head Model <a href="http://flame.is.tue.mpg.de/">website</a></h5><h3 id="hand">hand</h3><h5 id="mano-paper-website">• MANO <a href="https://ps.is.mpg.de/uploads_file/attachment/attachment/392/Embodied_Hands_SiggraphAsia2017.pdf">paper</a><a href="https://mano.is.tue.mpg.de/">website</a></h5><h5 id="nimble-a-non-rigid-hand-model-with-bones-and-muscles-paper">•NIMBLE: A Non-rigid Hand Model with Bones and Muscles <a href="https://arxiv.org/pdf/2202.04533v1.pdf">paper</a></h5><h3 id="method">method</h3><h5 id="ghum-ghuml-generative-3d-human-shape-and-articulated-pose-models-github">•GHUM&amp; GHUML: Generative 3D Human Shape and Articulated Pose Models <a href="https://github.com/google-research/google-research/tree/master/ghum">github</a></h5><h5 id="large-scale-3d-morphable-models-paper-code">•Large Scale 3DMorphable Models <a href="https://link.springer.com/article/10.1007/s11263-017-1009-7">paper</a><a href="https://github.com/menpo/lsfm">code</a></h5><h5 id="morphable-face-models---an-open-framework-paper-code">•MorphableFace Models - An Open Framework <a href="https://arxiv.org/abs/1709.08398">paper</a> <a href="https://github.com/unibas-gravis/basel-face-pipeline">code</a></h5><h2 id="dataset">dataset</h2><h3 id="face-1">face</h3><h5 id="maad-face-a-massively-annotated-attribute-dataset-for-face-images-paper">•MAAD-Face: A Massively Annotated Attribute Dataset for Face Images <a href="https://github.com/pterhoer/MAAD-Face">paper</a></h5><h5 id="facescape-3d-facial-dataset-and-benchmark-for-single-view-3d-face-reconstruction-paper">•FaceScape: 3D Facial Dataset and Benchmark for Single-View 3D FaceReconstruction <a href="https://arxiv.org/pdf/2111.01082v1.pdf">paper</a></h5><h5 id="w-lp-aflw2000-3d-homepage">• 300W-LP &amp; AFLW2000-3D <a href="http://www.cbsr.ia.ac.cn/users/xiangyuzhu/projects/3DDFA/main.htm">homepage</a></h5><h5 id="aflw-website">• AFLW <a href="https://www.tugraz.at/institute/icg/research/team-bischof/lrs/downloads/aflw/">website</a></h5><h5 id="realy-rethinking-the-evaluation-of-3d-face-reconstruction-paper-website">•REALY: Rethinking the Evaluation of 3D Face Reconstruction <a href="https://arxiv.org/pdf/2203.09729v1.pdf">paper</a> <a href="https://www.realy3dface.com/">website</a></h5><h5 id="faceverse-high-quality-3d-face-dataset-github">• FaceVerse-HighQuality 3D Face Dataset <a href="https://github.com/LizhenWangT/FaceVerse-Dataset">github</a></h5><h5 id="dad-3dheads-a-large-scale-dense-accurate-and-diverse-dataset-for-3d-head-alignment-from-a-single-image-paper-github">•DAD-3DHeads: A Large-scale Dense, Accurate and Diverse Dataset for 3DHead Alignment from a Single Image <a href="https://arxiv.org/pdf/2204.03688v1.pdf">paper</a> <a href="https://github.com/PinataFarms/DAD-3DHeads">github</a></h5><h5 id="multiface-a-dataset-for-neural-face-rendering-paper">•Multiface: A Dataset for Neural Face Rendering <a href="https://arxiv.org/pdf/2207.11243.pdf">paper</a></h5><h3 id="hand-1">hand</h3><h5 id="grab-a-dataset-of-whole-body-human-grasping-of-objects-github">•GRAB: A Dataset of Whole-Body Human Grasping of Objects <a href="https://github.com/otaheri/GRAB">github</a></h5><h5 id="reconstructing-hand-object-interactions-in-the-wild-github">•Reconstructing Hand-Object Interactions in the Wild <a href="https://github.com/ZheC/MOW">github</a></h5><h5 id="ego2handspose-a-dataset-for-egocentric-two-hand-3d-global-pose-estimation-paper">•Ego2HandsPose: A Dataset for Egocentric Two-hand 3D Global PoseEstimation <a href="https://arxiv.org/pdf/2206.04927v1.pdf">paper</a></h5><h5 id="interhand2.6m">• Interhand2.6M</h5><h3 id="body-1">body</h3><h5 id="ntu60-x-towards-skeleton-based-recognition-of-subtle-human-actions-code">•NTU60-X: TOWARDS SKELETON-BASED RECOGNITION OF SUBTLE HUMAN ACTIONS <a href="https://arxiv.org/pdf/2101.11529.pdf">code</a></h5><h5 id="agora-avatars-in-geography-optimized-for-regression-analysis-homepage">•AGORA: Avatars in Geography Optimized for Regression Analysis <a href="https://agora.is.tue.mpg.de/">homepage</a></h5><h5 id="amass-archive-of-motion-capture-as-surface-shapes-paper-github">•AMASS: Archive of Motion Capture as Surface Shapes <a href="https://arxiv.org/pdf/1904.03278.pdf">paper</a> <a href="https://github.com/nghorbani/amass">github</a></h5><h5 id="asl-skeleton3d-and-asl-phono-two-novel-datasets-for-the-american-sign-language-paper">•ASL-Skeleton3D and ASL-Phono: Two Novel Datasets for the American SignLanguage <a href="https://arxiv.org/pdf/2201.02065v1.pdf">paper</a></h5><h5 id="mpi-inf-3dhp-homepage">•MPI-INF-3DHP <a href="https://vcai.mpi-inf.mpg.de/3dhp-dataset/">homepage</a></h5><h5 id="human3.6m-website">•Human3.6M <a href="http://vision.imar.ro/human3.6m/description.php">website</a></h5><h5 id="dpw-website">•3DPW <a href="https://virtualhumans.mpi-inf.mpg.de/3DPW/">website</a></h5><h5 id="pennaction-website">•PennAction <a href="http://dreamdragon.github.io/PennAction/">website</a></h5><h5 id="insta-variety-github">•Insta Variety <a href="https://github.com/akanazawa/human_dynamics/blob/master/doc/insta_variety.md">github</a></h5><h5 id="posetrack-homapage">•PoseTrack <a href="https://posetrack.net/">homapage</a></h5><h5 id="kinetics-400-website">•Kinetics-400 <a href="https://deepmind.com/research/open-source/kinetics">website</a></h5><h5 id="renderpeople-website">•RenderPeople <a href="https://renderpeople.com/">website</a></h5><h5 id="buff-website">•BUFF <a href="https://buff.is.tue.mpg.de/">website</a></h5><h5 id="people-snapshot-dataset-homepage">•People Snapshot Dataset <a href="https://graphics.tu-bs.de/people-snapshot">homepage</a></h5><h5 id="multi-garment-homepage">•Multi-Garment <a href="https://virtualhumans.mpi-inf.mpg.de/mgn/">homepage</a></h5><h5 id="iper-website">•iPER <a href="https://svip-lab.github.io/dataset/iPER_dataset.html">website</a></h5><h5 id="zju-mocap-homepage">•ZJU-MoCap <a href="https://chingswy.github.io/Dataset-Demo/">homepage</a></h5><h5 id="smartportraits-depth-powered-handheld-smartphone-dataset-of-human-portraits-for-state-estimation-reconstruction-and-synthesis-paper">•SmartPortraits:Depth Powered Handheld Smartphone Dataset of Human Portraits for StateEstimation, Reconstruction and Synthesis <a href="https://arxiv.org/pdf/2204.10211v1.pdf">paper</a></h5><h5 id="mvp-human-dataset-for-3d-human-avatar-reconstruction-from-unconstrained-frames-paper">•MVP-HumanDataset for 3D Human Avatar Reconstruction from Unconstrained Frames <a href="https://arxiv.org/pdf/2204.11184v1.pdf">paper</a></h5><h5 id="humman-multi-modal-4d-human-dataset-for-versatile-sensing-and-modeling-paper">•HuMMan:Multi-Modal 4D Human Dataset for Versatile Sensing and Modeling <a href="https://arxiv.org/pdf/2204.13686v1.pdf">paper</a></h5><h5 id="dmpb-dataset-github">•3DMPB-dataset <a href="https://github.com/boycehbz/3DMPB-dataset">github</a></h5><h5 id="imar_vision_datasets_tools-github">•imar_vision_datasets_tools<a href="https://github.com/sminchisescu-research/imar_vision_datasets_tools">github</a></h5><h5 id="rich-real-scenes-interaction-contacts-and-humans-github">•RICH:Real scenes, Interaction, Contacts and Humans <a href="https://github.com/paulchhuang/rich_toolkit">github</a></h5><h3 id="method-1">method</h3><h5 id="neuralannot-neural-annotator-for-3d-human-mesh-training-sets-paper">•NeuralAnnot:Neural Annotator for 3D Human Mesh Training Sets <a href="https://arxiv.org/pdf/2011.11232.pdf">paper</a></h5><h3 id="others">others</h3><h5 id="k-hairstyle-a-large-scale-korean-hairstyle-dataset-for-virtual-hair-editing-and-hairstyle-classification-homepage">•K-Hairstyle: A Large-scale Korean hairstyle dataset for virtual hairediting and hairstyle classification <a href="https://www.arxiv-vanity.com/papers/2102.06288/">homepage</a></h5><h5 id="simulated-garment-dataset-for-virtual-try-on-address">•Simulated garment dataset for virtual try-on <a href="https://github.com/isantesteban/vto-dataset">address</a></h5><h5 id="deepfashion-homepage">• DeepFashion <a href="https://liuziwei7.github.io/projects/DeepFashion.html">homepage</a></h5><h5 id="delving-into-high-quality-synthetic-face-occlusion-segmentation-datasets-paper">•Delving into High-Quality Synthetic Face Occlusion Segmentation Datasets<a href="https://arxiv.org/pdf/2205.06218v1.pdf">paper</a></h5><h5 id="a-gan-facial-flow-for-face-animation-with-generative-adversarial-networks-paper">•3A-GAN: Facial Flow for Face Animation with Generative AdversarialNetworks <a href="https://arxiv.org/pdf/2205.06204v1.pdf">paper</a></h5><h5 id="scanned-objects-by-google-research-website">• Scanned Objects byGoogle Research <a href="https://ai.googleblog.com/2022/06/scanned-objects-by-google-research.html?continueFlag=69eb10990d1859f21dd21d22d96e2b22">website</a></h5><h2 id="labs">labs</h2><h5 id="max-planck-institute-website">• max planck institute <a href="https://ps.is.tuebingen.mpg.de/publications">website</a></h5><h5 id="yebin-liu-website">• Yebin Liu <a href="http://www.liuyebin.com/">website</a></h5><h5 id="zju3dv-github">• ZJU3DV <a href="https://github.com/zju3dv">github</a></h5><h5 id="hujun-bao-google-scholar">• Hujun Bao <a href="https://scholar.google.com/citations?hl=zh-CN&amp;user=AZCcDmsAAAAJ&amp;view_op=list_works&amp;sortby=pubdate">googlescholar</a></h5><h5 id="ustc-3dv-homepage">• USTC-3DV <a href="http://staff.ustc.edu.cn/~juyong/index.html">homepage</a></h5><h5 id="hao_li-homepage">• Hao_Li <a href="http://www.hao-li.com/Hao_Li/Hao_Li_-_about_me.html">homepage</a></h5><h2 id="other-related-awesome">other related awesome</h2><h5 id="awesome-clothed-human-github">• awesome-clothed-human <a href="https://github.com/weihaox/awesome-clothed-human">github</a></h5><h5 id="curated-list-of-awesome-3d-morphable-model-software-and-data-github">•curated-list-of-awesome-3D-Morphable-Model-software-and-data <a href="https://github.com/3d-morphable-models/curated-list-of-awesome-3D-Morphable-Model-software-and-data">github</a></h5><h5 id="awesome-hand-pose-estimation-github">•awesome-hand-pose-estimation <a href="https://github.com/xinghaochen/awesome-hand-pose-estimation">github</a></h5><h5 id="awesome-3d-body-papers-github">• Awesome 3D Body Papers <a href="https://github.com/3DFaceBody/awesome-3dbody-papers">github</a></h5><h5 id="body_reconstruction_references-github">•Body_Reconstruction_References <a href="https://github.com/chenweikai/Body_Reconstruction_References#data-and-code">github</a></h5><h5 id="d-face-reconstruction-paper-list-github">•3D-face-reconstruction-paper-list <a href="https://github.com/czh-98/3D-face-reconstruction-paper-list">github</a></h5><h5 id="awesome_talking_face_generation-github">•awesome_talking_face_generation <a href="https://github.com/YunjinPark/awesome_talking_face_generation">github</a></h5><h5 id="cg3dv-twitter-github">• CG&amp;3DV Twitter <a href="https://github.com/USTC3DV/Truck_of_Twitter_Messages">github</a></h5><h2 id="survey">survey</h2><h5 id="recovering-3d-human-mesh-from-monocular-images-a-survey-paper-github">•Recovering 3D Human Mesh from Monocular Images: A Survey <a href="https://arxiv.org/pdf/2203.01923v1.pdf">paper</a> <a href="https://github.com/tinatiansjz/hmr-survey">github</a></h5><h5 id="d-human-pose-estimation-a-survey-paper">• 2D Human PoseEstimation: A Survey <a href="https://arxiv.org/pdf/2204.07370v1.pdf">paper</a></h5><h5 id="a-survey-of-non-rigid-3d-registration-paper">• A Survey ofNon-Rigid 3D Registration <a href="https://arxiv.org/pdf/2203.07858v1.pdf">paper</a></h5><h5 id="d-face-reconstruction-in-deep-learning-era-a-survey-paper">• 3DFace Reconstruction in Deep Learning Era: A Survey <a href="https://link.springer.com/article/10.1007/s11831-021-09705-4">paper</a></h5><h5 id="towards-efficient-and-photorealistic-3d-human-reconstruction-a-brief-survey-paper">•Towards efficient and photorealistic 3D human reconstruction: A briefsurvey <a href="https://www.sciencedirect.com/science/article/pii/S2468502X21000413">paper</a></h5><h5 id="survey-on-3d-face-reconstruction-from-uncalibrated-images-paper">•Surveyon 3D face reconstruction from uncalibrated images <a href="https://arxiv.org/abs/2011.05740">paper</a></h5><h5 id="state-of-the-art-on-3d-reconstruction-with-rgb-d-cameras-paper">•Stateof the Art on 3D Reconstruction with RGB-D Cameras <a href="https://zollhoefer.com/papers/EG18_RecoSTAR/paper.pdf">paper</a></h5>]]></content>
      
      
      <categories>
          
          <category> Face Reconstruction </category>
          
          <category> Paper List </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>论文笔记--Self-Supervised 3D Face Reconstruction via Conditional Estimation</title>
      <link href="/2022/09/29/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0--Self-Supervised%203D%20Face%20Reconstruction%20via%20Conditional%20Estimation/"/>
      <url>/2022/09/29/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0--Self-Supervised%203D%20Face%20Reconstruction%20via%20Conditional%20Estimation/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><strong>摘要</strong></h2><p>本文提出一种条件估计的框架，利用自监督学习方式实现从2D单视角图片学习3D人脸参数，该方法简称CEST。CEST是一个综合分析过程，大体上看是从面部图片得到3D面部参数（形状，反射率，视角，光照），根据这些参数重建出2D图片。</p><p>在这个过程中，结合3D人脸参数之间的统计相关性来耦合各个参数估计值，具体来说就是某一项参数的估计不仅取决于源图像，也取决于前面已经预测推导出来的参数。此外，本文还利用视频帧之间的反射对称性和一致性来提高人脸参数的解耦。<span id="more"></span></p><h2 id="局限性"><strong>局限性</strong></h2><p>训练数据的稀缺；拥有ground-truth的数据集对神经网络的泛化能力提升有限；之前的工作对目标参数（形状，反射率，光照等）都是单独估计的，没有考虑这些参数之间的联系。</p><h2 id="创新点"><strong>创新点</strong></h2><ol type="1"><li>提出一种条件估计的网络框架，明确考虑各项三维人脸参数之间的相关性，多项参数（视角，形状，反射率，光照）是依次推导求得。</li><li>提出了一种随机优化策略，有效地将反射对称性和一致性约束纳入CEST。随着视频帧数的增加，CEST的计算复杂度呈线性增加，而不是二次增加。</li></ol><h2 id="方法"><strong>方法</strong></h2><h3 id="整体概览">整体概览</h3><p><img src="/2022/09/29/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0--Self-Supervised%203D%20Face%20Reconstruction%20via%20Conditional%20Estimation/pipeline.png"><img src="/2022/09/29/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0--Self-Supervised%203D%20Face%20Reconstruction%20via%20Conditional%20Estimation/simplified_pipeline.png" alt="整体框架"></p><h3 id="参数估计">参数估计</h3><p>与很多拟合3DMM得到重建人脸模型思路相仿，需要先得到形状<span class="math inline">\(\hat{S}\)</span>，视角点<span class="math inline">\(\hat{v}\)</span>，反射率<span class="math inline">\(\hat{R}\)</span>，光照<span class="math inline">\(\hat{\ell}\)</span>等参数，接着利用这些参数拟合人脸模型，最后通过渲染<span class="math inline">\(\hat{I}=\mathcal{R}(S,R,v,\ell)\)</span>得到重建2D人脸图片，整个过程都是一个端到端的预测方式。各项参数获取公式如下：</p><p><span class="math display">\[\hat{v}=f_v(I;\theta)\]</span><span class="math display">\[\hat{S}=f_s(I,\hat{v})\]</span><span class="math display">\[\hat{R}=f_r(I,\hat{v},\hat{S};\theta_r)\]</span><span class="math display">\[\hat{\ell}=f_{\ell}(I,\hat{v},\hat{S},\hat{R};\theta_{\ell})\]</span></p><p>各项参数含义分析：</p><p>视角<span class="math inline">\(v\)</span>：利用给定图像预测视角参数，包含7个参数，分别式3D空间旋转角度，平移坐标和缩放因子。</p><p>形状<span class="math inline">\(S\)</span>：形状的预测取决于给定的图像和预测视角参数，利用预测的视角v，我们可以将图像在二维平面上与标准的视图进行对齐，经过网络输出形状系数，形状系数一共有228个参数。</p><p>反射率<span class="math inline">\(R\)</span>：反射率的预测不仅取决于给定的图像，还取决于预测的视角和形状，本文采用uv贴图作为网络输入来预测反射率</p><p>光照<span class="math inline">\(\ell\)</span>：以给定的图像、被照亮的纹理图和反射率的UV图为输入，得到光照参数，光照参数包含9个参数。</p><h3 id="损失函数">损失函数</h3><p>通过一个人脸分割网络来识别像素是否属于人脸区域，用分割网络对人脸图片进行掩模处理，每张图片的像素都有一个掩码值<span class="math inline">\(M(i,j)\)</span>，若<span class="math inline">\(M(i,j)=1\)</span>则该像素包含在重建过程中，否则不包括。光学度损失可表示如下：</p><p><span class="math display">\[\mathcal{L}_{ph}=\varepsilon(I,S,R,v,\ell,M)=||M \otimes I - M \otimes\hat{I}||_1=||M \otimes I - M \otimes \hat{I}||_1\]</span></p><p><span class="math inline">\(||.||_1\)</span>表示测量L1范数，<span class="math inline">\(\otimes\)</span>表示element-wise乘法。</p><p>根据反射率对称性和一致性约束对上述的损失函数作了一定优化： <span class="math display">\[\mathcal{L}_{ph} =\frac{1}{N}\sum^{N}_{i=1,\xi_j=\xi_i}(\varepsilon(I_i,S_i,R_j,v_i,\ell_i,M_i)+\varepsilon(I_i,S_i,R^{\bowtie}_i,v_i,\ell_i,M_i))\]</span></p><p>还有一个根据人脸2D关键点构造的损失和一个形状系数正则化损失，分别如下：</p><p><span class="math display">\[\mathcal{L}_{kp} =\frac{1}{NN_{kp}}\sum^{N}_{i=1}\sum^{N_{kp}}_{j=1}||Q_i(k_j)-q_i(j)||_1\]</span><span class="math display">\[\mathcal{L}_{rg} = \frac{1}{N}\sum^{N}_{i=1}||\alpha_i||^2_2\]</span></p><p>整体损失函数 <span class="math display">\[\mathcal{L} = \mathcal{L}_{ph} + \lambda_1\mathcal{L}_{kp} +\lambda_2\mathcal{L}_{rg}\]</span></p>]]></content>
      
      
      <categories>
          
          <category> Face Reconstruction </category>
          
          <category> Paper Notes </category>
          
          <category> ICCV </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Face Reconstruction </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文笔记--Weakly-Supervised Multi-Face 3D Reconstruction</title>
      <link href="/2022/09/28/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0--Weakly-Supervised%20Multi-Face%203D%20Reconstruction/"/>
      <url>/2022/09/28/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0--Weakly-Supervised%20Multi-Face%203D%20Reconstruction/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要">摘要</h2><p>三维人脸重建在媒体应用领域扮演着重要的角色，其中包括数字娱乐，社交媒体，情感分析和人物辨识。利用单张图片估计人脸参数模型，首先需要检测面部区域和面部关键点坐标，然后裁剪出每个人脸区域，利用深度学习方法对该区域图片进行参数回归预测。</p><p>不同于常规方法（对每个检测人脸区域独立的预测），本文提出一种端到端的多个面部三维重建方法（Multi-FaceReconstruction），可以利用单个网络推理预测出多个人脸实例参数。与此同时，本文为每个图像中的人脸使用了全局相机模型，使得重建出来的头部位置和面部朝向都与原图相同。</p><p>本文提出的方法不仅可以减少特征提取过程中的计算冗余，而且网络模型也更加容易部署。<span id="more"></span></p><h2 id="基本介绍">基本介绍</h2><p>常规的方法都是将人脸校正到特定的帧图片上，首先利用检测器定位面部区域和面部坐标，然后对面部进行裁剪后输入到神经网络，整个的过程包含了面部检测和面部参数回归估计两项任务。</p><h2 id="局限性">局限性</h2><p>相比于前几年单目图片重建方法，很少有工作专注于单目重建多个面部。其次，进行多面部重建的方法，只重建了面部形状，而忽视了面部纹理，光照信息；而且这些方法大多是以监督学习的方式进行训练，需要大量的几何细节ground-truth。由于训练样本要么是通过渲染合成的，要么是从传统的基于优化的方法中获得的，因此它们通常对姿势和光照的变化不太鲁棒。</p><p>本文提出的方法是一张图片里多个人脸共享相同的相机参数，这意味着相机焦距和相机中心坐标在3D参数模型上是一致的。而常规的方法是一个人脸对应一个相机模型参数，忽视了在3D视角下每个个体的头部位置和朝向这些高阶语义信息。</p><h2 id="创新点">创新点</h2><ol type="1"><li>提出一个弱监督多面部重建网络框架，仅用一次前向传播就可以<strong>预测多个人脸实例</strong>的拟合3DMM模型参数</li><li>提出能够在3D场景中恢复相对头部位置和面部朝向的全局相机模型</li><li>阶段式训练方法，优化整体网络架构</li></ol><h2 id="方法">方法</h2><h3 id="计算耗时比较">计算耗时比较</h3><figure><img src="/2022/09/28/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0--Weakly-Supervised%20Multi-Face%203D%20Reconstruction/convention_method.png#pic_center" alt="常规方法"><figcaption aria-hidden="true">常规方法</figcaption></figure><figure><img src="/2022/09/28/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0--Weakly-Supervised%20Multi-Face%203D%20Reconstruction/proposed_method.png" alt="本文方法"><figcaption aria-hidden="true">本文方法</figcaption></figure><p><span class="math display">\[常规方法耗时：T_c = T_{Encoder} +n·(T_{Rectify}+T_{Encoder&#39;}+T_{Decoder})\]</span><span class="math display">\[本文方法耗时：T_p = T_{Encoder} + n·T_{Decoder} \]</span></p><h3 id="网络结构">网络结构</h3><figure><img src="/2022/09/28/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0--Weakly-Supervised%20Multi-Face%203D%20Reconstruction/encoder_decoder.png" alt="编码器解码器"><figcaption aria-hidden="true">编码器解码器</figcaption></figure><p><span class="math display">\[\delta=(\delta_m,\delta_{id},\delta_{exp},\delta_{alb},\delta_{pose},\delta_{illum})\]</span><span class="math display">\[\delta_{m}\in{\mathbb{R}^{1}}, \delta_{id}\in{\mathbb{R}^{80}},\delta_{exp}\in{\mathbb{R}^{64}}, \delta_{alb}\in{\mathbb{R}^{80}},\delta_{illum}\in{\mathbb{R}^{27}}\]</span></p><p>Encoder输出258个参数<span class="math inline">\(\delta\)</span>，包含有身份<span class="math inline">\(\delta_{id}\)</span>，表情<span class="math inline">\(\delta_{exp}\)</span>，反照率<span class="math inline">\(\delta_{alb}\)</span>，姿态<span class="math inline">\(\delta_{pose}\)</span>，光照<span class="math inline">\(\delta_{illum}\)</span>，图片每个人脸位置<span class="math inline">\(\delta_m\)</span></p><h3 id="dmm">3DMM</h3><p>3DMM作为Decoder，参数化方法合成人脸，人脸的形状，纹理表示如下：</p><p>备注：3表示x, y, z三个坐标值；N表示顶点数</p><p><span class="math display">\[ S = \bar{S} +\delta_{id}*S_{id}+\delta_{exp}*S_{exp}\]</span><span class="math display">\[A = \bar{A} + \delta_{alb}*A_{alb}\]</span><span class="math display">\[S\in{\mathbb{R}^{3 \times N}}, A\in{\mathbb{R}^{3 \times N},N=35709}\]</span></p><h3 id="相机模型">相机模型</h3><p>透视投影矩阵: <span class="math inline">\(f_g是焦距，w和h是图像的宽和高\)</span> <span class="math display">\[K = \begin{bmatrix}    f_g &amp; 0 &amp; w/2 \\    0 &amp; f_g &amp; h/2 \\    0 &amp; 0 &amp; 1\end{bmatrix}\]</span></p><p>旋转矩阵：<span class="math inline">\(R\in{\mathbb{R}^{3 \times3}}\)</span>，由<span class="math inline">\(\delta_{rot}\in{SO(3)}\)</span>计算得到。<span class="math inline">\(\delta_{rot}\)</span>包含三个参数，分别是沿着x轴，y轴，z轴旋转角度</p><p>平移矩阵：<span class="math inline">\(t=\begin{bmatrix}t_x &amp; t_y&amp;t_z\end{bmatrix}^T\)</span>；平移矩阵内部参数的计算取决于预测的平移向量<span class="math inline">\(\delta_{trans}=\begin{bmatrix}d_x &amp; d_y &amp;d_z\end{bmatrix}^T\)</span>决定。(备注：<span class="math inline">\(c_x,c_y\)</span>是每个人脸在图像平面的中心位置)<span class="math display">\[\begin{bmatrix}    t_x \\    t_y  \\    t_z\end{bmatrix} =\begin{bmatrix}    d_z(d_x+c_x-w/2)/f_g \\    d_z(d_y+c_y-h/2)/f_g \\    d_z\end{bmatrix}\]</span></p><p>姿态系数：前文提到的姿态系数<span class="math inline">\(\delta_{pose}=(\delta_{rot},\delta_{trans})\)</span></p><p>图像空间坐标表示：<span class="math inline">\(p\propto{K(RX+t)},p=(u,v,1)^T\)</span></p><h3 id="损失函数">损失函数</h3><p>整体损失函数有5项函数构成，分别叫center loss，pixel-wiseloss，perception-level loss，sparse landmark reprojectionloss，regularization loss。其中后面三项的函数设计的思路或者内容与<a href="https://starrylight.top/2022/09/24/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0--Accurate%203D%20Face%20Reconstruction%20with%20Weakly-Supervised%20LearningFrom%20Single%20Image%20to%20Image%20Set/#more">CVPRWorkshop2019论文</a>的损失函数一致</p><p>后续补充损失函数介绍~~~</p>]]></content>
      
      
      <categories>
          
          <category> Face Reconstruction </category>
          
          <category> Paper Notes </category>
          
          <category> arXiv </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Face Reconstruction </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PyTorch随机种子的设置</title>
      <link href="/2022/09/27/PyTorch--Random%20Seed/"/>
      <url>/2022/09/27/PyTorch--Random%20Seed/</url>
      
        <content type="html"><![CDATA[<h2 id="随机数">随机数</h2><p>随机数是专门的随机实验结果，其特点在于产生的后面那个数与前一个数是没有联系的。真正的随机数是利用物理现象产生的，比如掷钱币、骰子、转轮等，产生随机数的方法统称为随机数发生器。</p><h2 id="伪随机数">伪随机数</h2><p>伪随机数其实是具有一定的规律性的，它是由计算机使用算法模拟出来的，但由于其产生的算法相对来说较为复杂，规律周期较长，并不容易查找出其规律。</p><span id="more"></span><h2 id="随机种子">随机种子</h2><p>随机种子是相对于随机方法而言，随机方法常见的有生成随机数，随机排序等。在深度学习中，常用的随机方法有网络的随机初始化，训练集的随机打乱。在深度学习中的随机种子是属于伪随机数，由随机数算法生成一种<strong>均匀分布且难以预测的序列</strong>。</p><h2 id="设置随机种子意义及弊端">设置随机种子意义及弊端</h2><p><strong>意义：</strong>在其他超参和网络等各项参数不变的情况下，在PyTorch中预先设置好随机种子后，可以保证<strong>多次运行此段代码能够得到完全一样的结果</strong>，此举保证了结果的<strong>可复现性</strong>。</p><p><strong>弊端：</strong>确定性操作通常比非确定性操作慢，因此模型的单次运行性能可能会降低。但是，确定随机种子可以通过促进试验、调试和回归测试来节省开发时间。</p><h2 id="设置随机种子方法">设置随机种子方法</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">setup_seed</span>(<span class="params">seed</span>): <span class="comment"># 1314 3407</span></span><br><span class="line">    random.seed(seed)</span><br><span class="line">    np.random.seed(seed)</span><br><span class="line">    torch.manual_seed(seed)</span><br><span class="line">    torch.cuda.manual_seed_all(seed)</span><br><span class="line">    torch.backends.cudnn.deterministic = <span class="literal">True</span></span><br></pre></td></tr></table></figure><h2 id="参考资料">参考资料</h2><p><a href="https://blog.csdn.net/xu624735206/article/details/124999824">pytorch设置随机种子</a></p><p><a href="https://blog.csdn.net/qq_24224067/article/details/106451064?spm=1001.2101.3001.6661.1&amp;utm_medium=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1-106451064-blog-124999824.t0_edu_mlt&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1-106451064-blog-124999824.t0_edu_mlt&amp;utm_relevant_index=1">Python随机种子介绍，PyTorch 中随机种子的设置与应用</a></p><p><a href="https://zhuanlan.zhihu.com/p/92622101?utm_medium=social&amp;utm_oi=947120030148726784&amp;utm_psn=1558129920798064640&amp;utm_source=wechat_session">深度学习--随机种子</a></p><p><a href="https://pytorch.org/docs/stable/notes/randomness.html">PyTorchReproducibility</a></p>]]></content>
      
      
      <categories>
          
          <category> PyTorch Summary </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PyTorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>综述笔记--Review Face Reconstruction</title>
      <link href="/2022/09/24/%E7%BB%BC%E8%BF%B0%E7%AC%94%E8%AE%B0--Review%20Face%20Reconstruction/"/>
      <url>/2022/09/24/%E7%BB%BC%E8%BF%B0%E7%AC%94%E8%AE%B0--Review%20Face%20Reconstruction/</url>
      
        <content type="html"><![CDATA[<h2 id="简介">简介</h2><p>传统3D人脸重建方法，大多是立足于图像信息，如基于图像亮度、边缘信息、线性透视、颜色、相对高度、视差等等一种或多种信息建模技术进行3D人脸重建。这方面的技术论文，无论国内外都相当多，也较杂乱，一时间个人也不好具体统计，总之其中也是有很多不错的思想和方法的，当然这也不是本文重点内容。基于模型的3D人脸重建方法，是目前较为流行的3D人脸重建方法；3D模型主要用三角网格或点云来表示，现下流行的模型有通用人脸模型(CANDIDE-3)和三维变形模型(3DMM)及其变种模型，基于它们的3D人脸重建算法既有传统算法也有深度学习算法。端到端3D人脸重建方法，是近年新起的方法；它们绕开了人脸模型，设计自己的3D人脸表示方法，采用CNN结构进行直接回归，端到端地重建3D人脸。<span id="more"></span></p><h2 id="d可形变模型3d-morphable-model">3D可形变模型(3D MorphableModel)</h2><p>可形变模型（MorphableModel）这一名词来源于计算机图形学中一个名叫Morphing技术的图像生成算法。Morphing技术主要思想：如果两幅图像中存在一定的对应关系，那么就可以利用这个对应关系生成具一副有平滑过渡效果的图像。3DMM，即三维可变形人脸模型，是一个通用的三维人脸模型，用固定的点数来表示人脸。它的核心思想就是人脸可以在三维空间中进行一一匹配，并且可以由其他许多幅人脸正交基加权线性相加而来。（线性模型）</p><h2 id="未完待续">未完待续</h2>]]></content>
      
      
      <categories>
          
          <category> Face Reconstruction </category>
          
          <category> Review </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Paper Review </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文笔记--Cross-modal Deep Face Normals with Deactivable Skip Connections</title>
      <link href="/2022/09/24/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0--Cross-modal%20Deep%20Face%20Normals%20with%20Deactivable%20Skip%20Connections/"/>
      <url>/2022/09/24/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0--Cross-modal%20Deep%20Face%20Normals%20with%20Deactivable%20Skip%20Connections/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要">摘要</h2><p>当下单目重建多采用数据驱动的策略，但是受限于真实标签数据的缺乏，导致这种方法很困难。本文提出一种跨模态网络架构，可以利用所有图像和法线数据（无论是否配对），通过encoder和decoder的跳跃连接实现面部细节在图像和法线维度上进行传递。本文方法的核心就是一个融合deactivableskipconnection的模块，该方法通过相同的端到端架构集成了自动编码和图像到法线转换的功能。<span id="more"></span></p><h2 id="贡献">贡献</h2><ul><li>一种可以利用跨模态学习从单张人脸图像估计法线的框架；</li><li>可停用的跳跃连接架构模式（deactivable skip connection）</li><li>SOTA效果</li></ul><h2 id="方法">方法</h2><p><img src="https://gitee.com/Forever_XS/cloud_img/raw/master/blog/image-20220708143501613.png" alt="image-20220708143501613" style="zoom:50%;"></p><p>该架构允许利用成对或非成对的图像/法线数据进行图像到法线的转换（<span class="math inline">\(I\)</span>-&gt;<span class="math inline">\(\hat{N}\)</span>），在训练期间通过图像到图像（<span class="math inline">\(I\)</span>-&gt;<span class="math inline">\(\hat{I}\)</span>）和法线到法线（<span class="math inline">\(N\)</span>-&gt;<span class="math inline">\(\hat{N}\)</span>）的转换过程进行正则化。<span class="math inline">\(E_I\)</span>到<span class="math inline">\(D_N\)</span>的跳跃连接可以传递面部细节信息。</p><h3 id="deactivable-skip-connection">deactivable skip connection：</h3><figure><img src="/2022/09/24/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0--Cross-modal%20Deep%20Face%20Normals%20with%20Deactivable%20Skip%20Connections/image-20220707185938605.png" alt="image-20220707185938605"><figcaption aria-hidden="true">image-20220707185938605</figcaption></figure><p>在特征图从encoder到decoder传递过程中，这个skipconnection可以选择开启或关闭。</p><p>在进行 normal-&gt;normal (<span class="math inline">\(E_N\)</span>-&gt;<span class="math inline">\(D_N\)</span>) 传递过程中，<span class="math inline">\(D_N\)</span>的每一层输出<span class="math inline">\(F_{D}^{n-i}=f(F_{D}^{n-i-1})\)</span></p><p>在进行 image-&gt;normal（<span class="math inline">\(E_I\)</span>-&gt;<span class="math inline">\(D_N\)</span>）传递过程中，<span class="math inline">\(D_N\)</span>的每一层输出<span class="math inline">\(F_{D}^{n-i}\)</span>是由 前一层输出的<span class="math inline">\(f(F_{D}^{n-i-1})\)</span>上和<span class="math inline">\(F_E^i\)</span>同样通道数量的特征图，与<span class="math inline">\(F_E^i\)</span>特征图进行element-wisemax操作，得到新的特征图后和剩余通道数量的<span class="math inline">\(f(F_{D}^{n-i-1})\)</span>特征图进行相加得到。</p><p>这样做允许在不发生传输操作时将信息从编码器传输到解码器，而不会降低性能，就像自编码器正常工作时一样。</p><h3 id="loss-function">loss function：</h3><p><img src="https://gitee.com/Forever_XS/cloud_img/raw/master/blog/image-20220708143717592.png" alt="image-20220708143717592" style="zoom: 80%;"></p><p>训练过程只能对一个模态进行输入，要么是法线图要么是原图。</p><ul><li>当有原图输入，同时也有图片和法线图的ground truth时，先进行normal tonormal，再进行 image to normal，最后进行image toimage。上述的两个loss值进行同样比重求和得到最终loss。</li><li>当只有images或者只有normals时，就只进行image to image或者normal tonormal的传输过程。</li></ul>]]></content>
      
      
      <categories>
          
          <category> Face Reconstruction </category>
          
          <category> Paper Notes </category>
          
          <category> CVPR </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Face Reconstruction </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文笔记--Regressing Robust and Discriminative 3D Morphable Models with a very Deep Neural Network</title>
      <link href="/2022/09/24/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0--Regressing%20Robust%20and%20Discriminative%203D%20Morphable%20Models%20with%20a%20very%20Deep%20Neural%20Network/"/>
      <url>/2022/09/24/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0--Regressing%20Robust%20and%20Discriminative%203D%20Morphable%20Models%20with%20a%20very%20Deep%20Neural%20Network/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><strong><em>摘要</em></strong></h2><p>​人脸的3维形状是易于区分的，但尽管如此，3维人脸仍很少用于人脸识别以及3维人脸数据经常是在受控环境下采集的。现有的方法在野外环境下对3D人脸的估计是不稳定的，会因同一主题的不同照片而发生变化，要么是过拟合，要么是过于通用（平均脸）。本文直接对输入图片处理，通过CNN回归3DMM的形状和纹理参数。<strong>本文通过提供一种生成大量标记示例的方法来克服所需的训练数据不足的问题</strong>。本文方法在<strong>MICC数据集</strong>实现SOTA效果，同时在人脸识别效果上也有成效，本文在人脸识别上是使用3D人脸形状作为表示，而不是其他系统使用的不透明深度特征向量。<span id="more"></span></p><h2 id="介绍"><strong><em>介绍</em></strong></h2><p>​3D人脸形状是易于辨识的，因为每个人的人脸形状是不同的，且人脸形状不受光照和纹理等因素影响。目前没有关于在野外环境中成功使用单视图人脸形状估计来识别具有挑战性的无约束人脸的方法。</p><p><img src="https://gitee.com/Forever_XS/cloud_img/raw/master/blog/image-20220428131155413.png" alt="image-20220428131155413" style="zoom: 33%;"></p><h2 id="方法"><strong><em>方法</em></strong></h2><p>​本文借助深度卷积神经网络来实现单张图片输入，回归得到3DMM面部形状参数。训练这样的CNN需要大量的带有正确3D面部形状标签的无约束面部数据。作者认为，之前没有将CNN用在三维人脸建模方面，主要是因为从二维图像重建三维人脸模型，我们需要回归高维的形状参数，这就要求非常深的网络，而训练非常深的网络又需要大量的训练数据，已知的三维人脸模型的训练集非常少。针对这一问题，作者提出利用一个对象的多姿态人脸图片生成准确率相当高的三维人脸形状（Automated3D face reconstruction from multiple images using qualitymeasures），然后把生成的模型作为训练集；对于鲁棒性和区别性的人脸形状怎么解决呢？借鉴二维空间中的深度卷积神经网络模型，而且模型还是现成的~</p><p>​ 目前有三个主要现状认识：</p><ol type="1"><li><p>利用同一人脸的多辐图像可以得到精确的三维估计</p></li><li><p>正确的3D人脸形状的标签很少可以利用，而每个主体却是包含有多张照片的</p></li><li><p>深度神经网络能很好提取和区分人脸特征</p><p>方法概览：</p><figure><img src="https://gitee.com/Forever_XS/cloud_img/raw/master/blog/image-20220428131711825.png" alt="image-20220428131711825"><figcaption aria-hidden="true">image-20220428131711825</figcaption></figure></li></ol><h3 id="generating-training-data"><strong>Generating Trainingdata</strong></h3><p>​3DMM的表示采用的是BFM模型，模型数学含义表达式如公式(1)所示。其中<span class="math inline">\(\alpha\)</span>和<span class="math inline">\(\beta\)</span>分别表示形体和纹理参数，且都是99维的向量。<span class="math display">\[S^{&#39;}=\hat{s}+W_S\alpha{\quad}{\quad}T^{&#39;}=\hat{t}+W_T\beta\]</span> ​ 在中科院的CASIA WebFacedataset选定一张照片，利用CLNF做人脸检测，得到68个人脸关键点和该图片的置信度。将得到的关键点用来初始化估计人脸模型的姿态，该姿态用六个自由度表示，三个自由度是旋转角度，三个是位移。然后再优化3DMM的形状，纹理，姿态，光照和颜色，利用前人的方法解决定位误差。一旦损失函数收敛，就得到的形状参数和纹理参数，这就是单个图像得到的3DMM估计。</p><p>​一张图片的3DMM参数可以用公式(2)表示，N表示同一目标下所有视图数量，一个目标的一张视图的3DMM参数用<span class="math display">\[\gamma_i\]</span>表示，一个目标的3DMM参数用<span class="math inline">\(\gamma\)</span>表示，<span class="math inline">\(w_i\)</span>表示归一化后的置信度，如公式(3)所示。至此，每个CASIA数据集的目标对象都有一个合并好的3DMM参数向量<span class="math inline">\(\gamma\)</span>相关联。 <span class="math display">\[γ_i=[\alpha_i,\beta_i]\quad i\in1..N\quad(2)\]</span></p><p><span class="math display">\[{\gamma}={\sum}_{i=1}^{N}{w_i·\gamma_i}\quad{and}\quad{\sum}_{i=1}^{N}w_i=1\quad(3)\]</span></p><h3 id="learning-to-regress-pooled-3dmm">Learning to regress pooled3DMM</h3><p>​使用之前生成的数据用于训练一个函数式，理想情况下，同一主题的不同照片通过该函数式回归得到的3DMM特征向量是一致的。本文采用了ResNet101，并经过人脸识别数据集的预训练。本文将该网络的最后一层全连接层进行修改，使其输出198维的3DMM特征向量<span class="math inline">\(\gamma\)</span>。</p><p>​本文是将CNN在CASIA数据集上进行微调，标签真值是之前生成的3DMM估计；同一目标对象的不同视角的图片共用一个3DMM形状参数；本文还采用了16层的VGG-Face架构进行训练验证，其结果稍逊于ResNet101。</p><p>​3DMM向量属于多元高斯分布，均值在原点处，代表平均人脸。因此，在训练过程中采用标准的欧几里得损失会使得人脸模型太泛化，没有区别性。本文提出一种非对称欧拉损失，公式如下：其中<span class="math inline">\(\gamma\)</span>是标签值，<span class="math inline">\(\gamma_p\)</span>是预测值，<span class="math inline">\(\lambda_1\)</span>和<span class="math inline">\(\lambda_2\)</span>控制过度和不足的估计误差之间的平衡，<span class="math inline">\(\lambda_1\)</span>和<span class="math inline">\(\lambda_2\)</span>都等于1时就是传统的欧几里得损失。本文设定<span class="math inline">\(\lambda_1\)</span>的值为1，<span class="math inline">\(\lambda_2\)</span>的值为3，使模型更快摆脱欠拟合并且鼓励网络生成更真实的三维人脸模型。<span class="math display">\[\ell(\gamma_p,\gamma)=\lambda_1·\underbrace{||\gamma^+-\gamma_{max}||^{2}_{2}}_{over-estimate}+\lambda_2·\underbrace{||\gamma_{p}^{+}-\gamma_{max}||^{2}_{2}}_{under-estimate}\]</span></p><p><span class="math display">\[{\gamma}^{+}{\doteq}{abs(\gamma)}{\doteq}{sign(\gamma)}·{\gamma}{\quad}{\quad}{\gamma^{+}_{p}{\doteq}{sign(\gamma)·\gamma_p}}{\quad}{\quad}{\gamma_{max}{\doteq}{max(\gamma^+,\gamma_{p}^{+})}}\]</span></p><p>​ 训练参数设置：SGD，batch_size: 144，momentum: 0.9，l2 weight decay:0.0005，learning rate: 0.01</p><p>​3DMM参数直接通过CNN对输入图像回归得到，没有进行纹理渲染的优化，只为得到准确的形状。</p><h3 id="parameter-based-3d-3d-recognition">Parameter based 3D-3Drecognition</h3><p>​ 用余弦相似度来判断两个人脸三维模型是否相似，公式如下： <span class="math display">\[s(\gamma_1,\gamma_2)=\frac{\gamma_{p1}·\gamma_{p2}^T}{||\gamma_{p1}||·||\gamma_{p2}||}\]</span></p><h2 id="实验结果">实验结果</h2><p>评价指标：</p><figure><img src="C:/Users/Admin/AppData/Roaming/Typora/typora-user-images/image-20220428132309766.png" alt="image-20220428132309766"><figcaption aria-hidden="true">image-20220428132309766</figcaption></figure><p>重建实验效果及对比：</p><figure><img src="https://gitee.com/Forever_XS/cloud_img/raw/master/blog/image-20220428132703834.png" alt="image-20220428132703834"><figcaption aria-hidden="true">image-20220428132703834</figcaption></figure><p>野外数据重建效果：</p><p><img src="https://gitee.com/Forever_XS/cloud_img/raw/master/blog/image-20220428132730719.png" alt="image-20220428132730719" style="zoom: 67%;"></p><p><strong>END</strong></p>]]></content>
      
      
      <categories>
          
          <category> Face Reconstruction </category>
          
          <category> Paper Notes </category>
          
          <category> CVPR </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Face Reconstruction </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文笔记--To Fit or Not to Fit Model-based Face Reconstruction and Occlusion Segmentation from Weak Supervision</title>
      <link href="/2022/09/24/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0--To%20Fit%20or%20Not%20to%20Fit%20Model-based%20Face%20Reconstruction%20and%20Occlusion%20Segmentation%20from%20Weak%20Supervision/"/>
      <url>/2022/09/24/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0--To%20Fit%20or%20Not%20to%20Fit%20Model-based%20Face%20Reconstruction%20and%20Occlusion%20Segmentation%20from%20Weak%20Supervision/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><strong>摘要</strong></h2><p>​由于遮挡物的剧烈的可变性，遮挡下的人脸重建极具挑战性。目前最成功的方法是通过逆向渲染来拟合3D人脸模型，并假设遮挡物给定分割，以此避免拟合遮挡物。然而，训练一个遮挡分割模型需要大量的注释数据。在这项工作中，我们介绍了一种基于模型的3D人脸重建方法，该方法对遮挡具有高度鲁棒性，但不需要任何遮挡注释进行训练。在我们的方法中，我们利用了生成人脸模型只能合成人脸而不是遮挡物的事实。我们使用此属性来指导遮挡分割网络的决策过程并完成无监督训练。目<strong>前主要挑战是模型拟合和遮挡分割相互依赖，需要共同推理。</strong>CelebA-HQ、AR数据库和 Now Challenge的定性和定量实验表明，所提出的方法在遮挡下实现了最先进的 3D 人脸重建。此外，尽管在没有任何遮挡注释的情况下进行了训练，但分割网络仍能准确定位遮挡。</p><span id="more"></span><h2 id="介绍"><strong>介绍</strong></h2><p>​单目3D人脸重建旨在估计人脸的姿态、形状和反照率，以及场景的光照条件和相机参数。从单个图像中解决所有这些因素是一个不适定问题。面部自动编码器面临的一个主要挑战是在野外环境下模型的性能仍然受到诸如遮挡，极端照明和姿势等因素限制。遮挡导致的一个核心问题是人脸模型会拟合被遮挡的人脸区域，导致重建的人脸失真。因此，一个遮挡的鲁棒的3D人脸重建问题就是去决定一张图像中哪些像素是需要去拟合，哪些像素是不需要去拟合的。</p><p>​在本文的工作中，设计了一种基于模型的人脸重建方法，该方法具有高度的遮挡鲁棒性，不需要任何的人工遮挡注释。特别地，本文提出以一种合作的方式去训练一个面部自编码器和一个分割网络。分割网络决定人脸模型是否需要拟合某一像素的问题，以便人脸重建不受遮挡影响。分割网络采用无监督的方式去训练分割网络，利用了生成的人脸模型只能合成人脸而不能合成遮挡的事实。同时可以利用目标原始图像和渲染生成图像之间的差异作为监督信号来指导分割网络的训练。反过来，人脸重建网络通过使用来自分割网络的预测在拟合期间掩盖被遮挡的像素，从而对遮挡具有鲁棒性。这也导致了协同效应，遮挡分割引导面部自编码器拟合易于分类为面部区域的图像区域，改进的人脸拟合反过来又使得分割网络能够改进其预测。</p><p>​训练过程遵循EM算法的核心思想，通过在给定当前分割掩码估计的情况下训练面部自编码器和随后基于当前3D面部重建训练分割网络之间交替进行。分割网络的无监督训练是通过在估计的遮挡掩码下正则化和保留目标图像和重建图像之间的相似性来实现的，通过引入了几个损失来实现这一点。设计的模型在三份数据集进行验证，分别是<strong>CelebA-HQ,AR, NoW challenge</strong>。</p><p>​ 总之，我们在本文中做出了以下贡献：</p><ol type="1"><li>实现一种基于模型的 3D人脸重建方法，该方法具有高度鲁棒的遮挡，无需任何人工遮挡注释。</li><li>设计的模型在遮挡下的 3D人脸重建中实现了SOTA，并在野外图像上提供了面部遮挡掩码的准确估计。</li></ol><h2 id="方法"><strong>方法</strong></h2><p>​本文的目标是由严重遮挡的单一图片重建出鲁棒的3D人脸。为解决该问题，本文将基于模型的人脸自动编码器𝑅 与分割网络 𝑆集成在一起，并在它们之间产生协同作用。分割掩码在模型拟合期间消除遮挡的估计，使重建网络对遮挡具有鲁棒性。重建的结果给分割网络提供了参考，促使分割网络的准确性提升。</p><figure><img src="https://gitee.com/Forever_XS/markdown_paper_picture/raw/master/blog_casia/image-20220509144350144.png" alt="image-20220509144350144"><figcaption aria-hidden="true">image-20220509144350144</figcaption></figure><p>​</p><h5 id="training-the-segmentation-network">Training the segmentationnetwork</h5><p>​在训练分割网络时，人脸自编码器的参数是固定的，只优化分割网络。我们没有寻找标记数据，而是提出了四种损失来增强图像之间的内在相似性。每个损失都可以包括指示面部或相反的像素。损失在感知级别或像素级别上起作用，以充分利用视觉线索。分割网络训练时的四种损失如下：</p><figure><img src="https://gitee.com/Forever_XS/markdown_paper_picture/raw/master/blog_casia/image-20220509151017598.png" alt="image-20220509151017598"><figcaption aria-hidden="true">image-20220509151017598</figcaption></figure><p>​ 除了上述四种损失外，还添加正则化项损失<span class="math inline">\(L_{bin}=-\sum_x(M(x)-0.5)^2\)</span>来鼓励面部掩码是二值化分布（0或1）。总体损失函数式如下，其中<span class="math inline">\(\eta_1=15\quad\eta_2=3\quad\eta_3=0.5\quad\eta_4=2.5\quad\eta_5=10\)</span><span class="math display">\[L_S=\eta_1L_{neighbor}+\eta_2L_{dist}+\eta_3L_{area}+\eta_4L_{presv}+\eta_5L_{bin}\]</span></p><h5 id="training-the-face-autoencoder">Training the faceautoencoder</h5><p>​ 训练面部自编码器网络时，冻结分割网络参数。训练损失函数如下：</p><p><img src="https://gitee.com/Forever_XS/markdown_paper_picture/raw/master/blog_casia/image-20220509155639300.png" alt="image-20220509155639300" style="zoom: 67%;"></p><h5 id="unsupervised-initialization">Unsupervised Initialization</h5><p>​ 使用遮挡的鲁棒损失生成初级掩码：</p><figure><img src="https://gitee.com/Forever_XS/markdown_paper_picture/raw/master/blog_casia/image-20220509160342837.png" alt="image-20220509160342837"><figcaption aria-hidden="true">image-20220509160342837</figcaption></figure><h2 id="网络">网络</h2><p>​ encoder采用ResNet50，segmentation network采用UNet。</p>]]></content>
      
      
      <categories>
          
          <category> Face Reconstruction </category>
          
          <category> Paper Notes </category>
          
          <category> arXiv </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Face Reconstruction </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文笔记--Accurate 3D Face Reconstruction with Weakly-Supervised Learning:From Single Image to Image Set</title>
      <link href="/2022/09/24/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0--Accurate%203D%20Face%20Reconstruction%20with%20Weakly-Supervised%20LearningFrom%20Single%20Image%20to%20Image%20Set/"/>
      <url>/2022/09/24/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0--Accurate%203D%20Face%20Reconstruction%20with%20Weakly-Supervised%20LearningFrom%20Single%20Image%20to%20Image%20Set/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><strong>摘要</strong></h2><p>​  最近，基于深度学习的3D人脸重建方法在质量和效率上都显现出可喜的成果。然而，深度神经网络的训练通常需要大量的数据，而具有真实3D人脸形状标签的人脸图像却很少。该文章提出一种深度3D人脸重建方法：</p><span id="more"></span><ol type="1"><li><p><strong>利用一个稳健的混合损失函数进行弱监督学习，该函数同时考虑了image-level 和 perception-level 的鲁棒性</strong></p><p><strong>（image-level损失指的是重建的人脸模型渲染得到的图片和输入图片的像素值应尽可能一致；perception-level损失指的是重建的人脸模型渲染得到的图片和输入图片的内在特征应尽可能一致）</strong></p></li><li><p><strong>利用不同图像的互补信息进行形状聚合，进行多图像人脸重建。</strong></p></li></ol><p>​  文章提出的方法快速，准确且对遮挡和大姿态的图像具有鲁棒性。该文章在MICCFlorence 和Facewarehouse数据集上进行实验论证，同时系统地与最近几年的15种方法进行对比，实验结果证明本文提出的方法展现出了目前最先进的性能。</p><p>​  Code (Pytorch):https://github.com/sicxu/Deep3DFaceRecon_pytorch</p><h2 id="介绍"><strong>介绍</strong></h2><p>​  从非受限场景下的2D图像中忠实地恢复人脸的3D形状是一项具有挑战性的任务，并且具有许多应用，例如人脸识别，面部媒体操控，人脸动画等。最近，人们对使用CNN来实现单个图像重建3D人脸的兴趣激增，以代替使用复杂且要大量优化的传统方法。由于真实的3D人脸数据稀少，以往许多方法都使用合成数据或者使用传统方法拟合的3D形状作为替代形状标签，但是这种做法会受到域差距和不准确的训练标签影响。</p><p>​  无监督学习的关键是一个可微的图像形成过程，它使用网络预测来渲染人脸图像，监督信号源于输入图像和渲染对应物之间的差异。该问题提出了混合级损失函数和一种新颖的基于肤色的光度误差注意策略，该方法对人脸遮挡问题有一定解决。该文章也在重点研究同一目标多图像的人脸重建问题。</p><p>​  该文章训练一个辅助网络。借助该网络回归得到承载身份的3D模型系数的置信度，并通过基于置信度的聚合获得最终的身份系数。它可以利用位姿差异更好地融合互补信息，学习更准确的3D形状。</p><p>​  该文章的主要两项贡献是：</p><ol type="1"><li><strong>提出一种基于 CNN的单图像人脸重建方法，该方法利用混合级图像信息进行弱监督学习。改进的损失函数包括图像级损失和感知级损失。使用低维3DMM 子空间，仍然能够胜过具有“无限制”3D 表示的现有技术。</strong></li><li><strong>提出一种用于多图像人脸重建聚合的新型形状置信度学习方法。置信预测子网也以弱监督方式进行训练，方法明显优于朴素聚合（例如形状平均）。</strong></li></ol><h2 id="方法"><strong>方法</strong></h2><h3 id="整体架构"><strong>整体架构</strong></h3><figure><img src="/2022/09/24/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0--Accurate%203D%20Face%20Reconstruction%20with%20Weakly-Supervised%20LearningFrom%20Single%20Image%20to%20Image%20Set/full_process.png" alt="框架流程"><figcaption aria-hidden="true">框架流程</figcaption></figure><h3 id="模型理念"><strong>模型理念</strong></h3><p>​<strong>3D face model</strong>：使用一个CNN对输入图像处理，得到<span class="math inline">\({\alpha}, {\beta}, {\delta}, {p},{\gamma}\)</span>等参数，<span class="math inline">\({\alpha}\in{R}^{80},{\beta}\in{R}^{64},{\delta}\in{R}^{80}\)</span>。表达式如下所示，最终模型包括36k个顶点：</p><p><span class="math display">\[S=S(\alpha,\beta)=\bar{S}+B_{id}\alpha+B_{exp}\beta\\T=T(\delta)=\bar{T}+B_t\delta\]</span></p><p>​<span class="math inline">\({S},{T}\)</span>分别表示平均脸的形状和纹理参数，<span class="math inline">\(B_{id}, {B_{exp}},{B_t}\)</span>分别是身份，表情和纹理的PCA基（都经过标准偏差归一化处理）；使用<strong>BFM</strong>预测<span class="math inline">\(\bar{S},B_{id},\bar{T},B_t\)</span>，使用<strong>FaceWarehouse</strong>预测<span class="math inline">\(B{exp}\)</span>。</p><p>​<strong>IlluminationModel</strong>：假设人脸是Lambertian平面，使用球谐函数SH估计场景光照。法向是<span class="math inline">\(n_i\)</span>，皮肤纹理是<span class="math inline">\(t_i\)</span>的顶点<span class="math inline">\(s_i\)</span>辐射度为下式，其中<span class="math inline">\({\phi_b}:R^3\rightarrow{R}\)</span>是SH基函数，<span class="math inline">\(\gamma_b\)</span>是对应的SH系数，<span class="math inline">\(\gamma\in{R}^9\)</span>，<span class="math inline">\(B=3\)</span> <span class="math display">\[C(n_i,t_i|\gamma)=t_i·{\sum}_{b=1}^{B^2}{\gamma_b}{\phi_b}(n_i)\]</span>​ <strong>Camera Model</strong>：透视投影，3D人脸姿态<span class="math inline">\(p\)</span>由旋转<span class="math inline">\(R\in{SO(3)}\)</span>和平移<span class="math inline">\(t\in{R^3}\)</span>表示。</p><p>​<strong>Summary</strong>：最终预测的向量结果<span class="math inline">\(x=(\alpha,\beta,\delta,\gamma,{p})\in{R^{239}}\)</span>，（备注：80+64+80+9+6=239）利用ResNet50来回归预测这239个参数，将ResNet50的最后一层全连接输出改为239，该网络称为R-Net。</p><h3 id="单张图片人脸重建"><strong>单张图片人脸重建</strong></h3><p>​  给定一张RGB图片<span class="math inline">\(I\)</span>，作者使用R-Net回归参数向量<span class="math inline">\(x\)</span>，根据得到的人脸模型可以渲染得到新的图片<span class="math inline">\(I&#39;\)</span>，训练时不需要任何真实标签，而是根据<span class="math inline">\(I&#39;\)</span>来计算损失。</p><h4 id="image-level-loss">Image-Level Loss</h4><h5 id="robust-photometric-loss">Robust Photometric Loss</h5><p>​  原始图片和重建图片之间的像素差异作为损失是直观的方法，本文基于此提出了一种鲁棒的、皮肤感知的图像损失：式子定义如下：</p><!-- $$\begin{matrix}L_{photo}(x)=\frac{\sum_{i\in{M} }A_i·||I_i-I'_i(x)||_2}{\sum_{i\in{M} }A_i}\\A_i=\{ {^{1,\quad{if\quad{P_i>0.5}}}_{P_i,\quad{otherwise} } }\end{matrix}$$ --><p><span class="math display">\[L_{photo}(x)=\frac{\sum_{i\in{M}}A_i·||I_i-I&#39;_i(x)||_2}{\sum_{i\in{M} }A_i}\\\]</span><span class="math display">\[A_i=\begin{cases}    1 &amp; P_i&gt;0.5\\    P_i &amp; otherwise\end{cases}\]</span> ​  其中<span class="math inline">\(i\)</span>表示像素索引；<span class="math inline">\(M\)</span>表示投影的人脸区域；<span class="math inline">\(||·||\)</span>表示<span class="math inline">\(l_2\)</span>距离；<span class="math inline">\(A\)</span>是一个基于皮肤颜色的attentionmask，借助在一个皮肤数据集训练好的贝叶斯分类器来预测每个像素<span class="math inline">\(i\)</span>上的皮肤颜色概率<span class="math inline">\(P_i\)</span> 。</p><h5 id="lamdmark-loss">Lamdmark Loss</h5><p>​  在训练中使用2D图片上的人脸关键点作为弱监督信息，利用sota的3D人脸对齐方法来检测训练图片的68个人脸坐标<span class="math inline">\(q_n\)</span>，将重建人脸的3D关键点投影到图像空间得到<span class="math inline">\(q&#39;_n\)</span>，计算两者的距离作为损失。<span class="math inline">\(w_n\)</span>是坐标点权重，经过实验发现在嘴巴内部和鼻子处的权重设为20，其他地方的权重设为1最合适，式子如下。</p><p><span class="math display">\[L_{lan}(x)=\frac{1}{N}{\sum}_{n=1}^{N}w_n||q_n-q&#39;_n(x)||^2\]</span></p><h4 id="perception-level-loss">Perception-Level Loss</h4><p><span class="math display">\[L_{per}(x)=1-\frac{\left&lt;f(I),f(I&#39;(x))\right&gt;}{||f(I)||·||f(I&#39;(x))||}\]</span></p><h4 id="正则化">正则化</h4><p>​  为防止人脸形状和纹理退化，使用3DMM系数的正则项：</p><p><span class="math display">\[L_{coef}(x)=w_{\alpha}||\alpha||^2+w_{\beta}||\beta||^2+w_{\gamma}||\gamma||^2\\w_\alpha=1.0 \quad w_\beta=0.8 \quad w_\gamma=1.7e-3\]</span></p><p>​  尽管BFM模型的面部纹理是使用特殊设备获得的，但仍有一些阴影（如环境光遮蔽），为此再添入一个正则项：</p><p><span class="math display">\[L_{tex}(x)=\sum_{c\in{r,g,b}}var(T_{c,R}(x))\]</span></p><h4 id="损失函数总结">损失函数总结</h4><p>​  损失函数的总式子如下，其中<span class="math inline">\(w_{photo}=1.9\quad w_{lan}=1.6e-3 \quad w_{per}=0.2 \quad w_{coef}=3e-4 \quadw_{tex}=5\)</span> <span class="math display">\[L(x)=w_{photo}L_{photo}(x)+w_{lan}L_{lan}(x)+w_{per}L_{per}(x)+w_{coef}L_{coef}(x)+w_{tex}L_{tex}(x)\]</span></p><h3 id="多张图片人脸重建"><strong>多张图片人脸重建</strong></h3><p>​  除了从单张人脸图片重建人脸，如何从一个人的多张脸部图片，去重建一个更加精确的人脸模型也是一个很有意义的问题。不同的图片可能采集自不同的姿态、光照等，能够互相提供补充信息，这样重建出来的人脸对于遮挡、不佳的光照等情况更加鲁棒。</p><figure><img src="/2022/09/24/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0--Accurate%203D%20Face%20Reconstruction%20with%20Weakly-Supervised%20LearningFrom%20Single%20Image%20to%20Image%20Set/multi_image.png" alt="多视图重建流程"><figcaption aria-hidden="true">多视图重建流程</figcaption></figure><p>​  作者从单张图片人脸重建的结果学习一个置信度(反映重建质量)，作者针对人脸形状参数<span class="math inline">\(\alpha\in{R^{80}}\)</span>生成一个反映置信度的向量<span class="math inline">\(c\in{R^{80}}\)</span>，图片集为<span class="math inline">\(I:=\{I^j|=j=1,...M\}\)</span>，最终的人脸形状参数为：</p><p><span class="math display">\[a_{aggr}=(\sum_{j}c^j\odot\alpha^j)\oslash(\sum_jc^j)\]</span><span class="math display">\[\odot和\oslash分别表示哈达玛积和商\]</span></p><p>​  作者提出C-Net来预测置信度<span class="math inline">\(c\)</span>，由于R-Net能够预测诸如人脸姿态、光照等高阶信息，很自然地想到将其特征图运用到C-Net中来，作者同时使用了R-Net的浅层和深层特征，如前图所示。为了训练C-Net，首先从一张图片<span class="math inline">\(I^j\)</span>得到人脸系数<span class="math inline">\(\hat{x}^j\)</span>，<span class="math inline">\(\hat{x}^j=(\alpha_{aggr},\beta^j,\delta^j,\gamma^j,p^j)\)</span>，然后生成重建图片<span class="math inline">\(I^{j&#39;}\)</span>，C-Net的损失函数如下所示，<span class="math inline">\(L(·)\)</span>是前面单张人脸图片重建的损失函数</p><p><span class="math display">\[\mathcal{L}(\{\hat{x}^j\}) = \frac{1}{M}\sum_{j=1}^{M}L(\hat{x}^j)\]</span></p>]]></content>
      
      
      <categories>
          
          <category> Face Reconstruction </category>
          
          <category> Paper Notes </category>
          
          <category> CVPR </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Face Reconstruction </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
